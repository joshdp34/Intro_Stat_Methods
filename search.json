[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 2381 Introductory Statistical Methods",
    "section": "",
    "text": "Introduction\nThese are the lecture notes for STA 2381 - Introductory Statistical Methods.\nPrerequisites: None\n\nCourse Description:\nParametric statistical methods. Topics range from descriptive statistics through regression and one-way analysis of variance. Applications are typically from biology and medicine. Computer data analysis is required.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01.html",
    "href": "01.html",
    "title": "1  Introduction to Statistics",
    "section": "",
    "text": "1.1 What is Statistics?\nGuiding question: How do we use data to make better decisions?\nWhen people say “trust the data,” it can sound like magic. It isn’t. Statistics is the discipline that helps us collect, organize, and interpret data so that our decisions are more principled and less guess-y. In this course, we’ll treat statistics as a practical toolkit you’ll use across science, business, health, and everyday life.\nThe basic premise is simple: if we describe the data clearly and account for uncertainty honestly, we can make better choices. That might mean deciding which treatment is more effective, which ad design leads to more clicks, or whether a new policy seems to be working.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "01.html#what-is-statistics",
    "href": "01.html#what-is-statistics",
    "title": "1  Introduction to Statistics",
    "section": "",
    "text": "“The non-scientist in the street probably has a clearer notion of physics, chemistry and biology than of statistics, regarding statisticians as numerical philatelists, mere collector of numbers.” - Stephen Senn, Dicing with Death: Chance, Risk and Health\n\n\n\n\n\nWhat is “statistics,” exactly?\nStatistics is the science of collecting, organizing, analyzing, and interpreting data to make decisions or draw conclusions. It’s not just about numbers—it’s about what those numbers tell us.\nIf Statistics concerns data, then we should define “data.” First, note that “data” is a plural word. “Datum” is singular, although it is common to hear someone refer to “data” in the singular. A “datum” is a piece of information or fact. So “data” is a collection of facts or information we collect. That could mean\n\nthe amount of profit a company makes,\nthe growth of plants under some conditions,\nor how many people voted in an election.\n\nA helpful way to organize the subject of Statistics is to distinguish two complementary activities:\n\nDescriptive statistics help us summarize and visualize what we observed—think graphs, tables, and numerical summaries. The goal is clarity.\nInferential statistics help us generalize from a sample to a broader group (or process) and quantify our uncertainty about that generalization. The goal is justified conclusions.\n\nEven in a short conversation about data, you’ll hear a few recurring ideas:\n\nA population is the full set of people, items, or occasions we care about (all Baylor first-years this fall, all batteries produced this week).\nA sample is the subset we actually observe.\nA parameter is a (usually unknown) number that describes a population (the true average battery life, for example).\nA statistic is a number we compute from a sample (the sample’s average battery life) that we use to learn about the parameter.\n\nWe’ll study these terms in more detail soon; for now, hold on to the big idea: we summarize what we see (description) and we reason beyond what we see (inference).\n\n\nWhy decisions need both description and inference\nSuppose a clinic tests a new flu-prevention program among 200 volunteer patients. A month later, 18% of the “usual care” group got the flu, compared to 12% of the “new program” group. Descriptively, the new program looks better. Inferentially, we ask: could this gap be due to chance? If we ran the study again with different patients, might the difference shrink or flip? Statistics gives us a way to quantify that uncertainty and decide what to do next.\nA similar story plays out in business A/B tests, manufacturing quality checks, and sports analytics. The descriptive picture tells us what happened in the data; inference tells us how strongly that evidence supports a decision.\n\n\nVariability, bias, and honest uncertainty\nTwo forces shape every data story:\n\nVariability is the natural fluctuation we see from case to case or study to study. Even fair coins produce streaks.\nBias is systematic deviation—our design, measurement, or selection method pushes results in a consistent direction.\n\nGood statistical practice aims to reduce bias and acknowledge variability. We’ll use design principles to minimize bias and inferential tools to express uncertainty honestly.\n\n\nHow we’ll work in this course (and in JMP)\nOur general workflow:\n\nStart with a clear question and name the observational units (what a single case is) and variables (what we record about each case).\nDecide how the data were or will be collected (survey, experiment, database pull).\nUse descriptive statistics and graphics to get oriented.\nBuild an inferential argument when you need to generalize or compare.\nCommunicate a conclusion in context—what it means, what it doesn’t, and what to do next.\n\nJMP Pro 17 note. In JMP, we’ll lean on Graph Builder, Distribution, and Fit Y by X for description; and on Analyze platforms (e.g., Fit Y by X, Fit Model) for inference. You’ll learn to read the output and connect it to the logic above.\n\n\nA first look at long-run regularity (illustration)\nTo make “variability vs. long-run behavior” concrete, here’s a quick simulation of coin flips. Early on, the proportion of heads jumps around. As the number of flips grows, the proportion tends to settle near 0.5. We’ll rely on this idea—randomness in the short run, stability in the long run—throughout the course.\n\n\n\n\n\n\n\n\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nStatistics\nThe discipline of learning from data to describe patterns and make decisions under uncertainty.\n\n\nData\nRecorded information about cases (people/items/occasions) used as evidence for questions of interest.\n\n\nDescriptive statistics\nMethods for summarizing and visualizing what was observed (tables, graphs, numerical summaries).\n\n\nInferential statistics\nMethods for generalizing from a sample to a population and quantifying uncertainty.\n\n\nPopulation\nThe full group or process we want to understand.\n\n\nSample\nThe subset we actually observe and analyze.\n\n\nParameter\nA (usually unknown) numerical characteristic of a population.\n\n\nStatistic\nA numerical summary computed from a sample, used to learn about a parameter.\n\n\nVariability\nNatural fluctuation in data from case to case or study to study.\n\n\nBias\nSystematic deviation caused by design, measurement, or selection issues.\n\n\nObservational unit\nThe “one thing” a single row in the data represents (a person, part, game, etc.).\n\n\n\n\n\nCheck your understanding\n\nIn your own words, how is describing data different from inferring from data? Give a short example for each.\nIdentify the population, sample, parameter, and statistic in this scenario: A battery company tests 60 batteries from today’s production line and finds an average life of 7.8 hours. The company wants to know the true average life of all batteries produced today.\nA streaming service tests two home-page designs on 5,000 visitors each. Version B produces a 0.6 percentage-point higher click-through rate than Version A. What questions would you ask before recommending the company switch to Version B?\nExplain the difference between variability and bias using a dartboard analogy.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nDescription vs. inference. Description summarizes what happened in the observed data (e.g., “The median wait time yesterday was 11 minutes.”). Inference uses the sample to say something about a broader group or process, with uncertainty (e.g., “We estimate the typical wait time for all days like yesterday is 11 minutes, with margin of error ±2 minutes.”).\nBattery scenario. Population: all batteries produced today. Sample: the 60 tested batteries. Parameter: the true mean life of all batteries produced today. Statistic: the sample mean of 7.8 hours.\nA/B test questions. How were visitors assigned to versions (randomly)? Were there differences in traffic sources or device types? Is the effect stable over time? What margin of error or confidence interval accompanies the difference? What outcome do we ultimately care about (sign-ups, retention), and does the change affect it?\nDartboard analogy. Variability: darts land around the bullseye but are spread out randomly. Bias: darts consistently land off to the lower-left—systematically shifted rather than centered.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "01.html#populations-and-samples",
    "href": "01.html#populations-and-samples",
    "title": "1  Introduction to Statistics",
    "section": "1.2 Populations and Samples",
    "text": "1.2 Populations and Samples\n\n“To understand God’s thoughts, we must study statistics, for these are the measures of his purpose.” - Florence Nightingale\n\nGuiding question: What’s the difference between a population and a sample?\nWhen you start any statistical investigation, the first two things to name are the population and the sample. The population is the full group or process you want to understand; the sample is the smaller set of cases you actually observe. Most of the time we can’t measure everyone or everything, so we sample wisely and then use statistics to bridge from the sample back to the population.\n\nDefining the population (clearly!)\nA population can be concrete (“all tires produced by Line A this week”) or conceptual (“all patients like these under similar conditions”). Two refinements are helpful:\n\nThe target population is the group you truly care about.\nThe accessible population is the group you can practically reach, often represented by a sampling frame—a list or mechanism from which you select the sample.\n\nClarity matters. If your target is “all Baylor first-year students this fall,” using a sampling frame of “students who attend welcome week” might miss commuters or students who arrived late. That gap can create bias if the missed students differ in a systematic way.\n\n\nWhat counts as a sample?\nA sample is the subset of the population you measure. We’ll study “how to sample” in Chapter 2; for now, focus on what a good sample does: it represents the population well enough that statistics computed from the sample are informative about the population.\nA special case is a census, where you attempt to measure every unit in the population. Censuses are rare in practice due to cost, time, and logistics—sampling is our workhorse.\n\n\nParameters and statistics (the bridge between the two)\nA parameter is a number that describes the population (usually unknown). A statistic is a number computed from a sample. We use statistics as estimates of parameters.\nBecause samples vary, statistics vary too. That natural fluctuation is sampling error. We’ll learn how to quantify it with margins of error and confidence intervals in later chapters.\n\n\nObservational units vs. sampling units\nTwo related terms often get mixed:\n\nThe observational unit is “one row of data” (one person, one battery, one game).\nThe sampling unit is “the thing you sample” (could be the same as the observational unit, but in cluster designs it might be a school or a household).\n\nBeing explicit about units guards against design mistakes and double-counting.\n\n\nWhy sampling works (and when it doesn’t)\nSampling works when your sample is representative of the population and your measurement is trustworthy. It struggles when parts of the population are systematically excluded (coverage problems), when participation differs by outcome (nonresponse), or when the way you select units is related to the outcome (selection effects). We’ll study these threats—and how to reduce them—next chapter.\n\n\nA quick illustration: samples differ, the goal doesn’t\nBelow is a simple simulation that shows how sample means wiggle from sample to sample even when we know the “true” population. As sample size grows, the wiggle shrinks.\n\n\n\n\n\n\n\n\n\nEven though each sample tells a slightly different story, they all tend to cluster around the true mean of 10. That’s the intuition behind using a statistic to learn about a parameter.\n\n\nConnecting to JMP Pro 17\nIn JMP you won’t “declare” the population, but you should always write it down in words before you analyze. Practically:\n\nUse Tables → Subset or randomization tools to create a sample when you have a large file standing in for a population.\nUse Analyze → Distribution to summarize your sample (means, proportions).\nKeep notes in the Data Table (Table panel → Notes) to record what your population and sampling frame are supposed to be.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nPopulation\nThe full group or process you want to understand.\n\n\nTarget population\nThe group you truly care about answering a question for.\n\n\nAccessible population\nThe portion of the target population you can practically reach.\n\n\nSampling frame\nThe list or mechanism from which the sample is drawn.\n\n\nSample\nThe subset of units you actually observe and measure.\n\n\nCensus\nAn attempt to measure every unit in the population.\n\n\nParameter\nA numerical characteristic of a population (e.g., \\(\\mu, p\\)).\n\n\nStatistic\nA numerical summary from a sample (e.g., \\(\\bar{x}, \\hat{p}\\)) used to estimate a parameter.\n\n\nSampling error\nNatural variation in a statistic from sample to sample.\n\n\nBias\nSystematic deviation caused by design, coverage, or selection issues.\n\n\nObservational unit\nThe “one case” a single row of data represents.\n\n\nSampling unit\nThe entity selected during sampling (may differ from the observational unit).\n\n\n\n\n\nCheck your understanding\n\nA battery plant tests 80 batteries from today’s production line and finds an average life of 7.8 hours. Identify the population, sample, parameter, and statistic.\nYour target is “all Baylor first-year students this fall.” You gather data from an email list of students who signed up for an early-interest program. What is the sampling frame? Name a potential source of bias.\nIn a household survey, you sample addresses, then interview every adult living there. What are the sampling units? What are the observational units?\nA marketing team uses comments on their Instagram post to judge product satisfaction among all customers. Explain why this may not represent the population.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nPopulation: all batteries produced today. Sample: the 80 tested batteries. Parameter: the true mean life of all batteries produced today. Statistic: the sample mean of 7.8 hours.\nSampling frame: students on the early-interest email list. Potential bias: students who didn’t sign up (e.g., commuters, late enrollees) may be under-represented, creating coverage/selection bias.\nSampling units: addresses (households) selected. Observational units: adults within each sampled household.\nInstagram commenters are a self-selected subset; satisfied or dissatisfied users may be more likely to comment, and customers not on Instagram are excluded—both threaten representativeness.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "01.html#variables-and-types-of-data",
    "href": "01.html#variables-and-types-of-data",
    "title": "1  Introduction to Statistics",
    "section": "1.3 Variables and Types of Data",
    "text": "1.3 Variables and Types of Data\n\n“Not everything that can be counted counts, and not everything that counts can be counted.” - Albert Einstein\n\nGuiding question: What kinds of data exist, and how do we recognize them?\nEvery data table is built from two ingredients: cases (the rows) and variables (the columns). A variable is any characteristic we record on each case. Choosing good analyses—and even drawing the right graph—depends on knowing what type of variable you’re working with.\n\nWhat is a variable?\nA variable can take different values across cases. “Resting heart rate,” “major,” “state of residence,” and “did the patient improve?” are all variables. Variables live in context: we should always be able to say what one row represents (the observational unit) and what each column means and how it was measured (the unit of measurement when relevant).\n\n\nThe two big families\nMost variables fall into two broad families. The names vary by textbook, but the ideas are stable.\n\nCategorical variables\nA categorical variable places each case into a group or label. Arithmetic on the labels doesn’t make sense.\n\nNominal: categories with no natural order (e.g., blood type, home state, device brand).\nOrdinal: categories with a meaningful order but uneven or unknown spacing (e.g., Likert ratings from “Strongly disagree” to “Strongly agree”; race finish places: 1st, 2nd, 3rd).\nBinary: exactly two categories (e.g., success/failure, disease/no disease).\n\n\n\n\n\n\n\nLikert Scale\n\n\n\n\n\nA Likert Scale is a common psychometric scale used in questionnaires to measure attitudes, opinions, or behaviors. It presents a statement and asks respondents to indicate their level of agreement or frequency on a symmetric, typically 5- or 7-point scale. The options range from one extreme (e.g., “Strongly Disagree”) to the opposite extreme (e.g., “Strongly Agree”), often including a neutral or middle point.\nExample: “How satisfied are you with our service?”\n\nStrongly Disagree\nDisagree\nNeutral\nAgree\nStrongly Agree\n\n\n\n\n\n\nQuantitative variables\nA quantitative variable records a number where arithmetic is meaningful.\n\nDiscrete: counts that jump in whole steps (e.g., number of ER visits, defects per unit).\n\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nIn math, the possible values of discrete data form what is known as countable set. This means the values form a collection (set) of values that can be put into a one-to-one correspondence with the natural numbers. In other words, a set is countable if you can list its elements in a sequence, even if the list is infinitely long.\nThere are two type of countable sets:\n\nFinite Countable Set\n\nThe set has a limited number of elements.\nExample: \\(\\{2, 4, 6, 8\\}\\) — there are exactly 4 elements.\n\nCountably Infinite Set\n\nThe set has infinitely many elements, but you can still list them in an ordered sequence.\nExample: The set of natural numbers \\(\\{1, 2, 3, 4, ...\\}\\)\nEven the set of all integers \\(\\{..., -2, -1, 0, 1, 2, ...\\}\\) is countable — you can reorder them as \\(0, 1, -1, 2, -2, 3, -3, ..\\) and still list them one by one.\n\n\n\n\n\n\nContinuous: measurements that, in principle, vary on a smooth scale (e.g., blood pressure, time to failure, height).\n\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nThe possible values of continuous data form an uncountable set. A set is uncountable if there’s no way to list all its elements in a sequence, even infinitely.\n\nExample: Real numbers between 0 and 1 — there are infinitely more of these than there are natural numbers.\nIn set notation, an example of an uncountable set is \\(\\{x: x\\ge 3\\}\\).\n\n\n\n\nTo decide if a variable is discrete or continuous, first think of an interval of possible values of the variable1. Can you count how many values are in that interval? If so, then it is discrete, if not, then it is continuous. In practice, we can simplify this further: counts are discrete, measurements are usually continuous.\n\n\n\nMeasurement scales you’ll hear about\nYou’ll occasionally see measurement scales—nominal, ordinal, interval, and ratio—used to describe variables.\n\nInterval: numeric scales where differences make sense but zero is arbitrary (°F, °C). Ratios aren’t meaningful (“40°F isn’t twice as hot as 20°F”).\nRatio: numeric scales with a true zero where ratios do make sense (Kelvin, length, mass, time). “10 minutes is twice 5 minutes” does make sense.\n\nYou don’t need to memorize the taxonomy; the practical takeaway is to be cautious about ratios on interval scales and to think about what operations make sense for each variable.\n\n\n“Identifier” and date/time variables\nSome columns are crucial for tracking rows but aren’t meant for analysis:\n\nIdentifier (ID) variables (e.g., student ID, order number, ZIP code) are labels, not quantities—even when they’re made of digits.\nDate/time variables carry timestamps that can be treated as either categorical (e.g., day of week) or quantitative (e.g., time since start). Be explicit about which role you intend.\n\n\n\nWhy types matter (graphs and analyses)\n\nCategorical → bar chart or pie chart; proportions/percents; chi-square-type tools later.\nQuantitative → dotplot, histogram, boxplot; means/medians/standard deviations; t-tools and regression later.\nOrdinal often behaves like categorical for display, but its order lets you use medians or ordered models.\nDiscrete vs. continuous matters for how the plot should look (bars separated vs. bins on a continuum) and for certain modeling choices.\n\n\n\nWorking in JMP Pro 17\nJMP keeps two concepts straight for each column:\n\nData Type (what the values are): Numeric, Character, or Date/Time.\nModeling Type (how you plan to analyze them): Continuous, Nominal, or Ordinal.\n\nDouble-click a column header (or right-click → Column Info) to set these. Typical mappings:\n\nCharacter + Nominal → categorical labels (e.g., “TX”, “CA”).\nNumeric + Continuous → quantitative measures (e.g., weight, time).\nNumeric + Nominal → numeric codes that are actually categories (e.g., 0/1 flags, ZIP codes, jersey numbers).\nNumeric + Ordinal → ordered categories encoded as 1–5 (Likert items).\n\nIf a graph or analysis looks odd in JMP, check these settings first—they control which menu options and displays you’ll see in Graph Builder and Analyze.\n\n\nA quick illustration (discrete vs. continuous)\nThe displays below look different because one variable is discrete (counts) and the other is continuous (measurements).\n\n\n\n\n\n\n\n\n\nIf you tried to “average” home states, you’d get nonsense; if you treated blood pressure as a category, you’d throw away useful information. Getting types right keeps your analysis honest and effective.\n\n\nCommon pitfalls (and quick fixes)\n\nDigits don’t guarantee “quantitative.” ZIP codes, part numbers, and jersey numbers are identifiers; set them to Character + Nominal in JMP.\nCollapsing a continuous variable into categories (“high/medium/low”) can simplify communication but often loses power. Keep the original too.\nTreating an ordinal scale as if equal steps are guaranteed can be misleading. Consider medians or nonparametric tools—or justify the approximation.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nCategorical variable\nPlaces a case into a group/label; arithmetic on labels is not meaningful.\n\n\nNominal\nCategorical with no natural order.\n\n\nOrdinal\nCategorical with a natural order but uneven/unknown spacing.\n\n\nBinary\nA categorical variable with exactly two categories.\n\n\nQuantitative variable\nNumeric values where arithmetic is meaningful.\n\n\nDiscrete\nQuantitative counts that change in whole steps.\n\n\nContinuous\nQuantitative measurements on a (nearly) continuous scale.\n\n\nIdentifier (ID)\nA label used to distinguish cases; not for numerical analysis.\n\n\nDate/time\nA timestamped variable that can be treated as categorical or quantitative depending on the question.\n\n\nMeasurement scale\nDescribes how numbers relate to the thing measured (nominal, ordinal, interval, ratio).\n\n\nInterval\nNumeric scale with arbitrary zero; differences meaningful, ratios not.\n\n\nRatio\nNumeric scale with a true zero; differences and ratios meaningful.\n\n\nUnit of measurement\nThe physical unit used to record a quantitative variable (e.g., mmHg, seconds).\n\n\n\n\n\nCheck your understanding\n\nClassify each variable as categorical nominal, categorical ordinal, binary, quantitative discrete, or quantitative continuous:\n\nNumber of missed classes this semester\nPain rating on a 0–10 scale\nWhether a chip passes final inspection\nZIP code\nBody temperature (°F)\n\nA researcher converts systolic blood pressure into “Low” (&lt;110), “Normal” (110–139), and “High” (≥140). Name one advantage and one drawback of this recoding.\nGive an example where a date/time variable should be treated as (a) categorical and (b) quantitative.\nFor each of the following, say whether ratios are meaningful and explain briefly:\n\nTemperatures in °C\nHours of weekly exercise\nIQ scores\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n(a)Quantitative discrete (counts). (b) Categorical ordinal (ordered scale with uneven spacing)—often treated as numeric for convenience, but it’s ordinal by design. (c) Binary (pass/fail). (d) Identifier (categorical nominal label; not a quantitative variable). (e) Quantitative continuous.\nAdvantage: simplifies communication and enables comparisons across broad groups. Drawback: throws away information; analyses lose power and can depend on arbitrary cutpoints.\n(a)Categorical: day of week when a call was received (Mon–Sun). (b) Quantitative: minutes since admission, time to recovery, or days from treatment to follow-up.\n(a)°C is an interval scale: ratios aren’t meaningful (20°C isn’t “twice as hot” as 10°C). (b) Hours of exercise is a ratio scale with a true zero: ratios are meaningful (4 hours is twice 2 hours). (c) IQ is typically treated as an interval scale: differences are interpretable; ratios are not.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "01.html#the-role-of-statistics-in-research",
    "href": "01.html#the-role-of-statistics-in-research",
    "title": "1  Introduction to Statistics",
    "section": "1.4 The Role of Statistics in Research",
    "text": "1.4 The Role of Statistics in Research\nGuiding question: How does the statistical process turn questions into answers?\nResearch isn’t just “run a test and see what comes out.” It’s a disciplined loop that starts with a clear research question, moves through careful study design and data collection, and ends with reasoned conclusions—always with honesty about uncertainty. Statistics is the glue in this loop: it helps you plan how to learn, summarizes what you saw, and quantifies what you can responsibly claim.\n\nFrom question to estimand to design\nEvery project should name three things up front:\n\nThe research question: a plain-language statement of what you want to learn (“Does the new tutoring program improve exam scores for Calculus I students?”).\nThe target parameter: the specific quantity in the population you want to know (e.g., the difference in mean exam scores between students who receive tutoring and those who don’t).\nThe study design: how you’ll collect data that speak to the target parameter (sampling plan, measures, and whether you’ll assign treatments).\n\nTwo broad design families appear everywhere:\n\nAn observational study records what already happens without assigning treatments. It’s great for describing associations but vulnerable to confounding (other factors moving with your variable of interest).\nAn experiment assigns a treatment and compares outcomes. With good randomization and control, experiments support stronger causal claims.\n\nYour choice should match the target parameter and the practical constraints.\n\n\nThe statistical process: a workable blueprint\nHere’s a simple, reusable workflow:\n\nSpecify the problem clearly. Name the population, observational units, and variables.\nDesign the study. Choose an observational or experimental approach, plan the sample, and anticipate sources of bias.\nCollect data with quality in mind. Strive for reliable measurement and good documentation.\nExplore first. Use exploratory data analysis (EDA)—graphs and summaries—to understand patterns, errors, and context before modeling.\nModel and infer. Pick a tool that matches the question and data type (comparison of means, relationships, predictions).\nQuantify uncertainty. Report estimates with confidence intervals and, when appropriate, p-values (confidence intervals and p-values will be covered in Chapters 9 and 10, respectively). Distinguish statistical significance from practical significance.\nDecide and communicate. Put results back in context: what do they mean, what they don’t, and what action (if any) follows.\nMake it reproducible. Ensure someone else could re-create your steps and numbers from the same data.\n\nYou won’t always march in a perfect line—often you loop back after EDA to refine the question or design. That’s good science.\n\n\nWorking in JMP Pro 17\nJMP is built for this process:\n\nDocument the plan and steps. Use File → New → Journal to create a running research notebook. Paste screenshots, notes, and output as you go.\nSave scripts to reproduce output. In most reports, click the red triangle ► Save Script → To Data Table (or To Journal). This stores a runnable recipe with the data.\nExplore first. Start with Graph → Graph Builder and Analyze → Distribution to profile variables and check data quality.\nFit models with assumptions in view. Use Analyze → Fit Y by X for two-variable comparisons and Analyze → Fit Model for multiple predictors. Residual and diagnostic tools are right there in the platform menus.\nShare and rerun. Bundle data, scripts, and notes with Projects or send a Journal so collaborators can reproduce your analysis.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nResearch question\nThe plain-language question your study seeks to answer.\n\n\nTarget parameter\nThe specific population quantity you aim to learn (e.g., mean difference, proportion).\n\n\nStudy design\nThe plan for collecting data (sampling, measurement, assignment) to answer the question.\n\n\nObservational study\nA design that observes existing groups without assigning treatments.\n\n\nExperiment\nA design that assigns treatments (often at random) and compares outcomes.\n\n\nTreatment\nThe condition or intervention applied in an experiment.\n\n\nConfounding\nA third factor related to both treatment and outcome that distorts comparisons.\n\n\nExploratory data analysis (EDA)\nEarly summaries/graphs used to understand data and spot issues.\n\n\nReproducibility\nThe ability for someone else to re-create your results from the same data and documented steps.\n\n\n\n\n\nCheck your understanding\n\nA hospital asks: “Does a new scheduling system reduce ER wait times compared to the current system?”\n\nState a suitable target parameter.\nName a design choice (observational vs. experimental) and one reason for your choice.\n\nA university compares GPA between students who use tutoring and those who don’t, with no assignment or randomization. Name a potential confounder and how an experiment would address it.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n(a)Target parameter: the mean difference in ER wait time (new minus current) for all eligible ER visits. (b) Design: a randomized experiment (e.g., randomize days or shifts to “new” vs. “current”), so groups differ only by scheduling system on average, improving causal interpretation.\nPotential confounder: prior academic preparation (e.g., incoming math placement). Students who seek tutoring might differ systematically. An experiment could randomly assign eligible students to tutoring or control (or randomize access), balancing confounders by design.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "01.html#the-statistical-process",
    "href": "01.html#the-statistical-process",
    "title": "1  Introduction to Statistics",
    "section": "1.5 The Statistical Process",
    "text": "1.5 The Statistical Process\n\n“An approximate answer to the right problem is worth a good deal more than an exact answer to an approximate problem.” -John Tukey\n\nGuiding question: Why does context matter as much as calculation?\nTwo analysts can compute the same number and reach very different conclusions—because numbers live inside stories. The context of your data—who and what were measured, when and where, and under what conditions—determines which comparisons are fair, which models fit, and what your results mean. In short: good statistics is not just calculation; it’s thoughtful interpretation anchored in how the data were produced.\n\nWhat “context” actually includes\nWhen we say “use context,” we mean four concrete ingredients:\n\nData context. What are the observational units? What do the variables represent, and what are their units of measurement?\nDesign context. How were cases selected or assigned? Is this an observational study or an experiment? Where could bias or confounding sneak in?\nMeasurement context. What is the precise operational definition of each variable? Are instruments calibrated? Is the measurement reliable and valid for the construct?\nAnalysis context. What assumptions do your methods make, and are they plausible here? What domain constraints or conventions matter?\n\nTogether, these pieces describe the data-generating process (DGP)—the mechanism by which the numbers in your table came to be. We make better choices when we reason about the DGP, not just the dataset.\n\n\nSame numbers, different stories: why stratification matters\nComparisons can flip when you ignore a meaningful background factor—a classic “wait, what?” known as Simpson’s paradox. Below is a toy illustration with a medical “success” outcome, two treatments (A, B), and a background factor “severity” with two levels (Easy/Hard). Within each severity group, A does slightly better. But because B is given more often to Easy cases, B looks better overall.\n\n\n\nSuccess rates by severity (within-group)\n\n\nseverity\ntreatment\nsuccesses\ntotal\nrate\n\n\n\n\nEasy\nA\n93\n100\n0.93\n\n\nEasy\nB\n828\n900\n0.92\n\n\nHard\nA\n657\n900\n0.73\n\n\nHard\nB\n72\n100\n0.72\n\n\n\n\n\n\nOverall success rates (ignoring severity)\n\n\ntreatment\nsuccesses\ntotal\nrate\n\n\n\n\nA\n750\n1000\n0.75\n\n\nB\n900\n1000\n0.90\n\n\n\n\n\nIgnoring context (severity) leads to a different recommendation than comparing like with like. The fix is simple in principle: compare within relevant strata or adjust for them in a model.\n\n\nA practical context checklist (before you compute)\nSlow down—ask and answer these, in writing:\n\nWho/what is a row? Name the observational unit.\nHow were data obtained? Sampling plan? Assignment? Time window? Inclusion/exclusion rules?\nWhat exactly was measured? Give operational definitions and units.\nWhich comparisons are fair? Identify potential confounders and plan to stratify or adjust.\nWhat assumptions will you need? Independence? Linearity? Constant variance? Distributional shape? (We will discuss these assumptions in later chapters)\nWhat will your claim be? Descriptive statement, association, or causal effect? Match your language to your design.\n\n\n\nWorking in JMP Pro 17: making context visible\nJMP gives you several context-friendly tools:\n\nDocument units and roles. Right-click a column → Column Info to set Units, Modeling Type (Nominal/Ordinal/Continuous), and notes. This prevents accidental “numeric” analysis of IDs.\nCompare like with like. Use Graph → Graph Builder with Group X/Y or Wrap to make side-by-side comparisons within levels of a context variable (e.g., severity, grade level).\nStratify or filter. Turn on Local Data Filter (from the report’s red triangle) to subset or facet analyses without touching the source table.\nAdjust in a model. In Analyze → Fit Model, include the context variable(s) as effects and consider an interaction to check for effect modification (i.e., whether the effect changes with context).\nUse “By” wisely. Many platforms allow a By variable to run the same analysis separately within each level—great for quick stratified summaries.\n\n\n\nLanguage that respects context\nBe precise. In observational data, prefer “associated with,” not “caused by.” Reserve causal language for designs (or assumptions) that justify it. Match conclusions to the population actually studied to protect external validity (generalizability), and avoid threats to internal validity (biases that distort the effect).\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nContext\nThe who/what/when/where/how surrounding the data that shapes fair comparisons and interpretation.\n\n\nData-generating process (DGP)\nThe mechanism or process by which the observed data were produced.\n\n\nStratification\nComparing groups within levels of a context variable to reduce confounding.\n\n\nSimpson’s paradox\nA reversal of an association when data are aggregated versus when compared within relevant strata.\n\n\n\n\n\nCheck your understanding\n\nA hospital reports that Treatment B has a higher overall recovery rate than Treatment A. After stratifying by initial severity, A has a higher recovery rate in each severity group. Name the phenomenon.\nA university analyzes GPA vs. tutoring usage from administrative records (no random assignment). What language should they use to describe the relationship? Name one likely confounder.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n(a)Simpson’s paradox.\nUse association language: “Tutoring use is associated with higher GPA,” not causal. Likely confounders include prior preparation (placement scores), motivation, or course load.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "01.html#footnotes",
    "href": "01.html#footnotes",
    "title": "1  Introduction to Statistics",
    "section": "",
    "text": "In real world applications, you are usually limited by how you measure. For instance, you may be measuring the length of insects and you measure to the nearest millimeter. This limitation should not play a role in determining continuous RVs. So in theory, insects could measure between 10 and 20 millimeters. You only measure in millimeters but an insect could be 10.1, 10.114, 10.675, 10.000004, etc. Since there are infinite number of values in 10 to 20 that insects could measure in theory, we say the length is continuous.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "02.html",
    "href": "02.html",
    "title": "2  Collecting Data",
    "section": "",
    "text": "2.1 Sampling Methods\nGuiding question: What makes a sample “representative”?\nA sample is just a subset of your population, but not every subset is equally useful. A representative sample matches the important characteristics of the population so well that if you analysed the sample’s data, you would reach the same conclusions you would have by measuring everyone. In other words, it’s a smaller group whose answers reflect those of the larger group. Designing studies that produce representative samples is the heart of statistics: it lets us reduce costs and time without sacrificing credibility.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "02.html#sec-sampling_methods",
    "href": "02.html#sec-sampling_methods",
    "title": "2  Collecting Data",
    "section": "",
    "text": "“And I knew exactly what to do. But in a much more real sense, I had no idea what to do.” - Michael Scott\n\n\n\n\nProbability vs. non‑probability sampling\nThere are two big approaches to sampling:\n\nIn probability sampling, you use random mechanisms so that every member of the population has a known chance of being selected. This ensures that any differences between the sample and the population are due to random sampling error, not researcher choice, and makes it possible to quantify uncertainty in estimates.\nIn non‑probability sampling, you choose participants based on convenience, volunteer responses, judgement or quotas. These methods are cheaper and often necessary for exploratory or qualitative work, but they make it hard to know whether the sample really reflects the population. Lack of a representative sample reduces the validity of conclusions and can introduce sampling bias.\n\nWhenever you want to generalize, probability sampling is the gold standard.\n\n\nSimple random sampling (SRS)\nIn a simple random sample (SRS), each individual in the population has an equal chance of being selected. You start with a complete sampling frame (a list of all units) and use a random number generator to pick your sample. In practice, SRS can be done with or without replacement; without replacement avoids picking the same unit twice. SRS is conceptually simple, but it requires a complete list and may be impractical when populations are spread out or access is difficult.\nExample: Suppose you want to survey 100 employees of a social media marketing company out of 1,000. You assign each employee a number from 1 to 1,000 and use a random number generator to select 100 numbers. Those employees become your sample. Because each employee had the same chance of selection, the sample is likely to be representative if the sample size is large enough.\n\n\nSystematic sampling\nSystematic sampling makes SRS easier by selecting every \\(k\\)-th unit from an ordered list. For example, pick a random starting point between 1 and 10, then choose every 10th person. If the list order has no hidden pattern, this approach approximates SRS while being cheaper to implement. However, if the ordering itself is related to the variable of interest (e.g., employees grouped by seniority), systematic sampling can inadvertently oversample or undersample certain groups.\nExample: All employees of a company are listed in alphabetical order. You randomly select a starting point among the first ten names—say, the 6th person—and then select every 10th person on the list (6, 16, 26, 36, …) until you have 100 employees. This produces a sample that is easy to implement but may be risky if the list order coincides with job role or seniority.\n\n\nStratified sampling\nIn stratified sampling, you first divide the population into subgroups (strata) based on an important characteristic (e.g., gender, age, region). You then sample randomly within each stratum in proportion to its size. Stratification ensures that each subgroup is properly represented and often reduces sampling error, especially when differences between strata are substantial. The trade‑off is increased complexity and cost, since you must track and sample separately within each stratum.\nExample: A company has 800 junior employees and 200 senior employees. To ensure the sample reflects the seniority balance, you sort the population into two strata and randomly select 80 junior employees and 20 senior employees. Your 100‑person sample preserves the 80/20 split of the population.\n\n\nCluster sampling\nCluster sampling also divides the population into groups, but here each cluster is (ideally) a mini‑population containing a mix of units. Instead of sampling individuals from every cluster, you randomly select a few clusters and include every individual within them or take a sample from them. This method is useful for large, geographically dispersed populations because it reduces travel or administrative costs. The downside is that if clusters differ substantially from one another, estimates may have higher variance than those from an SRS of the same size.\nExample: A company operates offices in 10 cities across the country, each with roughly the same number of employees. To conduct an employee survey, you randomly select 3 offices and survey everyone at those offices. This cuts down on travel but only works well if the offices are similar.\n\n\nNon‑probability methods\nSometimes you cannot (or choose not to) use random selection. Non‑probability methods are practical for exploratory research or hard‑to‑reach groups, but they cannot guarantee a representative sample.\n\nConvenience sampling\nA convenience sample includes whoever is easiest to reach. It is inexpensive but cannot produce generalizable results.\nExample: You want to learn about student support services, so after each of your classes you ask fellow students to complete a quick survey. Because you only surveyed students in your own classes, the sample is not representative of all students.\n\n\nVoluntary response sampling\nA voluntary response sample consists of people who opt in on their own. These participants are often those with strong opinions, leading to self‑selection bias.\nExample: You email a survey to the entire student body asking for opinions about a new campus policy. Only those who feel strongly (either for or against) are likely to respond, so the results cannot be trusted to reflect the average student’s view.\n\n\nPurposive sampling\nIn purposive sampling, you choose participants because they have specific characteristics that are particularly informative.\nExample: You want to understand the experiences of disabled students using university services, so you purposefully select students with varied support needs to gather a range of perspectives. This approach is useful for qualitative insight but not for estimating population-level quantities.\n\n\nSnowball sampling\nSnowball sampling recruits participants via referrals from initial subjects. It’s used when populations are difficult to identify or contact.\nExample: To study experiences of homelessness, you interview one person who agrees to participate; she introduces you to other people she knows who are also homeless. The sample grows like a snowball, but you have little control over how representative it is.\n\n\n\nWorking in JMP Pro 17\nAlthough you won’t implement complex sampling plans in JMP, it’s helpful to know how to create random subsets for practice or model validation. To select a simple random sample from a data table, use Rows → Row Selection → Random Sample to mark a proportion or number of rows at random, then Tables → Subset to create a new table of just those rows. For stratified sampling, add a column indicating the stratum (e.g., gender) and use Row Selection → Stratify or a script to select random rows within each level. To take a cluster sample from a large data set, you can create a column identifying clusters (e.g., schools) and use Tables → Subset to extract all rows from randomly chosen clusters. While JMP has tools for convenience (e.g., sorting and taking the first \\(n\\) rows), remember that non‑probability sampling cannot support generalizable inference.\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nRepresentative sample\nA sample that accurately reflects key characteristics of the population.\n\n\nProbability sampling\nSampling technique using random selection so each unit has a known chance of inclusion.\n\n\nNon‑probability sampling\nSampling techniques based on convenience or judgement without randomisation.\n\n\nSimple random sampling\nEvery unit has an equal chance of selection; implemented via random number generators.\n\n\nSystematic sampling\nSelecting every \\(k\\)-th unit from an ordered list after a random start.\n\n\nStratified sampling\nDividing the population into subgroups and randomly sampling within each subgroup.\n\n\nCluster sampling\nRandomly selecting entire groups (clusters) and studying all units within them.\n\n\nConvenience sampling\nIncluding the most accessible units; prone to sampling and selection bias.\n\n\nVoluntary response\nSampling based on participants who choose to respond, often those with strong opinions.\n\n\nPurposive sampling\nSelecting cases based on researcher judgement of what is most informative.\n\n\nSnowball sampling\nRecruiting participants via referrals from initial subjects, often for hidden populations.\n\n\n\n\n\nCheck your understanding\n\nYou want to estimate the average GPA of all first‑year students at your university.\n\nName two probability sampling methods you could use.\nBriefly explain why a convenience sample of your friends might mislead you.\n\nA researcher selects every 5th name from a sorted list of patients to survey. What sampling method is this? Under what circumstance might this method introduce bias?\nCompare stratified sampling and cluster sampling. Give an example of a scenario where each would be appropriate.\nExplain why voluntary response samples often yield extreme views and cannot be trusted for generalizing to a population.\nIn JMP, how could you create a simple random sample of 150 observations from a data table with 2,000 rows?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n(a)Simple random sampling (assign each first‑year student a number and randomly select using a random number generator); stratified sampling (divide students by major or residence hall and sample proportionally within each group). (b) Your friends are likely from similar classes or social circles, so they may have similar study habits; they might not reflect the broader student body.\nThis is systematic sampling. It works well if the list has no pattern related to the outcome. If patients are sorted by appointment time, every 5th patient might always be a morning appointment, which could bias results if morning and afternoon patients differ.\nStratified sampling divides the population into meaningful groups and samples within each (e.g., sampling men and women separately when studying height). It ensures each subgroup is represented. Cluster sampling selects whole groups (e.g., choosing three hospitals at random and surveying all nurses within them) to save cost when the population is geographically spread out.\nPeople with strong positive or negative feelings are more likely to volunteer, while those who are neutral remain silent. This self‑selection skews the sample, so the responses do not reflect the average opinion in the population.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "02.html#experimental-design",
    "href": "02.html#experimental-design",
    "title": "2  Collecting Data",
    "section": "2.2 Experimental Design",
    "text": "2.2 Experimental Design\n\n“All life is an experiment. The more experiments you make the better.” -Ralph Waldo Emerson\n\nGuiding question: How do we design surveys and experiments?\nStatistics gives us two complementary ways to gather evidence: surveys and experiments. In a survey we choose cases from a population, ask questions, and summarize what we learn. In an experiment we assign treatments to units and observe responses. The logic of inference depends on both good sampling and good design. A common confusion is that random sampling (how you pick cases) and random assignment (how you distribute them to treatments) are the same thing. They’re not: random sampling increases external validity (generalizability), while random assignment increases internal validity (causal credibility).\n\nPrinciples of good experimental design\nExperiments aim to isolate the effect of a treatment by controlling other sources of variation. Four key principles help us get there:\n\nRandomization. Assigning units to conditions by chance ensures that, on average, groups are comparable on both observed and unobserved characteristics. In a completely randomized design, every unit is assigned independently—like students randomly given different cell phone use limits in a sleep study. In a randomized block design, units are first grouped on a relevant characteristic (e.g., soil plots by rainfall zone) and then randomly assigned within each block. Randomization also applies to order of runs: in a manufacturing experiment, running treatments in random order prevents the effect of ambient conditions (e.g., temperature) from confounding with other variables.\nFor example, in a drug study with 20 mice and two test kits (A and B), you might randomly select 10 mice to receive kit A and the remaining 10 to receive kit B. Randomization transforms systematic variation (such as age or weight differences) into random variation, preventing confounding of the treatment effect with extraneous factors.\nControl and placebo. Experiments compare a treatment group to a control group. The control may receive “usual care” or a placebo to account for expectations. Placebo-controlled designs are common in medicine because participants may improve simply because they believe they’re being treated. By keeping all other conditions identical and varying only the active ingredient, the researcher can attribute differences in outcomes to the treatment itself rather than to the act of giving the treatment.\nReplication. Repeating the same treatment on multiple experimental units lets us estimate natural variability and distinguish real effects from random noise. For example, measuring battery life on several cells under the same charging condition gives a better estimate than using just one battery.\nBlocking. If you know a nuisance factor (like soil type or patient age) affects the response, block on it. In a block design, you divide units into homogeneous groups and then randomize treatments within each block. In biological experiments, mice are naturally grouped by litter, and chemical samples often come in batches; these natural groupings can serve as blocks. By assigning treatments randomly within each litter or batch, you control for variation due to the block (e.g., genetic or batch differences) and gain more precise estimates of the treatment effect.\n\nA good design often combines these ideas: you might randomize fertilizer treatments within rainfall blocks and measure each plot’s yield multiple times. Designs also come in two broad structures: between‑subjects experiments, where each unit receives exactly one condition, and within‑subjects (or repeated‑measures) experiments, where each unit experiences all conditions in random order. Within‑subjects designs reduce variability but require care to avoid order effects—randomizing or counterbalancing the order of conditions is essential.\n\n\nDesigning unbiased survey questions\nSurveys need two kinds of care: how you select respondents and how you ask questions. For representative sampling methods, see Section 2.1. Here we focus on question design. Well‑written questions are the foundation of trustworthy research. Biased or confusing wording can distort data and mislead decision‑makers. Common pitfalls include:\n\nLeading questions: wording that nudges respondents toward a particular answer.\nBiased: “Don’t you agree that our new app is much easier to use?”\nUnbiased: “How would you rate the ease of use of our new app?”\nLoaded questions: questions that assume something controversial is true (e.g., “When did you stop wasting time on your phone?”)\nDouble‑barreled questions: asking about two things but allowing only one answer. The respondent may have different answers for each part, so the result is uninterpretable. For example, “Do you intend to leave work and return to full‑time study this year?”—someone may be planning to leave work but not to study, or vice versa. Another example: “How would you rate our products and level of service?” Because it blends product quality with service quality, a single rating cannot reveal which aspect the respondent is judging.\nAmbiguous wording: vague or multi‑interpretation phrases that lead different respondents to answer different questions. For example, a survey item that asks “How do we compare to our competitors?” leaves respondents guessing whether you mean price, quality, or customer service.\n\nTo craft unbiased survey questions:\n\nUse neutral language. Avoid emotionally charged words and let respondents choose their own answer.\nBe specific and clear. Define terms and make sure every respondent interprets the question the same way.\nAsk one thing at a time. Split double‑barreled questions into separate items.\nBalance response options. Provide symmetrical, evenly spaced choices (ie. 5-point Likert Scale).\nPilot test your survey. Try it on a small group to catch confusing wording or unexpected interpretations.\n\nThese principles help respondents understand your survey and give honest answers.\n\n\nPutting it all together\nIn practice, you often combine surveys and experiments. For example, a company might randomly assign new customers to receive one of two onboarding emails (experiment), then survey them about satisfaction. You would:\n\nRandomly assign customers to email A or B.\nEnsure both groups are comparable (randomization).\nCollect follow‑up satisfaction surveys using neutral, clear questions.\nReplicate across multiple cohorts and possibly block by customer type (e.g., new vs. returning).\n\n\n\nWorking in JMP Pro 17\n\nRandomize and block. When running experiments in JMP, create a column for treatment assignments using Col → Formula → Random to simulate random assignment. Use Tables → Sort to randomize run order, or define blocking factors as separate columns and use Fit Model to include them in your analysis.\nControl and replicate. For experiments with control and treatment conditions, use Analyze → Fit Y by X for simple comparisons or Analyze → Fit Model for multifactor designs. Replicate treatments by duplicating rows; JMP will treat these as separate experimental units.\nDesign surveys. Use Rows → Row Selection → Random Sample to draw pilot samples. Store survey questions in a Data Table with labels and notes. To pilot test question wording, subset your sample and collect feedback, then refine the wording before launching the full survey.\n\n\n\nRecap\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nRandom assignment\nAssigning sampled units to treatment conditions by chance to create comparable groups.\n\n\nTreatment group / control group\nGroups receiving the experimental intervention and baseline comparison, respectively.\n\n\nPlacebo\nAn inert treatment used to mimic the experience of the intervention to control for expectations.\n\n\nReplication\nRepeating the same treatment on multiple experimental units to estimate variability.\n\n\nBlocking\nGrouping similar units and randomizing within each group to control a nuisance factor.\n\n\nBetween‑subjects design\nEach unit experiences only one condition; comparisons are across subjects.\n\n\nWithin‑subjects design\nEach unit experiences all conditions in random order.\n\n\nLeading question\nA survey question that suggests a particular answer.\n\n\nLoaded question\nA survey question containing an assumption or implication.\n\n\nDouble‑barreled question\nA single question that asks about two things.\n\n\nAmbiguous wording\nVague terms that can be interpreted differently by different respondents.\n\n\n\n\n\nCheck your understanding\n\nExplain the difference between random sampling and random assignment. Why are both important, and in what contexts do they apply?\nName the four principles of good experimental design and give a brief example of each.\nConsider this survey question: “How satisfied are you with the cost and quality of your textbooks?” Identify the problem and rewrite the question.\nIn a study of exam performance, 60 students volunteer for tutoring and 60 do not. The volunteer group has a higher average GPA than the non‑volunteer group. Explain why this study may not show that tutoring causes better performance. How could you redesign it?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nRandom sampling determines who gets into the study. Every member of the population has a known chance of selection, improving generalizability. Random assignment determines which condition participants experience, creating comparable groups and allowing causal conclusions. Surveys rely on random sampling; experiments rely on random assignment.\nRandomization: assign units by chance (e.g., randomize phone use levels to study sleep). Control/placebo: include a baseline or placebo condition to isolate the treatment effect. Replication: repeat treatments on multiple units, like testing several batteries under the same condition. Blocking: group units by a nuisance factor (e.g., soil type) and randomize within blocks.\nThe question is double‑barreled—it asks about cost and quality. Rewrite as two separate questions (e.g., “How satisfied are you with the cost of your textbooks?” and “How satisfied are you with the quality of your textbooks?”).\nVolunteers may differ systematically from non‑volunteers (e.g., motivation or prior GPA). Random assignment is missing. To infer causality, randomly assign students to tutoring or control groups and compare outcomes, possibly blocking on prior GPA.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "02.html#observational-studies-vs.-experiments",
    "href": "02.html#observational-studies-vs.-experiments",
    "title": "2  Collecting Data",
    "section": "2.3 Observational Studies vs. Experiments",
    "text": "2.3 Observational Studies vs. Experiments\n\n“You can observe a lot by just watching.” - Yogi Berra\n\nGuiding question: Why are observational studies sometimes problematic for conclusions?\nAt first glance, collecting data looks the same whether you’re watching what happens or deliberately changing something. But how you gather the data matters tremendously for what you can conclude.\n\nObservational studies: watching without intervening\nIn an observational study, researchers record what happens without actively assigning exposures or treatments. Common types include:\n\nCohort studies, where a group of people linked by a characteristic (e.g., birth year) is followed over time. Researchers compare outcomes between those exposed to some factor and those not exposed.\nA classic example of a cohort study is the Framingham Heart Study (FHS), which has been described in detail in the International Journal of Epidemiology. In 1948 the National Heart Institute recruited a community‑based cohort of 5,209 adults aged 30–59 years from Framingham, Massachusetts, to investigate causes of cardiovascular disease (CVD). Two of every three families in the town were randomly sampled and invited; 4,494 (about 69%) agreed to participate, and an additional 715 volunteers joined. This initial prospective cohort has been followed every two to four years with detailed medical histories, physical examinations, electrocardiograms and laboratory tests. By following participants longitudinally for decades, the FHS identified major risk factors for CVD—such as high blood pressure, cholesterol, and smoking—helping to shape modern cardiovascular prevention guidelines.\nCase–control studies, where people with a condition (“cases”) are compared to similar people without it (“controls”) to look for differences in past exposures.\nFor example, a German study1 compared 118 patients with a rare form of eye cancer called uveal melanoma to 475 healthy patients who did not have this eye cancer. The patients’ cell phone use was measured using a questionnaire. On average, the eye cancer patients used cell phones more often. The cases were those who had developed uveal melanoma and the controls were those who did not uveal melanoma. The cell phone use was compared between the two groups.\n\nBecause participants choose their own behaviors, observational data reflect the real world and are often the only ethical way to study harmful exposures. For example, you can’t ethically assign people to smoke or not smoke, so the long‑term effects of smoking are studied by tracking smokers and non‑smokers over time.\nObservational studies are usually quicker and cheaper than experiments and have high ecological validity (they mirror everyday life). But they have a critical limitation: you can’t be sure whether differences in outcomes are caused by the exposure or by other factors that differ between groups. For example, a highly publicized 1985 study from Johns Hopkins University linked coffee consumption to an increased risk of heart disease, especially for heavy drinkers. The study’s findings, published in the American Journal of Epidemiology, were later challenged by other research that pointed out the failure to adequately control for the effect of cigarette smoking. Once smoking was controlled for, the link between coffee consumption and increased risk of heart diseas was no longer significant.\n\n\nExperiments: deliberately changing something\nIn an experiment, researchers assign treatments or interventions to units and observe the effects. Randomization—assigning units by chance—ensures that, on average, the groups are comparable on both observed and unobserved characteristics. The classic experimental design is the completely randomized design: participants are randomly allocated to receive a new drug, a placebo, or no treatment, and outcomes are compared. Experiments are considered the gold standard for establishing causality because randomization eliminates systematic differences between groups.\nExperiments also offer a controlled environment, making it easier to isolate the effect of a single factor. However, they can be expensive, time-consuming, or unethical to conduct.\nFor example, suppose we were interested in the association between eye cancer and smart phone use. Suppse we conduct an experiment, such as the following:\n\nPick half the students from your school at random and tell them to use a smart phone each day for the next 50 years.\nTell the other half of the student body not to ever use smart phones.\nFifty years from now, analyze whether cancer was more common for those who used smart phones.\n\nThere are obvious difficulties with such an experiment:\n\nIt’s not ethical for a study to expose over a long period of time some of the subjects to something (such as smart phone radiation) that we suspect may be harmful.\nIt’s difficult in practice to make sure that the subjects behave as told. How can you monitor them to ensure that they adhere to their treatment assignment over the 50-year experimental period?\nWho wants to wait 50 years to get an answer?\n\nThus, an observational study would be preferred over an experiment.\n\n\nWhy observational studies can mislead\nObservational data are susceptible to confounding—a situation where a third factor influences both the exposure and the outcome, creating a spurious association. For example, an observational cohort might find that people who meditate have lower rates of heart disease. But meditators may also exercise more and eat healthier diets, making it unclear whether meditation or lifestyle explains the difference. Similarly, people who choose to take daily vitamins might generally have healthier habits, so observed vitamin benefits may reflect those habits rather than the vitamins themselves.\nRandomization is the only method that can eliminate potential confounders by balancing both measured and unmeasured factors across treatment groups. In observational research, statistical methods like stratification, regression adjustment and propensity score matching can reduce bias, but they depend on untestable assumptions: all confounders must be measured correctly and modeled properly. Many important confounders may be unknown or infeasible to measure. Even meticulously controlled observational studies cannot remove all confounding. As a result, observational evidence alone “cannot support conclusions of causation”.\nBelow is a simple simulation illustrating confounding. Shoe size and reading ability appear positively related, but both are driven by age. When age is not controlled, a misleading association emerges.\n\n\n\n\n\n\n\n\n\nThe scatterplot shows a strong correlation between shoe size and reading, even though neither directly affects the other. The common cause is age. Observational studies must always consider whether a hidden variable like age could be responsible for an observed association.\n\n\nWorking with observational data in JMP Pro 17\nIn JMP you can explore observational data using Graph → Graph Builder or Analyze → Fit Y by X to visualize associations. Use Analyze → Fit Model for regression or ANOVA, including potential confounders as predictors. JMP will happily fit a model and produce p-values, but you must interpret the results cautiously. Remember:\n\nIncluding covariates can adjust for measured confounders, but unmeasured confounders remain.\nAssociations observed in scatterplots or regression analyses do not imply causation.\nUse notes and metadata fields to document the study design—observational or experimental—and any potential sources of bias.\n\n\n\nRecap\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nObservational study\nA study in which researchers record exposures and outcomes without assigning treatments or interventions.\n\n\nCohort study\nObservational design where a group is followed over time to compare outcomes between exposed and unexposed members.\n\n\nCase–control study\nObservational design where people with a condition (“cases”) are compared to similar people without the condition (“controls”) to look for differences in past exposures.\n\n\nExperiment\nA study where researchers introduce an intervention and randomly assign subjects to treatment or control groups.\n\n\nConfounding\nA situation where a third factor influences both the exposure and the outcome, potentially creating a spurious association.\n\n\n\n\n\nCheck your understanding\n\nA nutrition researcher recruits people who already take daily multivitamins and compares their health outcomes to people who do not.\n\nIs this an observational study or an experiment?\nName at least two potential confounding variables.\n\nIn a randomized trial, half the participants are assigned to eat a Mediterranean diet and half to continue their usual diet. After a year, the first group shows lower cholesterol. Explain why randomization strengthens the causal interpretation.\nA cohort study finds that people who bike to work have lower rates of depression than those who drive. Suggest two reasons why this association may not reflect a causal effect of biking.\nDescribe a research question that would be unethical or impractical to answer via experiment but could be studied observationally. Explain why.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\na)This is an observational study because participants choose whether or not to take multivitamins. b) Potential confounders include diet quality, exercise habits, socioeconomic status, access to healthcare, smoking status and other health behaviors.\nRandomization assigns diets by chance, so, on average, both known and unknown factors (age, lifestyle, genetics) are balanced across the groups. Therefore, differences in cholesterol are likely due to the diet rather than pre‑existing differences.\nPeople who bike may have higher baseline fitness and better mental health; they might live in neighborhoods with better infrastructure or community support; they may also have lifestyles that promote well‑being (e.g., more time outdoors). Any of these confounders could explain the observed association.\nStudying the long‑term effects of smoking is unethical to do experimentally, because you can’t randomly assign people to smoke. Instead, researchers observe smokers and non‑smokers and compare outcomes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "02.html#sources-of-bias",
    "href": "02.html#sources-of-bias",
    "title": "2  Collecting Data",
    "section": "2.4 Sources of Bias",
    "text": "2.4 Sources of Bias\n\n“Normally if given a choice between doing something and nothing, I’d choose to do nothing. But I would do something if it helps someone do nothing. I’d work all night if it meant nothing got done.” - Ron Swanson\n\nGuiding question: How can bias creep into data collection?\nWhen we talk about bias in statistics, we mean a systematic error built into the way we select or measure our data. Bias is different from random sampling error. Sampling error comes from the natural variability you get when you observe only part of the population and tends to shrink as sample sizes grow. Bias, by contrast, does not go away with bigger samples; a flawed design simply produces more confident wrong answers. That makes it important to understand the different ways bias sneaks into our studies.\n\nHow bias differs from sampling error\nWhenever we select a sample, the numbers we compute (like the mean or proportion) will vary from one sample to the next. This variability is called sampling error. If we repeated our survey many times with different random samples, the average of those sample statistics would be the true population value, and the spread among them would reflect sampling error. Increasing the sample size reduces sampling error, but it does not correct for systematic flaws in how the sample was chosen. When the method of collecting or measuring data systematically favors some outcomes over others, we call it bias. A biased sample can be huge and still be wrong because its error is baked into the design.\nTo illustrate the difference, imagine a population with a true average income of $8 (in arbitrary units), made up of 70% low earners (income of 5) and 30% high earners (income of 15). Below we simulate two ways of sampling from this population: a fair simple random sample and a biased sample that over‑selects high earners (80% high, 20% low). As the sample size grows, the random sample mean settles near the true average, while the biased sample mean stays high. This shows that increasing the sample size reduces random error but does not fix bias.\n\n\n\n\n\n\n\n\n\n\n\nCommon sources of bias\nBias can enter at many points in the data‑collection process. Here are some of the most common culprits:\n\nCoverage (undercoverage) bias. A coverage bias occurs when some members of the population are not included in the sampling frame. The Literary Digest’s famous 1936 presidential poll relied on telephone directories and car registration lists, thereby missing less affluent voters who tended to support Franklin Roosevelt. Because those voters were excluded, the sample favored wealthier respondents and overpredicted Alfred Landon’s support.\n\n\n\n\n\nNonresponse bias. A nonresponse bias arises when selected individuals choose not to participate and the responders differ systematically from nonresponders. In the same 1936 survey only 25 % of those sampled returned the mail‑in ballot. Landon supporters were more likely to return the survey, so the results overestimated his popularity.\nVoluntary response bias. When people opt into a survey on their own—like call‑in radio polls about controversial topics—the sample disproportionately includes individuals with strong opinions. This voluntary response bias can produce extreme results because moderate voices remain silent.\nConvenience sampling bias. A convenience sample chooses whoever is easiest to reach. If you stand outside a gym to survey “all adults in the city,” your sample will overrepresent health‑conscious people. Convenience sampling often leads to coverage problems.\nResponse (measurement) bias. Even if we select the right people, the way we ask questions or record data can introduce response bias. Response bias occurs when the measurement process influences the answer: leading questions or unbalanced answer choices can nudge respondents toward particular responses. Social desirability bias occurs when people underreport socially undesirable behaviors or overreport virtuous ones.\nSurvivorship bias. When we only observe “survivors” and ignore those that dropped out or failed, we can mistake success for the rule.\n\n\n\nThe most famous example of this is the WWII bomber problem. During WWII, analysts tallied bullet holes on returning Allied bombers and saw clusters on wings and fuselage, with relatively few in engines and cockpit. The intuitive fix was to add armor where the holes were densest. Statistician Abraham Wald pointed out the trap: these data come only from planes that survived. Holes on the survivors mark places a plane can be hit and still make it home. The missing planes—those that didn’t return—are precisely the ones likely hit in the “clean” areas (e.g., engines). So Wald recommended reinforcing the areas with the fewest holes on the survivors, not the most. That’s survivorship bias: drawing conclusions from only the observed “winners” and ignoring the unseen “failures.”\nRecall bias. In retrospective studies, participants may not remember past events accurately. People who have developed an illness might recall exposures differently than healthy controls, leading to systematic differences.\nInterviewer bias. The interviewer’s tone, appearance or expectations can subtly influence responses. Neutral wording and training can reduce this effect.\nHealthy‑user bias and attrition bias. People who choose to participate in certain programs or who remain in a study for its duration often differ from those who do not, leading to biased estimates.\n\nEach of these biases stems from the way participants are chosen or how data are measured; they cannot be “averaged out” by larger samples.\n\n\nMitigating bias\nTo minimize bias:\n\nUse probability sampling whenever you want to generalize to a population. Random sampling helps guard against undercoverage and voluntary response bias.\nEnsure your sampling frame matches your target population. Consider oversampling underrepresented groups and weighting responses to reflect their true proportion.\nFollow up with nonresponders and offer multiple modes of participation to reduce nonresponse bias.\nDesign neutral, balanced questions and offer anonymity to reduce measurement and social desirability bias.\nDocument who was invited and who actually participated so you can assess potential biases.\nIn observational studies, adjust for measured differences between participants and nonparticipants using weighting or modeling; but remember that unmeasured biases may remain.\n\n\n\nWorking in JMP Pro 17\nJMP cannot magically remove bias, but it can help you detect and address it:\n\nExplore representativeness. Use Distribution and Graph Builder to compare your sample’s demographics to known population benchmarks. If certain groups are underrepresented, consider weighting or stratified analysis.\nIdentify nonresponse patterns. Create an indicator column for nonresponders and use Fit Y by X or Fit Model to see whether response differs by variables like age or region.\nDesign balanced surveys. Although JMP does not build surveys, you can use scripts or formulas to randomize question order, generate balanced answer scales, and check for item nonresponse.\nApply weights. If you oversample some groups, add a weight column and specify it in analyses (under the red triangle menu) so that estimates reflect the target population.\n\n\n\nRecap\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nCoverage bias\nSystematic error that arises when part of the population is missing from the sampling frame.\n\n\nNonresponse bias\nBias introduced when individuals who do not respond differ meaningfully from those who do respond.\n\n\nVoluntary response bias\nBias caused by allowing people to opt into a survey; respondents with strong opinions dominate the sample.\n\n\nResponse bias\nBias that arises from flaws in the measurement process, such as leading questions or social desirability.\n\n\nSampling error\nNatural variability in statistics from sample to sample; decreases with larger samples.\n\n\nBias\nSystematic error due to design or measurement; does not diminish with larger samples.\n\n\nSurvivorship bias\nFocusing only on observed “survivors” and ignoring those that failed, leading to overly optimistic conclusions.\n\n\n\n\n\nCheck your understanding\n\nA tech company sends an email survey to customers using its premium service. Over half of the recipients do not respond. The company concludes that 85 % of its customers are satisfied.\n\nIdentify two potential sources of bias.\n\nSuggest one way to mitigate each bias.\n\nA political action group hosts an online poll on its website asking visitors whether they support a proposed tax increase. Seventy‑five percent say “no.” What type of bias is most likely, and why does this poll not reflect general public opinion?\nSuppose you draw a simple random sample of 1,000 Baylor students from a roster and send them a questionnaire. Only 200 students respond. How could you use follow‑ups or weighting to reduce bias? Explain your reasoning.\nExplain the difference between sampling error and bias in your own words. Why can a huge sample still give a wrong answer?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n(a)Coverage bias and nonresponse bias. The company sampled only premium users (ignoring basic or free users) and most of the sampled customers did not respond, so respondents may differ from nonrespondents. (b) To reduce coverage bias, draw a sample from all customers or weight responses to reflect the full user base. To reduce nonresponse bias, send reminders, offer incentives or provide alternative modes (e.g., phone, mail).\nThis is voluntary response bias: only visitors who care enough to vote participate, and they may have strong opinions. A poll embedded on a partisan website cannot be generalized because participants are self‑selected and not representative of the broader population.\nThe low response rate introduces nonresponse bias. You could send follow‑up reminders, offer incentives, or contact nonrespondents by phone to increase participation. If demographic data are available for all sampled students, you can apply weights so that the 200 responders reflect the distribution of the 1,000 sampled students (and thus the target population).\nSampling error is the random fluctuation you see from one sample to the next; it decreases with larger samples. Bias is a systematic error built into the design or measurement; it doesn’t shrink with bigger samples. A huge convenience or volunteer sample can still give a wrong answer if it systematically excludes part of the population or asks questions in a biased way.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "02.html#footnotes",
    "href": "02.html#footnotes",
    "title": "2  Collecting Data",
    "section": "",
    "text": "Stang, A., Anastassiou, G., Ahrens, W., Bromen, K., Bornfeld, N., & Jöckel, K. H. (2001). The possible role of radio frequency radiation in the development of uveal melanoma. Epidemiology, 7-12.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "03.html",
    "href": "03.html",
    "title": "3  Describing Data with Tables and Graphs",
    "section": "",
    "text": "3.1 Organizing Categorical Data\nGuiding question: Which graphs work best for categorical data?\nWhen you collect data that fall into groups—like preferred streaming service, political affiliation, or type of pet—the first step is to count how many observations fall into each category. Those counts form the backbone of both tables and graphs for categorical data. In this section we’ll learn how to build simple frequency tables, translate them into proportions or percentages, and organize two categorical variables together in a two‑way table. Along the way we’ll see when different visual summaries make sense and preview bar and pie charts (covered in detail in Section 3.2).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Describing Data with Tables and Graphs</span>"
    ]
  },
  {
    "objectID": "03.html#sec-03_01",
    "href": "03.html#sec-03_01",
    "title": "3  Describing Data with Tables and Graphs",
    "section": "",
    "text": "“The greatest value of a picture is when it forces us to notice what we never expected to see.” – John Tukey\n\n\n\n\nFrequency and relative frequency tables\nA frequency table lists the possible values of a categorical variable and the number of times each value occurs. It’s one of the simplest ways to summarize data, but it packs a punch. Such tables usually have two columns: one for the categories and one for their counts. Creating one involves listing the distinct categories and tallying how many observations fall into each category.\nSometimes absolute counts aren’t enough—especially when comparing samples of different sizes. A relative frequency expresses each category’s count as a proportion of the total. When we add a column of proportions (or percentages) to a frequency table we get a relative frequency table. To compute the relative frequencies, divide each count by the total number of observations. Relative frequencies always add up to 1 (or 100% when expressed as percentages), which makes them handy for comparing distributions across different sample sizes.\n\nAn example: common symptoms in a clinic\nImagine you survey 30 patients at a local clinic about the primary symptom that brought them in. You record four categories: “Headache,” “Back pain,” “Fatigue,” and “Nausea.” We can organize the responses in a simple table of counts and proportions. Below we simulate such a survey and display the results.\n\n\n\n\n\nresponse\nfrequency\nrelative_frequency\n\n\n\n\nBack pain\n9\n0.3000000\n\n\nFatigue\n9\n0.3000000\n\n\nHeadache\n8\n0.2666667\n\n\nNausea\n4\n0.1333333\n\n\n\n\n\n\n\n\n\n\n\n\nThe table lists the four categories in alphabetical order with their counts and relative frequencies. For instance, if 8 of the 30 patients reported “Headache,” the relative frequency of “Headache” is \\(8/30 \\approx 0.27\\). The accompanying bar chart gives a visual sense of the same information: each bar’s height corresponds to a category’s frequency, and the bars are separated to emphasize that the categories have no inherent order. In practice you might reorder the bars to make the graph easier to read—perhaps putting the largest category first.\n\n\n\n\n\n\nPareto charts\n\n\n\n\n\nSometimes you want to highlight the few categories that account for most of the observations. A Pareto chart is a bar chart arranged in descending order of frequency and often paired with a cumulative percentage line. It helps you identify the “vital few and trivial many” in quality control and business applications. Pareto charts are useful when there are many categories and you want to focus attention on the most common causes or responses.\n\n\n\n\n\n\n\n\n\nTip:\n\n\n\n\n\nIn JMP Pro 17 you can create a frequency table by selecting Analyze → Distribution, assigning your categorical variable to the X role, and examining the resulting counts. To add relative frequencies, use the red triangle menu (▸) to choose Display Options → Show Percent. JMP’s Graph Builder will automatically construct a bar chart when you drag a categorical variable to the X‑axis and the count statistic to the Y‑axis.\n\n\n\n\n\n\nTwo‑way (contingency) tables\nWhat if you have two categorical variables and want to see how they interact? A contingency table (also called a two-way table) displays the counts for each combination of levels of the two variables. One variable defines the rows and the other defines the columns. Such tables are the starting point for examining associations and will underlie chi‑square tests in later chapters.\n\nAn example: symptom by age group\nSuppose we collect data on the same symptom question but also record each patient’s age group: “Under 30,” “30–50,” or “Over 50.” We can summarize the joint distribution in a two‑way table.\n\n\n\n\n\nage_group\nBack pain\nFatigue\nHeadache\nNausea\n\n\n\n\n30–50\n3\n4\n5\n3\n\n\nOver 50\n2\n3\n1\n0\n\n\nUnder 30\n4\n2\n2\n1\n\n\n\n\n\nEach cell in the table shows the number of patients who fall into the corresponding combination of age group and symptom. We can also compute row or column relative frequencies to see percentages within each group; for example, dividing each row by its total gives the distribution of symptoms within each age group. Contingency tables allow us to see whether symptom patterns differ across age groups and serve as input for clustered or stacked bar charts (discussed in Section 3.2).\n\n\n\nWhy percentages matter\nBecause categorical variables can have different numbers of levels and sample sizes can vary, relative frequencies are essential for fair comparisons. Reporting only counts can be misleading: 20 supporters of a movie genre in a survey of 50 people represent a large fraction, while 20 supporters in a survey of 500 people represent a much smaller fraction. Percentages standardize the scale.\nWhen displaying percentages, make sure they add to 100%. In a pie chart (a circular graph we’ll describe in the next section), each slice represents a category’s percentage of the whole. Pie charts are useful for showing how the total is divided among categories, but they become cluttered with too many slices. Bar charts are more flexible: you can reorder the bars, show counts or percentages, and compare multiple groups using side‑by‑side or stacked bars.\n\n\nWorking in JMP Pro 17\nIn JMP, tables and graphs for categorical variables are straightforward:\n\nTo create a frequency table, go to Analyze → Distribution, assign your categorical variable to X, and click OK. The report shows counts and percentages; use the red triangle (▸) menu to toggle percentages, counts, or both.\nFor two categorical variables, use Analyze → Fit Y by X and assign one variable to Y and the other to X. Choose Contingency Table from the platform to see the two‑way counts and associated statistics.\nTo visualize categorical distributions, open Graph Builder, drag the categorical variable to the X‑axis, and drop the N summary statistic onto the Y‑axis. You can change the chart type to “Bar” or “Pie.” Dragging a second categorical variable onto the Group drop zone will create clustered or stacked bars.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nFrequency table\nA table that lists each category of a variable and the number of observations in that category.\n\n\nRelative frequency\nThe proportion or percentage of observations in a category, equal to the category’s count divided by the total count.\n\n\nRelative frequency table\nA frequency table with an additional column showing the relative frequency of each category.\n\n\nTwo‑way (contingency) table\nA table that displays the counts for each combination of levels of two categorical variable.\n\n\n\n\n\nCheck your understanding\n\nIn a survey of 80 households, 32 own a dog, 20 own a cat, 12 own both, and the remainder own no pets. Construct a frequency table that shows the number and percentage of households in each pet ownership category (Dog only, Cat only, Both, None). Which visualization—a bar chart or a pie chart—would you choose, and why?\nExplain the difference between a frequency table and a relative frequency table. In what situations is it more informative to look at relative frequencies rather than absolute frequencies?\nWhat is a two‑way (contingency) table? Describe a scenario where a two‑way table could help you explore the relationship between two categorical variables.\nBar charts have spaces between bars and can be drawn in any order. Why are these design choices appropriate for categorical variables? What might go wrong if you drew the bars touching or forced them into a numerical order?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nPet ownership table. The four categories and their counts are: Dog only (20), Cat only (8), Both (12), None (40). The total number of households is 80. The relative frequencies are 25% dog only, 10% cat only, 15% both, and 50% none. A bar chart would be preferable here because it allows you to order the bars from most to least common and makes it easy to compare magnitudes. A pie chart could work for four categories, but it becomes harder to read when slices are similar in size or when there are many categories.\nFrequency vs. relative frequency. A frequency table reports the counts of observations in each category. A relative frequency table adds a column showing the proportion or percentage of observations in each category. Relative frequencies are more informative when comparing groups of different sizes or when you want to focus on the distribution rather than the sample size—for example, comparing survey results from two classes of different sizes.\nContingency table example. A two‑way table displays counts for each combination of levels of two categorical variable. For instance, you could record whether each patient in a clinic has insurance (Yes/No) and whether they arrived on time (On time/Late). A contingency table would show how many patients fall into each combination (e.g., insured & on time, insured & late, uninsured & on time, uninsured & late), helping you explore whether punctuality differs by insurance status.\nDesign choices. Categories have no intrinsic numeric order, so bars in a bar chart can be arranged in any order without misrepresenting the data. Leaving space between bars reinforces that the categories are distinct and unordered. If you drew the bars touching, it might suggest a continuous scale (like a histogram), which could confuse readers. Forcing categories into a numerical order might imply ranking where none exists.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Describing Data with Tables and Graphs</span>"
    ]
  },
  {
    "objectID": "03.html#sec-03_02",
    "href": "03.html#sec-03_02",
    "title": "3  Describing Data with Tables and Graphs",
    "section": "3.2 Bar Charts and Pie Charts",
    "text": "3.2 Bar Charts and Pie Charts\n\n“`Normally if given a choice between doing something and nothing, I’d choose to do nothing. But I would do something if it helps someone do nothing. I’d work all night if it meant nothing got done.” – Ron Swanson\n\nGuiding question: How do we make clear, truthful bar charts and pie charts?\nWhen you’ve tallied the counts of a categorical variable, your next job is to turn those numbers into a picture. Two of the simplest pictures—bar charts and pie charts—seem deceptively alike: each shows categories and their sizes. But as we’ll see, they serve different purposes and come with different design rules.\n\nWhat is a bar chart?\nAs we have already seen in Section 3.1, bar chart displays categories along one axis and uses the length of a bar on the other axis to represent a numerical value. Bars can be vertical (a “column” chart) or horizontal, and they are separated by gaps to emphasise that the categories are discrete. Because humans are good at comparing lengths that share a common baseline, bar charts are our go‑to tool for comparing counts, percentages or other statistics across categories.\nExample: distribution of blood types. Suppose a hospital records the blood type (A, B, AB or O) of 200 randomly chosen donors. The counts are shown in the table below along with a bar chart. Notice that the bars are separated and can be reordered to make patterns easy to see.\n\n\n\n\n\ntype\ncount\nprop\n\n\n\n\nA\n66\n0.330\n\n\nAB\n9\n0.045\n\n\nB\n31\n0.155\n\n\nO\n94\n0.470\n\n\n\n\n\n\n\n\n\n\n\n\nThe vertical bar chart emphasizes how common type O is relative to the others. You could flip the axes to make a horizontal bar chart if your category names are long or if you prefer to read labels on the y‑axis.\nDesign tips for bar charts. A few simple rules help make bar charts honest and clear:\n\nStart the axis at zero. Because bar length encodes value, truncating the axis exaggerates differences.\nKeep consistent spacing. Leave a gap between bars—about half the width of a bar is a good rule of thumb.\nSort deliberately. Arrange bars in a logical order (alphabetical, chronological, or by size) to help the reader scan.\nAvoid clutter and gimmicks. Skip 3‑D effects and decorative icons; they distort perception and add no information.\n\n\n\nWhat is a pie chart?\nA pie chart shows how a total is divided among categories. A circle is divided into slices; each slice represents a category and its angle corresponds to the category’s proportion of the whole. Pie charts are familiar and immediately signal a “part of a whole” story.\nExample: reasons for missing an appointment. A dental clinic tracks why patients miss scheduled cleanings. Out of 100 missed appointments, 50 were due to forgetfulness, 20 to fear, 15 to cost, and 15 to other reasons. A pie chart makes the share of each reason obvious.\n\n\n\n\n\n\n\n\n\nThe slices emphasize that half of the missed appointments were simply forgotten. However, imagine adding three more reasons of similar size. The slices would become crowded and hard to compare. Pie charts work only when the categories sum to a meaningful whole and there are no more than a few slices.\n\n\nWhen to use bar charts vs. pie charts\nAlthough you can plot the same data with either chart, they are not interchangeable. Use a bar chart when you want to:\n\nCompare values across categories or between groups.\nDisplay a statistic that does not sum to a meaningful whole (e.g., average pain scores by medication).\nShow many categories, even if some are small.\n\nUse a pie chart only when:\n\nThe values represent parts of a whole that add up to 100%.\nThe number of categories is small (ideally no more than five).\nYou care more about conveying the big picture of how the whole is divided than about exact comparisons.\n\nWhen you find yourself squinting at a pie chart to see which slice is bigger, switch to a bar chart; our brains judge lengths more accurately than angles.\n\n\nClustered and stacked bar charts\nSometimes you have two categorical variables and want to see how their categories interact. We introduced two‑way tables in Section 3.1; here’s how to graph them.\nA clustered (side‑by‑side) bar chart groups bars for each level of a second variable next to each other so you can compare across groups. For example, imagine you survey 120 patients about how satisfied they were with a new physical therapy program (satisfied, neutral, dissatisfied) and record whether they were in the treatment or control group. A clustered bar chart shows differences in satisfaction between the two groups.\n\n\n\n\n\n\n\n\n\nIn a stacked bar chart, bars for each category are stacked atop one another. This emphasizes the total size of each category but makes it harder to compare the segments across stacks. You might use a stacked chart to show how types of injuries (sprain, fracture, other) contribute to emergency visits across departments; if you convert each bar to 100% of its height, you get a 100% stacked bar chart that highlights composition within each group.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautions with stacked bars\nStacked bars are useful when you care about the total across categories, but they hide patterns in the middle segments. In the last plot, you can easily compare the overall emergency visits across departments and the share of fractures, but it’s harder to compare the “other” injuries across departments because their segments float at different heights. If your goal is to compare subgroups, a clustered bar chart is usually better.\n\n\n\nWorking in JMP Pro 17\nIn JMP, bar and pie charts live in the Graph Builder. Drag your categorical variable to the X‑axis and drop the N or % statistic onto the Y‑axis to create a bar chart. To cluster by a second categorical variable, drop it in the Group or Overlay zone, and choose Bar (Horizontal) or Bar (Vertical) from the chart palette. To stack, use the Stack option in the legend. To make a pie chart, drag the categorical variable to a blank canvas and choose Pie; JMP will automatically convert counts to percentages and label the slices. Use the red triangle (▸) menu to display data labels, reorder slices, or combine small categories into an “Other” slice.\n\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition or note\n\n\n\n\nBar chart\nGraph that displays categories along one axis and uses the length of bars to represent numeric values; great for comparing counts or percentages across categories.\n\n\nPie chart\nCircular chart in which slices represent how a total is divided among categories; appropriate only when values sum to a meaningful whole and the number of categories is small.\n\n\nClustered (side‑by‑side) bar chart\nBar chart where categories are grouped side by side for levels of a second variable; useful for comparing groups across categories.\n\n\nStacked bar chart\nBar chart where bars for each subgroup are stacked; shows composition and totals but makes it harder to compare individual segments.\n\n\n\n\n\n\n\n\n\nCheck your understanding\n\nA clinical trial reports the average pain score (on a 0–10 scale) for three physical therapy programs. Should you use a bar chart or a pie chart? Explain your reasoning.\nA nutritionist surveys 500 patients about their preferred breakfast type: cereal, fruit, eggs, or none. The counts are 150, 120, 80, and 150. Sketch how you would display this information with a pie chart. When might a bar chart be preferable?\nIn a mental health study, participants are classified into stress levels (low, moderate, high) and whether they attended counseling (Yes/No). Which type of bar chart would you use to compare stress levels between counseling and non‑counseling participants? What pattern would indicate that counseling is associated with lower stress?\nA bar chart shows the average number of cavities per patient in three dental clinics: 1.2, 1.3 and 1.4 cavities. The y‑axis starts at 1.0. Explain why this design might mislead and how to fix it.\nGive two reasons why pie charts often make it harder to compare categories than bar charts.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nYou should use a bar chart because the numbers represent average pain scores and do not sum to a meaningful whole. Pie charts imply a part‑to‑whole relationship and would be misleading here.\nThe four breakfast types form parts of the whole sample (500 patients). In a pie chart, the slices would be 30% cereal, 24% fruit, 16% eggs, and 30% none. However, a bar chart may be preferable because it allows you to order the categories from most to least common and makes it easier to compare the cereal and none categories, which are equal in size.\nA clustered (side‑by‑side) bar chart would let you compare the counts (or proportions) of low, moderate, and high stress within the counseling and non‑counseling groups. If counseling is associated with lower stress, you would expect the “low” bar to be taller (or the “high” bar shorter) in the counseling group than in the non‑counseling group.\nStarting the y‑axis at 1.0 truncates the bars and exaggerates small difference. To avoid misleading readers, start the axis at zero. Alternatively, use a dot plot or annotate the differences directly if the differences are small but meaningful.\nFirst, people judge lengths more accurately than angles; in a pie chart it is hard to gauge the exact size of a slice. Second, when slices are similar in size or there are many categories, comparing slices becomes difficult and the chart becomes cluttered. Bar charts avoid these issues by using a common baseline and allowing many bars.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Describing Data with Tables and Graphs</span>"
    ]
  },
  {
    "objectID": "03.html#sec-03_03",
    "href": "03.html#sec-03_03",
    "title": "3  Describing Data with Tables and Graphs",
    "section": "3.3 Organizing Quantitative Data",
    "text": "3.3 Organizing Quantitative Data\n\n“It’s not what you look at that matters, it’s what you see.” - Henry David Thoreau\n\nGuiding question: How should we organize quantitative data?\nCategorical variables live in their own world of discrete groups, but numerical variables—such as heights, blood pressures, cholesterol levels, or reaction times—can take on a continuum of values. To make sense of a list of numbers, we need to group them in a sensible way. In this section we learn how to organize quantitative data into tables that lay the foundation for graphs like histograms, stem‑and‑leaf plots, and dotplots. These tables help us see where the data concentrate, how spread out they are, and how the numbers accumulate.\n\nConstructing frequency distributions\nA frequency distribution for quantitative data is a table that divides the range of the data into non‑overlapping intervals (called classes) and lists the number of observations (the frequency) falling into each class. Each class has a lower limit and an upper limit, the class width is the difference between consecutive lower limits, and the range of the dataset is the difference between the maximum and minimum values. The goal is to choose enough classes to reveal the shape of the distribution without obscuring details.\nTo build a frequency distribution:\n\nDecide on the number of classes. A common guideline is to use between 5 and 20 classes; fewer classes oversimplify the data, while too many classes produce a noisy table.\nCompute the class width. Subtract the minimum value from the maximum value to obtain the range, divide by the chosen number of classes, and round up to a convenient number. This ensures all data points fit into a class without overlap.\nDetermine class limits. Begin at or slightly below the minimum value and add the class width repeatedly to set the lower and upper limits of each class.\nTally the frequencies. Count how many observations fall into each class.\n\nLet’s see these steps with an example from dentistry. Suppose a dental researcher records the number of cavities filled for 40 patients in a clinic over the past month. We want to summarize the distribution of cavities per patient.\n\n\n\n\n\nclass\nfrequency\n\n\n\n\n(-0.5,0.5]\n6\n\n\n(0.5,1.5]\n8\n\n\n(1.5,2.5]\n13\n\n\n(2.5,3.5]\n7\n\n\n(3.5,4.5]\n4\n\n\n(4.5,5.5]\n1\n\n\n(5.5,6.5]\n1\n\n\n\n\n\nThe table lists each class of cavities (0, 1, 2, 3, …) along with the number of patients in that class. For example, if 13 patients had exactly two cavities, the class “(1.5, 2.5]” (interpreted as 2 cavities) would have a frequency of 13. When the data are counts like this, classes of width 1 make sense; for continuous measurements like blood pressure, you would choose wider classes to group the data into ranges.\n\n\nRelative and cumulative frequencies\nRaw counts are helpful, but sometimes we want to know what proportion of observations fall into each class. The relative frequency of a class is the class frequency divided by the total number of observations. If you add successive relative frequencies as you move across the classes, you get the cumulative frequency, which tells you the proportion of observations less than or equal to a given class boundary.\nWe can extend our cavities example to compute relative and cumulative frequencies:\n\n\n\n\n\nclass\nfrequency\nrelative_frequency\ncumulative_frequency\n\n\n\n\n(-0.5,0.5]\n6\n0.150\n0.150\n\n\n(0.5,1.5]\n8\n0.200\n0.350\n\n\n(1.5,2.5]\n13\n0.325\n0.675\n\n\n(2.5,3.5]\n7\n0.175\n0.850\n\n\n(3.5,4.5]\n4\n0.100\n0.950\n\n\n(4.5,5.5]\n1\n0.025\n0.975\n\n\n(5.5,6.5]\n1\n0.025\n1.000\n\n\n\n\n\n\n\n\n\n\n\n\nThe cumulative frequency column increases steadily and reaches 1 at the end of the table. A graph of cumulative frequencies, called an ogive (pronounced “O-jive”), plots the cumulative proportion versus the class.\n\n\nChoosing the number of classes\nSelecting an appropriate number of classes is more art than science, but a few guidelines help. Many textbooks suggest between 5 and 20 classes; an alternative rule of thumb for histograms is to use 5 to 15 bars. As the sample size grows, more classes allow finer detail, but too many classes produce a sparse table. In practice, try several class widths and see which one provides a clear picture of the data. Software like JMP and R will automatically suggest class widths, but you can adjust them manually.\n\n\nWorking in JMP Pro 17\nTo organize quantitative data in JMP:\n\nUse Analyze → Distribution and assign your quantitative variable to the Y‑axis. JMP will display a histogram along with a frequency table showing counts, percentages, and cumulative percentages. The red triangle (▸) menu lets you change the number of bins or show the cumulative distribution.\nTo extract a frequency table without a histogram, go to Analyze → Tabulate, drag your quantitative variable to the Drop Zone for columns, and select a statistic such as N or Percent. JMP’s Tabulate platform can create grouped summaries by another variable (e.g., cavities by gender).\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nClass\nAn interval into which quantitative data are grouped in a frequency distribution; each class has lower and upper limits.\n\n\nFrequency distribution\nA table that lists classes of quantitative data and the number of observations in each class.\n\n\nClass width\nThe difference between consecutive lower class limits, computed by dividing the data range by the number of classes.\n\n\nRelative frequency\nThe fraction or percentage of observations in a class, equal to the class frequency divided by the total sample size.\n\n\nCumulative frequency\nThe running total of frequencies (or relative frequencies) up to a given class, used to construct an ogive.\n\n\nOgive\nA graph of cumulative frequencies or cumulative relative frequencies versus the upper class boundary, useful for identifying percentiles.\n\n\n\n\n\nCheck your understanding\n\nThe heights (in centimeters) of 50 adolescents undergoing orthodontic treatment are recorded. Explain how you would construct a frequency distribution for these heights. How might your choice of the number of classes affect your ability to see the shape of the distribution?\nDifferentiate between a relative frequency distribution and a cumulative frequency distribution. In what situations would you prefer to look at cumulative frequencies instead of simple frequencies?\nSuppose a hospital’s quality control team records the time (in minutes) it takes to triage 100 emergency room patients. The times range from 1 to 28 minutes. If you decide to use eight classes in your frequency distribution, what is the class width? After building the frequency table, describe how you would create an ogive for the data.\nA dataset contains the number of decayed teeth per child in a dental study. Why might a class width of 1 be appropriate for this frequency distribution? When might you choose a wider class width?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nConstructing a frequency distribution. List the minimum and maximum heights, decide on the number of classes (e.g., between 5 and 15), compute the class width by dividing the range by the number of classes, and set class limits starting at or below the minimum. Tally the number of heights falling into each class. Fewer classes smooth out details; more classes reveal fine structure but may result in many empty or low‑frequency intervals. Experimenting with different numbers of classes helps you see the underlying shape.\nRelative vs. cumulative frequency. A relative frequency distribution reports the proportion of observations in each class. A cumulative frequency distribution adds the proportions successively to show the total proportion up to each class. Cumulative frequencies are useful when you care about percentiles or thresholds—for example, determining the percentage of patients whose heights are below a certain value.\nCalculating class width and drawing an ogive. The range is 28 − 1 = 27 minutes. Dividing by eight classes gives 3.375 minutes; rounding up to 4 minutes yields a class width of 4. You would start your first class at or below 1 minute and create successive 4‑minute intervals (e.g., 0–4, 4–8, … ). After tallying frequencies, compute cumulative frequencies and plot them against the upper class boundaries to form the ogive.\nChoosing class width for count data. Decayed teeth are counted in whole numbers, so using a class width of 1 preserves each distinct count (0, 1, 2, …) and yields an easy‑to‑interpret distribution. If the counts range widely or if some counts are rare, a wider class width (e.g., grouping 4–6 teeth together) could reduce sparsity and simplify the table.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Describing Data with Tables and Graphs</span>"
    ]
  },
  {
    "objectID": "03.html#sec-03_4",
    "href": "03.html#sec-03_4",
    "title": "3  Describing Data with Tables and Graphs",
    "section": "3.4 Histograms, Stem‑and‑Leaf Plots, and Dotplots",
    "text": "3.4 Histograms, Stem‑and‑Leaf Plots, and Dotplots\nGuiding question: What’s the right way to visualize quantitative data?\nOnce quantitative data are grouped into classes, we can visualize the distribution using several complementary plots. Histograms show how the data are distributed across intervals; stem‑and‑leaf plots preserve individual values while offering a quick visual summary; and dotplots display each observation as a dot. Choosing the right tool depends on your sample size, measurement scale, and the story you want to tell.\n\nHistograms\nA histogram is a bar‑like graph in which adjacent rectangles represent the classes of a quantitative variable. The horizontal axis is measured in the units of the data, and the vertical axis shows the frequency or relative frequency in each class. Unlike bar charts for categorical data, the bars in a histogram touch to indicate that the underlying scale is continuous. Histograms are excellent for revealing the shape, center, and spread of a distribution.\nTo construct a histogram:\n\nCreate a frequency distribution with classes of equal width (see Section 3.3).\nOn the horizontal axis, draw the class boundaries; on the vertical axis, mark the frequencies or relative frequencies.\nDraw adjacent rectangles whose widths correspond to the class widths and whose heights correspond to the class frequencies. For a density histogram, scale the heights so that the area of each rectangle equals the class’s relative frequency; the total area of the histogram equals one】.\n\nThe number of bins (rectangles) influences the appearance of the histogram. A rule of thumb is to choose between 5 and 15 bins; using more bins adds detail but may create spiky plots. Avoid decorative 3‑D effects and keep the bars aligned on a common baseline.\n\nExample: Resting heart rates\nConsider a study of the resting heart rates (in beats per minute) of 100 individuals undergoing physical therapy.\n\n\n\n\n\n\n\n\n\nThe histogram reveals the overall shape, identifies any outliers, and helps us decide whether certain distribution assumptions might be reasonable. In JMP, you can create a histogram by assigning the variable to Y in the Distribution platform; use the red triangle (▸) menu to adjust the binning.\n\n\n\nStem‑and‑leaf plots\nA stem‑and‑leaf plot divides each observation into a “stem” (the leading digits) and a “leaf” (the last digit. You list the possible stems in order and write the leaves next to their stems. This simple display retains the actual data values while giving a visual impression of the distribution. It’s particularly useful for small to moderate sample sizes (roughly under 100 observations). Unlike histograms, a stem‑and‑leaf plot shows every data point, so you can reconstruct the original dataset if needed.\nFor example, suppose an orthopedic clinic records the times (in days) for 20 patients to regain full range of motion after knee surgery. We can construct a stem‑and‑leaf plot as follows:\n\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  0 | 88899\n  1 | 000002224\n  1 | 5578\n  2 | 14\n\n\nIn a manual stem‑and‑leaf plot, the stems would be 2, 3, 4, … representing tens of days, and the leaves would be the units digit. Many statistical software packages split each stem into two rows—one for leaves 0–4 and one for leaves 5–9—to provide more detail. Reading the plot, you can quickly see that most patients recover in the 10–14 day range and that the longest recovery took about 24 days. Because the stems are ordered, the plot shows the shape of the distribution and highlights extremes.\n\n\nDotplots\nA dotplot places a dot above a number line for each observation; if multiple observations share the same value, the dots stack vertically. Dotplots are easy to construct and interpret for small datasets and can handle discrete and continuous variables. They show clusters, gaps, and outliers while preserving individual data points.\nContinuing our dental theme, imagine researchers record the number of clinic visits per patient during a year for 25 orthodontic patients. A dotplot can reveal whether most patients require only a few visits or if some need frequent check‑ups.\n\n\n\n\n\n\n\n\n\nThe plot makes it clear that most patients had between one and three visits, while a few had zero or five visits. Dotplots are an excellent choice when exact values matter, such as showing distribution of test scores or daily step counts.\n\n\nChoosing the right display\n\nHistograms reveal the overall shape of a distribution and are most useful for moderate to large samples. They sacrifice individual data values to provide a smooth picture of the distribution.\nStem‑and‑leaf plots preserve actual data values and work well for small to moderate samples. They quickly show the shape and allow you to recover the raw data.\nDotplots also preserve individual values and are easy to interpret for small datasets; they can handle discrete counts and continuous measurements.\n\n\n\nWorking in JMP Pro 17\nIn JMP:\n\nTo create a histogram or stem‑and‑leaf plot, use Analyze → Distribution. For stem‑and‑leaf plots, JMP automatically splits stems if needed and prints the “leaf unit.” The red triangle (▸) menu lets you toggle between histogram and stem‑and‑leaf views.\nTo build a dotplot, open Graph Builder, drag your quantitative variable to the X‑axis, and choose “Dot Plot” from the element palette.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nHistogram\nA graph of adjacent rectangles representing classes of a quantitative variable; the horizontal axis is numeric and the bars touch.\n\n\nRelative frequency histogram\nA histogram in which the height of each bar shows the relative frequency (proportion) in that class, so the sum of the bar heights equals 1.\n\n\nStem‑and‑leaf plot\nA display that splits each data value into a stem (leading digits) and a leaf (trailing digit), preserving the original data.\n\n\nDotplot\nA plot that places a dot above each data value on a number line, with stacked dots representing repeated values.\n\n\n\n\n\nCheck your understanding\n\nExplain why a histogram is drawn with adjacent bars touching. How does this design reinforce the type of variable being plotted?\nFor a set of 15 recovery times (in days) after a dental procedure, would you prefer a histogram, a stem‑and‑leaf plot, or a dotplot? Justify your choice.\nIn a stem‑and‑leaf plot, what do the stems and leaves represent? Why might you choose to split the stems into two rows for certain datasets?\nSuppose a physical therapist records a patient’s range‑of‑motion angle (in degrees) each week for three months. Which type of plot would you use to look for improvements over time? What features would you look for in the plot?\nThe weekly number of cavity fillings at a dental clinic over a year shows a seasonal pattern. Describe how you would visualize this data and what patterns you might expect to see.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nWhy bars touch. The bars in a histogram are adjacent because the underlying variable is continuous, and there are no gaps between the classes. Drawing the bars together emphasizes that each class covers an interval on the number line; a gap would incorrectly suggest a break between values.\nChoosing a plot for 15 observations. For a sample of 15 recovery times, a stem‑and‑leaf plot or dotplot would preserve the individual values and make it easy to see exact times. A histogram could also work, but with so few observations its appearance would depend heavily on the chosen bin width. Stem‑and‑leaf plots are especially useful here because they reveal the shape and extremes while retaining the data.\nInterpreting stems and leaves. In a stem‑and‑leaf plot, the stem contains the leading digits of each number and the leaf is the final digit. Splitting stems into two rows—one for leaves 0–4 and one for leaves 5–9—provides more detail and smooths out long runs of leaves, making the distribution easier to read.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Describing Data with Tables and Graphs</span>"
    ]
  }
]