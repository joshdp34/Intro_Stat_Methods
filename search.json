[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 2381 Introductory Statistical Methods",
    "section": "",
    "text": "Introduction\nThese are the lecture notes for STA 2381 - Introductory Statistical Methods.\nPrerequisites: None\n\nCourse Description:\nParametric statistical methods. Topics range from descriptive statistics through regression and one-way analysis of variance. Applications are typically from biology and medicine. Computer data analysis is required.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01.html",
    "href": "01.html",
    "title": "1  Introduction to Statistics",
    "section": "",
    "text": "1.1 What is Statistics?\nSuppose someone shows you a coin. It is a typical coin like the one you see below.\nAlso suppose this individual claims that this coin will land on heads 90% of the time when it is flipped.\nThis claim seems unbelievable. A typical coin lands on heads bout 50% of time if the coin is fair.\nWhat would you say to this individual who is making this claim?\nYou would likely say Show me some evidence.\nSo this individual flips the coin and it lands on heads.\nThis result is not surprising if the coin is fair or if the claim of 90% is true. If the coin landed on tails then you would likely have some doubt of the claim. Since we have heads, we really don’t have enough information to confirm the 90% claim. So we ask for the coin to be flipped again.\nSuppose it flipped to a heads again. So now we have two heads in two flips of this coin.\nIs this enough evidence to convince you of the 90% claim?\nTo answer that question, you should consider the chances of getting this result under both scenarios:\nAlthough the 90% claim has the higher chance of getting two heads in two flips, we cannot rule out the fair coin scenario. A chance of 25% is not high, but it is not impossible. If fact, we would not consider a result with chances of 25% to be rare or unusual if that result happens.\nWe need more information. Suppose the coin if flipped 20 times. Here are the results:\nIn these 20 flips, we have 11 heads and 9 tails.\nWith this information, we do you think about the 90% claim? Let’s compare what we have seen in this result compared to what we expected to see under the two scenarios.\nThese results appear to show the 90% claim is not true. But let’s look at another set of 20 flips.\nIn this set of 20 flips, we have 17 heads and 3 tails including 14 heads in a row.\nWithout even considering the chances under the two scenarios, you are likely more convinced of the 90% claim given these results. Let’s look at the chances anyways:\nThe evidence is clearly in favor of the 90% claim.\nHere’s the catch: that second set of 20 flips was not made up. They are actually the results of the coin toss of Super Bowls 31-50. If we let heads represent the NFC team winning the coin toss and tails represent the AFC team winning the toss, then the second set of flips would be the results.\nDoes this seem impossible? Do we think the Super Bowl coin was somehow weighted to give the NFC team an advantage? Certainly not. In fact, each Super Bowl uses a different commemorative coin for the coin toss.\nWhat this illustrates is rare things happen. Even when a process is completely fair and governed only by chance, long streaks and surprising outcomes can still occur. That means that we cannot judge claims based on a few eye-catching outcomes or coincidences alone. Instead, we must step back and ask deeper questions:\nIn other words, real-world questions rarely come with certainty. Instead, they come with uncertainty, variability, chance, and incomplete information. We cannot rely on intuition alone. We need a disciplined way to use data to evaluate claims, measure evidence, and make informed decisions even when we cannot know the truth with absolute certainty.\nThat disciplined way of thinking is what statistics is about.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "01.html#what-is-statistics",
    "href": "01.html#what-is-statistics",
    "title": "1  Introduction to Statistics",
    "section": "",
    "text": "“The non-scientist in the street probably has a clearer notion of physics, chemistry and biology than of statistics, regarding statisticians as numerical philatelists, mere collector of numbers.” - Stephen Senn, Dicing with Death: Chance, Risk and Health\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nIf the coin if fair, the chances of getting two heads in two flips is 25%.\nIf the 90% claim is true, the chances of getting two heads in two flips is 81%1.\n\n\n\n\n\n\n\nIf the coin if fair, we expect to see around 10 heads. We saw 11 heads which is close.\nIf the 90% claim is true, we expect to see around 18 heads. We saw 11 heads which is far off. In fact, the chances of seeing at most 11 heads under this scenario is less than 0.01%.\n\n\n\n\n\n\nIf the coin if fair, the chances of getting at least 17 heads out of 20 flips is around 0.13%.\nIf the 90% claim is true, the chances of getting at least 17 heads out of 20 flips is around 87%.\n\n\n\n\n\nAFC\nNFC\nNFC\nNFC\n\nNFC\nNFC\nNFC\nNFC\n\nNFC\nNFC\nNFC\nNFC\n\nNFC\nNFC\nNFC\nAFC\n\nAFC\nNFC\nNFC\nNFC\n\n\n\n\n\nWhat outcomes would we expect to see if a claim were true?\nWhat outcomes would we expect to see if the claim were not true?\nAre the results we observed consistent with ordinary chance variation, or are they so unusual that they suggest something else is going on?\n\n\n\n\nWhat is “statistics,” exactly?\nStatistics is the science of collecting, organizing, analyzing, and interpreting data to make decisions or draw conclusions. It’s not just about numbers-it’s about what those numbers tell us.\nIf Statistics concerns data, then we should define “data.” First, note that “data” is a plural word. “Datum” is singular, although it is common to hear someone refer to “data” in the singular. A “datum” is a piece of information or fact. So “data” is a collection of facts or information we collect. That could mean\n\nthe amount of profit a company makes,\nthe growth of plants under some conditions,\nor how many people voted in an election.\n\nA helpful way to organize the subject of Statistics is to distinguish two complementary activities:\n\nDescriptive statistics help us summarize and visualize what we observed-think graphs, tables, and numerical summaries. The goal is clarity.\nInferential statistics help us generalize from a sample to a broader group (or process) and quantify our uncertainty about that generalization. The goal is justified conclusions.\n\nEven in a short conversation about data, you’ll hear a few recurring ideas:\n\nA population is the full set of people, items, or occasions we care about (all Baylor first-years this fall, all batteries produced this week).\nA sample is the subset we actually observe.\nA parameter is a (usually unknown) number that describes a population (the true average battery life, for example).\nA statistic is a number we compute from a sample (the sample’s average battery life) that we use to learn about the parameter.\n\nWe’ll study these terms in more detail soon; for now, hold on to the big idea: we summarize what we see (description) and we reason beyond what we see (inference).\n\n\nWhy decisions need both description and inference\nSuppose a clinic tests a new flu-prevention program among 200 volunteer patients. A month later, 18% of the “usual care” group got the flu, compared to 12% of the “new program” group.\nDescriptively, the new program looks better. Inferentially, we ask: could this gap be due to chance? If we ran the study again with different patients, might the difference shrink or flip? Statistics gives us a way to quantify that uncertainty and decide what to do next.\nA similar story plays out in business A/B tests, manufacturing quality checks, and sports analytics. The descriptive picture tells us what happened in the data; inference tells us how strongly that evidence supports a decision.\n\n\n\n\n\n\n\nHow we’ll work in this course\nOur general workflow:\n\nStart with a clear question and name the observational units (what a single case is) and variables (what we record about each case).\nDecide how the data were or will be collected (survey, experiment, database pull).\nUse descriptive statistics and graphics to get oriented.\nBuild an inferential argument when you need to generalize or compare.\nCommunicate a conclusion in context-what it means, what it doesn’t, and what to do next.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nStatistics\nThe discipline of learning from data to describe patterns and make decisions under uncertainty.\n\n\nData\nRecorded information about cases (people/items/occasions) used as evidence for questions of interest.\n\n\nDescriptive statistics\nMethods for summarizing and visualizing what was observed (tables, graphs, numerical summaries).\n\n\nInferential statistics\nMethods for generalizing from a sample to a population and quantifying uncertainty.\n\n\n\n\n\n\n\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nIn your own words, how is describing data different from inferring from data? Give a short example for each.\n\n\n\nA streaming service tests two home-page designs on 5,000 visitors each. Version B produces a 0.6 percentage-point higher click-through rate than Version A. What questions would you ask before recommending the company switch to Version B?\n\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nDescription vs. inference. Description summarizes what happened in the observed data (e.g., “The median wait time yesterday was 11 minutes.”). Inference uses the sample to say something about a broader group or process, with uncertainty (e.g., “We estimate the typical wait time for all days like yesterday is 11 minutes, with margin of error ±2 minutes.”).\n\n\n\nHow were visitors assigned to versions (randomly)? Were there differences in traffic sources or device types? Is the effect stable over time? What outcome do we ultimately care about (sign-ups, retention), and does the change affect it?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "01.html#populations-and-samples",
    "href": "01.html#populations-and-samples",
    "title": "1  Introduction to Statistics",
    "section": "1.2 Populations and Samples",
    "text": "1.2 Populations and Samples\n\n“To understand God’s thoughts, we must study statistics, for these are the measures of his purpose.” - Florence Nightingale\n\nWhen you start any statistical investigation, the first two things to name are the population and the sample. The population is the full group or process you want to understand; the sample is the smaller set of cases you actually observe. Most of the time we can’t measure everyone or everything, so we sample wisely and then use statistics to bridge from the sample back to the population.\n\nDefining the population\nAt the heart of every statistical study is a population-the full set of individuals, objects, or events about which we want to learn. Sometimes this population is very concrete and well defined, such as “all tires produced by Line A during the third shift this week.” Other times it is more abstract or conceptual, such as “all patients with this condition who would receive this treatment under similar circumstances.” In both cases, the population exists independently of the data you happen to collect.\nTwo important refinements help clarify what we really mean by “the population” in practice:\n\nThe target population is the group you ultimately care about and want to draw conclusions about. This is the population referenced in your research question or claim. For example, a university might care about all incoming first-year students, or a manufacturer might care about all tires it produces under normal operating conditions.\nThe accessible population is the portion of the target population that you can realistically reach or observe. This group is often represented by a sampling frame-a list, database, or mechanism that allows you to identify and select individuals for the study.\n\nIn an ideal world, the accessible population would perfectly match the target population. In reality, they often differ. Practical constraints-time, cost, geography, incomplete records, or nonresponse-mean that some members of the target population are harder (or impossible) to include.\nClarity about this distinction matters. Suppose your target population is “all Baylor first-year students this fall.” If your sampling frame consists of “students who attend welcome week,” you may systematically exclude commuters, students with work or family obligations, or those who arrive late to campus. If these excluded students differ in meaningful ways-academically, socially, or financially-then your sample may not represent the target population well. This mismatch introduces bias, meaning your results may consistently overestimate or underestimate the quantity you are trying to measure.\nBeing explicit about the target population, the accessible population, and the sampling frame forces you to confront these limitations up front. It also helps you interpret results honestly: conclusions drawn from the accessible population should not automatically be generalized beyond it without careful justification. In statistics, knowing who your data represent is just as important as knowing what the data say.\n\n\nWhat counts as a sample?\nA sample is the subset of the population from which data are actually collected. It consists of the specific individuals, objects, or observations you measure in your study. While the population defines what you care about, the sample defines what you observe. Everything that follows in a statistical analysis-summary statistics, graphs, models, and conclusions-is based on the sample.\nWe will study how to select samples carefully in Chapter 2. For now, the key idea is what a good sample accomplishes. A good sample represents the population well enough that quantities computed from the sample-such as means, proportions, or regression coefficients-provide useful information about the corresponding population quantities, called parameters. When this happens, the sample serves as a stand-in for the population.\nImportantly, a sample does not need to “look like” the population in every detail. Rather, it needs to capture the relevant sources of variability in the population without systematically favoring certain groups or outcomes over others. Poor samples can lead to misleading conclusions, not because the calculations are wrong, but because the data themselves fail to reflect the population of interest.\nA special case is a census, in which you attempt to measure every unit in the population. In principle, a census eliminates sampling variability, since there is no subset involved. In practice, however, censuses are rare. They are often expensive, time-consuming, and logistically complex. Even large-scale censuses can suffer from missing data, nonresponse, or measurement error, which means they are not immune to bias.\nFor these reasons, sampling is the workhorse of statistics. Carefully designed samples allow us to learn about large or even infinite populations using a manageable amount of data. The central challenge of statistical inference is to quantify how much uncertainty remains when we use a sample to speak about a population-and that challenge begins with understanding exactly what our sample represents.\n\n\nParameters and statistics\nA parameter is a numerical summary that describes a characteristic of the entire population. Examples include the true mean income of all households in a city, the proportion of all voters who support a particular candidate, or the standard deviation of tire lifetimes produced by a factory. Parameters are fixed values-they do not change-but they are usually unknown, because measuring an entire population is rarely feasible.\nA statistic, by contrast, is a numerical summary computed from a sample. Sample means, sample proportions, and sample standard deviations are all statistics. Unlike parameters, statistics depend on which individuals happen to be included in the sample, and therefore they can change from sample to sample. In practice, statistics are the quantities we can actually calculate, and we use them as estimates of the corresponding population parameters.\nThis distinction is central to statistical thinking. When you report a sample mean, you are not claiming it is the population mean. Instead, you are using it as your best guess, based on the available data. Different samples from the same population would produce different statistics, even if the sampling method were perfectly fair.\nThat inevitable variation from sample to sample is called sampling error. Sampling error does not mean that a mistake was made or that the data were collected incorrectly. Rather, it reflects the natural randomness involved in observing only a subset of the population. Even well-designed samples are subject to sampling error.\nA major goal of statistics is to measure and communicate this uncertainty. Later chapters will introduce tools such as margins of error, confidence intervals, and hypothesis tests, which allow us to quantify how close a statistic is likely to be to the true parameter. These tools do not eliminate uncertainty, but they help us reason carefully about what our data can-and cannot-tell us about the population.\n\n\n\n\n\n\n\n\n\nWhy sampling works (and when it doesn’t)\nSampling works when your sample is representative of the population and your measurement is trustworthy. It struggles when parts of the population are systematically excluded (coverage problems), when participation differs by outcome (nonresponse), or when the way you select units is related to the outcome (selection effects). We’ll study these threats-and how to reduce them-next chapter.\n\n\nA quick illustration: samples differ, the goal doesn’t\nThe illustration below shows a fundamental idea in statistics: different samples from the same population give different answers, but they are all trying to estimate the same underlying truth.\nIn this simulation, we imagine a population where the true mean is known to be 10 and the standard deviation is 2. From this population, we repeatedly take random samples and compute the sample mean each time. We do this for several different sample sizes, and then look at how the sample means behave.\nEach histogram shows the distribution of sample means from 1,000 different samples of the same size. The dashed vertical line marks the true population mean of 10.\n\n\n\n\n\n\n\n\n\nWhen the sample size is small (for example, (n = 20)), the sample means vary noticeably from one sample to another. Some are below 10, some are above, and a few are fairly far away. This variability is not a flaw in the method-it is the unavoidable consequence of working with limited data.\nAs the sample size increases (to (n = 50) and then (n = 200)), two things become apparent:\n\nThe sample means continue to center around the true mean of 10.\nThe spread of the sample means shrinks. Larger samples produce more stable, less “wiggly” estimates.\n\nThis shrinking spread reflects a reduction in sampling error. With more data, random fluctuations tend to cancel out, and the statistic becomes a more precise estimate of the parameter. Importantly, the target never changes: all of these samples-large or small-are aiming to learn about the same population mean.\nThis example captures the intuition behind statistical inference. We accept that individual samples differ, but we rely on probability and repeated-sampling logic to understand how much they differ and how close our estimates are likely to be to the truth. Later chapters will formalize this intuition using sampling distributions, standard errors, and confidence intervals, but the core idea is already visible here: uncertainty decreases with sample size, even when the goal stays the same.\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nPopulation\nThe full group or process you want to understand.\n\n\nTarget population\nThe group you truly care about answering a question for.\n\n\nAccessible population\nThe portion of the target population you can practically reach.\n\n\nSampling frame\nThe list or mechanism from which the sample is drawn.\n\n\nSample\nThe subset of units you actually observe and measure.\n\n\nCensus\nAn attempt to measure every unit in the population.\n\n\nParameter\nA numerical characteristic of a population (e.g., \\(\\mu, p\\)).\n\n\nStatistic\nA numerical summary from a sample (e.g., \\(\\bar{x}, \\hat{p}\\)) used to estimate a parameter.\n\n\nSampling error\nNatural variation in a statistic from sample to sample.\n\n\nBias\nSystematic deviation caused by design, coverage, or selection issues.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nA battery plant tests 80 batteries from today’s production line and finds an average life of 7.8 hours. Identify the population, sample, parameter, and statistic.\nYour target is “all Baylor first-year students this fall.” You gather data from an email list of students who signed up for an early-interest program. What is the sampling frame? Name a potential source of bias.\nA marketing team uses comments on their Instagram post to judge product satisfaction among all customers. Explain why this may not represent the population.\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nPopulation: all batteries produced today. Sample: the 80 tested batteries. Parameter: the true mean life of all batteries produced today. Statistic: the sample mean of 7.8 hours.\nSampling frame: students on the early-interest email list. Potential bias: students who didn’t sign up (e.g., commuters, late enrollees) may be under-represented, creating coverage/selection bias.\nInstagram commenters are a self-selected subset; satisfied or dissatisfied users may be more likely to comment, and customers not on Instagram are excluded-both threaten representativeness.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "01.html#variables-and-types-of-data",
    "href": "01.html#variables-and-types-of-data",
    "title": "1  Introduction to Statistics",
    "section": "1.3 Variables and Types of Data",
    "text": "1.3 Variables and Types of Data\n\n“Not everything that can be counted counts, and not everything that counts can be counted.” - Albert Einstein\n\nEvery data table is built from two essential ingredients: cases and variables. The cases form the rows of the table and represent the individual observational units-people, objects, events, or time points-on which data are collected. The variables form the columns and describe characteristics recorded for each case. Thinking clearly about both is crucial: a dataset only makes sense when you know what each row represents and what information each column contains.\n\nWhat is a variable?\nA variable is any characteristic that can take different values across cases. For example, resting heart rate may vary from person to person, major differs across students, state of residence varies geographically, and did the patient improve? can differ from one patient to the next. If a characteristic does not vary-if it takes the same value for every case-it does not function as a variable and typically provides no information for analysis.\nVariables always live in context. For any dataset, we should be able to answer two basic questions:\n\nWhat does one row represent? This is the observational unit (for example, one student, one hospital visit, or one manufactured item).\nWhat does each column represent, and how was it measured or recorded? For numerical variables, this includes the unit of measurement (beats per minute, dollars, hours, miles, etc.).\n\nContext matters because the same-looking numbers can mean very different things depending on how they were collected. A column of values like 72, 68, 75 might represent heart rates, exam scores, or daily temperatures, and each interpretation leads to different conclusions and appropriate analyses. Even non-numeric variables require clear definitions: knowing whether “major” refers to a declared major at enrollment or at graduation can change how the data should be interpreted.\nBeing explicit about variables and their meaning lays the groundwork for everything that follows. The choice of summary statistics, graphs, and statistical models depends not just on the values in a column, but on what kind of variable it is and what it represents.\n\n\nThe two big families\nMost variables encountered in practice fall into two broad families. Different textbooks use slightly different names-qualitative vs. quantitative, categorical vs. numerical-but the underlying ideas are the same. The key question is whether the values represent group membership or measured quantities.\n\nCategorical variables\nA categorical variable assigns each case to a group, category, or label. The values describe what kind of thing each case is, not how much of something it has. Because the categories are labels, performing arithmetic operations on them does not make sense-even if the categories are coded using numbers behind the scenes.\nFor example, coding blood types as 1, 2, 3, and 4 does not imply that type 4 is “twice” type 2 or that averaging blood types is meaningful. The numbers are merely labels.\nCategorical variables commonly appear in three forms:\n\nNominal variables consist of categories with no natural ordering. One category is not inherently “higher” or “lower” than another. Examples include blood type, home state, eye color, device brand, or political party. For nominal variables, we typically summarize data using counts, proportions, or percentages and visualize them with bar charts or pie charts.\nOrdinal variables have categories with a meaningful order, but the spacing between categories is uneven or unknown. Likert-scale responses such as “Strongly disagree,” “Disagree,” “Neutral,” “Agree,” and “Strongly agree” fall into this category, as do rankings like race finish places (1st, 2nd, 3rd) or course grades (A, B, C, D, F). While the order carries information, it is generally unsafe to treat the differences between categories as numerically equal.\nBinary variables are categorical variables with exactly two categories. Common examples include yes/no, success/failure, pass/fail, or disease/no disease. Binary variables are especially important in statistics because they often encode outcomes of interest and are closely connected to proportions and probabilities.\n\nRecognizing a variable as categorical-and identifying which subtype it belongs to-guides nearly every analytical choice, from the graphs you make to the summary measures you report and the statistical methods you apply.\n\n\n\n\n\n\nLikert Scale\n\n\n\n\n\nA Likert Scale is a common psychometric scale used in questionnaires to measure attitudes, opinions, or behaviors. It presents a statement and asks respondents to indicate their level of agreement or frequency on a symmetric, typically 5- or 7-point scale. The options range from one extreme (e.g., “Strongly Disagree”) to the opposite extreme (e.g., “Strongly Agree”), often including a neutral or middle point.\nExample:\n“How satisfied are you with our service?”\n\nStrongly Disagree\nDisagree\nNeutral\nAgree\nStrongly Agree\n\n\n\n\n\n\nQuantitative variables\nA quantitative variable records numerical values for which arithmetic operations—such as addition, subtraction, averaging, and differences—are meaningful. These variables represent how much, how many, or how long of something, and they arise from counting or measuring. Because the numbers carry inherent magnitude, quantitative variables support a wide range of numerical summaries and graphical displays.\nOne important subtype of quantitative variables is:\n\nDiscrete variables, which arise from counting. Discrete variables take on distinct, separated values—typically whole numbers—with gaps between possible values. Examples include the number of emergency room visits in a year, the number of defects on a manufactured item, the number of children in a household, or the number of emails received in a day. For these variables, values like 2.5 or 3.7 are not meaningful because you cannot have a fractional count of an event.\n\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nIn math, the possible values of discrete data form what is known as countable set. This means the values form a collection (set) of values that can be put into a one-to-one correspondence with the natural numbers. In other words, a set is countable if you can list its elements in a sequence, even if the list is infinitely long.\nThere are two type of countable sets:\n\nFinite Countable Set\n\nThe set has a limited number of elements.\nExample: \\(\\{2, 4, 6, 8\\}\\) - there are exactly 4 elements.\n\nCountably Infinite Set\n\nThe set has infinitely many elements, but you can still list them in an ordered sequence.\nExample: The set of natural numbers \\(\\{1, 2, 3, 4, ...\\}\\)\nEven the set of all integers \\(\\{..., -2, -1, 0, 1, 2, ...\\}\\) is countable - you can reorder them as \\(0, 1, -1, 2, -2, 3, -3, ..\\) and still list them one by one.\n\n\n\n\n\n\nContinuous variables arise from measurement rather than counting. In principle, they can take on any value within an interval on the number line, limited only by the precision of the measuring instrument. Examples include blood pressure, time to failure, height, weight, temperature, and distance. For these variables, values such as 172.4, 172.43, or 172.431 are all meaningful, even if we choose to record them with fewer decimal places.\n\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nThe possible values of continuous data form an uncountable set. A set is uncountable if there’s no way to list all its elements in a sequence, even infinitely.\n\nExample: Real numbers between 0 and 1 - there are infinitely more of these than there are natural numbers.\nIn set notation, an example of an uncountable set is \\(\\{x: x\\ge 3\\}\\).\n\n\n\n\nTo decide if a variable is discrete or continuous, first think of an interval of possible values of the variable2. Can you count how many values are in that interval? If so, then it is discrete, if not, then it is continuous. In practice, we can simplify this further: counts are discrete, measurements are usually continuous.\n\n\n\nMeasurement scales you’ll hear about\nIn addition to the broad distinction between categorical and quantitative variables, you may encounter another classification system based on measurement scales. These scales—nominal, ordinal, interval, and ratio—describe what kinds of comparisons and arithmetic operations are meaningful for a variable. While this taxonomy appears frequently in introductory texts, its real value lies in guiding interpretation, not in memorization. We have already defined nominal and ordinal scales above.\n\nInterval scales are numeric scales where differences are meaningful, but the zero point is arbitrary. Temperature measured in degrees Fahrenheit or Celsius is the classic example. A difference of 10°F represents the same change anywhere on the scale, but ratios do not carry meaning: 40°F is not “twice as hot” as 20°F because zero does not represent the absence of temperature.\nRatio scales are numeric scales with a true zero, representing the absence of the quantity being measured. On these scales, both differences and ratios are meaningful. Examples include temperature measured in Kelvin, as well as length, mass, time, age, and income. Statements like “10 minutes is twice as long as 5 minutes” or “this object weighs three times as much as that one” are valid precisely because the zero point has real meaning.\n\nYou do not need to memorize the full taxonomy to do good statistics. The practical takeaway is simpler: always think about what comparisons make sense for your variable. In particular, be cautious about interpreting ratios when working with interval scales, and ask yourself whether subtraction, averaging, or scaling by a factor has a clear real-world meaning. Keeping this perspective front and center helps prevent overinterpretation and ensures that numerical summaries align with the nature of the data.\n\n\n“Identifier” and date/time variables\nNot every column in a dataset is meant to be analyzed numerically. Some variables exist primarily to organize, link, or track observations, and treating them incorrectly can lead to serious analytical mistakes.\n\nIdentifier (ID) variables are used to uniquely label cases. Examples include student ID numbers, order or invoice numbers, patient IDs, license plate numbers, and ZIP codes. Although these values are often composed of digits, they are labels, not quantities. Arithmetic operations on ID variables are meaningless: averaging student ID numbers or computing differences between order numbers tells you nothing substantive about the data. Their purpose is to ensure that each row can be uniquely referenced, merged with other datasets, or traced back to its source—not to summarize or model.\n\nA common error is to accidentally treat ID variables as quantitative simply because they are stored as numbers. Good practice is to explicitly mark them as identifiers (or convert them to text) so they are not mistakenly included in summaries, plots, or statistical models.\n\nDate and time variables record when an observation occurred. These variables are more subtle because they can legitimately play different roles depending on the question being asked. On one hand, dates and times can be treated as categorical—for example, grouping observations by day of the week, month, or season. On the other hand, they can be treated as quantitative by measuring elapsed time, such as minutes since the start of an experiment, days since enrollment, or time to failure.\n\nBecause date/time variables are flexible, it is essential to be explicit about how they are being used. Are you comparing weekdays to weekends, or are you modeling a trend over time? The same column can support both approaches, but the analysis, visualization, and interpretation differ substantially.\nBeing deliberate about identifiers and date/time variables helps ensure that each column is used appropriately. It prevents nonsensical calculations, clarifies analytical intent, and keeps the focus on variables that genuinely carry information about the phenomenon under study.\n\n\nWorking in JMP\nJMP keeps two concepts straight for each column:\n\nData Type (what the values are): Numeric, Character, or Date/Time.\nModeling Type (how you plan to analyze them): Continuous, Nominal, or Ordinal.\n\nDouble-click a column header (or right-click → Column Info) to set these. Typical mappings:\n\nCharacter + Nominal → categorical labels (e.g., “TX”, “CA”).\nNumeric + Continuous → quantitative measures (e.g., weight, time).\nNumeric + Nominal → numeric codes that are actually categories (e.g., 0/1 flags, ZIP codes, jersey numbers).\nNumeric + Ordinal → ordered categories encoded as 1–5 (Likert items).\n\nIf a graph or analysis looks odd in JMP, check these settings first-they control which menu options and displays you’ll see in Graph Builder and Analyze.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nCategorical variable\nPlaces a case into a group/label; arithmetic on labels is not meaningful.\n\n\nNominal\nCategorical with no natural order.\n\n\nOrdinal\nCategorical with a natural order but uneven/unknown spacing.\n\n\n\nBinary | A categorical variable with exactly two categories. |\nQuantitative variable | Numeric values where arithmetic is meaningful. |\nDiscrete | Quantitative counts that change in whole steps. |\nContinuous | Quantitative measurements on a (nearly) continuous scale. |\nIdentifier (ID) | A label used to distinguish cases; not for numerical analysis. |\nDate/time | A timestamped variable that can be treated as categorical or quantitative depending on the question. |\nMeasurement scale | Describes how numbers relate to the thing measured (nominal, ordinal, interval, ratio). |\nInterval | Numeric scale with arbitrary zero; differences meaningful, ratios not. |\nRatio | Numeric scale with a true zero; differences and ratios meaningful. | |\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nClassify each variable as categorical nominal, categorical ordinal, binary, quantitative discrete, or quantitative continuous:\n\nNumber of missed classes this semester\nPain rating on a 0–10 scale\nWhether a chip passes final inspection\nZIP code\nBody temperature (°F)\n\nA researcher converts systolic blood pressure into “Low” (&lt;110), “Normal” (110–139), and “High” (≥140). Name one advantage and one drawback of this recoding.\nGive an example where a date/time variable should be treated as (a) categorical and (b) quantitative.\nFor each of the following, say whether ratios are meaningful and explain briefly:\n\nTemperatures in °C\nHours of weekly exercise\nIQ scores\n\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n(a)Quantitative discrete (counts). (b) Categorical ordinal (ordered scale with uneven spacing)-often treated as numeric for convenience, but it’s ordinal by design. (c) Binary (pass/fail). (d) Identifier (categorical nominal label; not a quantitative variable). (e) Quantitative continuous.\nAdvantage: simplifies communication and enables comparisons across broad groups. Drawback: throws away information; analyses lose power and can depend on arbitrary cutpoints.\n(a)Categorical: day of week when a call was received (Mon–Sun). (b) Quantitative: minutes since admission, time to recovery, or days from treatment to follow-up.\n(a)°C is an interval scale: ratios aren’t meaningful (20°C isn’t “twice as hot” as 10°C). (b) Hours of exercise is a ratio scale with a true zero: ratios are meaningful (4 hours is twice 2 hours). (c) IQ is typically treated as an interval scale: differences are interpretable; ratios are not.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "01.html#the-role-of-statistics-in-research",
    "href": "01.html#the-role-of-statistics-in-research",
    "title": "1  Introduction to Statistics",
    "section": "1.4 The Role of Statistics in Research",
    "text": "1.4 The Role of Statistics in Research\n\n“Scientific research is a process of guided learning. The object of statistical methods is to make that process as efficient as possible.” - George Box, William Hunter, and J. Stuart Hunter\n\nResearch is not a matter of “run a test and see what comes out.” It is a disciplined, iterative process that begins with a well-defined question, moves through thoughtful study design and careful data collection, and ends with conclusions that are supported by evidence and honest about uncertainty. At every stage, judgment matters.\nStatistics is the connective tissue in this loop. It helps you plan how to learn from data, provides tools to summarize and visualize what you observed, and supplies a principled way to assess what you can responsibly claim about the population. Without statistical thinking, it is easy to overinterpret results, confuse noise for signal, or answer a question you never intended to ask.\n\nFrom question to estimand to design\nEvery sound statistical project should clearly articulate three elements before any data are analyzed:\n\nThe research question is a plain-language statement of what you want to learn. It should be specific and tied to a population of interest. For example: “Does the new tutoring program improve exam scores for Calculus I students?”\nThe target parameter (often called the estimand) translates that question into a precise quantity in the population. In this example, the estimand might be the difference in mean exam scores between students who receive tutoring and those who do not. Naming the parameter forces clarity about what “improve” actually means.\nThe study design describes how data will be collected to estimate that parameter. This includes the sampling plan, how variables will be measured, and whether treatments or interventions will be assigned.\n\nThese three pieces must align. A mismatch—such as asking a causal question with a purely descriptive design—leads to weak or misleading conclusions.\nTwo broad families of study designs appear throughout statistics:\n\nAn observational study records what naturally occurs, without assigning treatments or interventions. These studies are common and often easier to carry out. They are well suited for describing patterns and associations, but they are vulnerable to confounding, where other variables are related to both the explanatory variable and the outcome.\nAn experiment actively assigns a treatment to units and compares outcomes across groups. When well designed—with randomization, control, and careful implementation—experiments allow for stronger causal conclusions because they help isolate the effect of the treatment from other factors.\n\nThe choice between these designs depends on the research question, ethical and practical constraints, and what kind of claim you ultimately want to make.\n\n\nThe statistical process: a workable blueprint\nWhile real research is rarely linear, the following workflow provides a practical and reusable blueprint:\n\nSpecify the problem clearly. Identify the population, the observational units, and the variables involved. Ambiguity here leads to confusion later.\nDesign the study. Choose an observational or experimental approach, plan how the sample will be obtained, and think carefully about potential sources of bias and confounding.\nCollect data with quality in mind. Aim for reliable measurement, consistent procedures, and thorough documentation. Poor data quality cannot be fixed by clever analysis.\nExplore first. Use exploratory data analysis (EDA)—graphs, tables, and summary statistics—to understand the data’s structure, detect errors or anomalies, and build intuition before formal modeling.\nModel and infer. Select statistical methods that match the question and the data type, whether that involves comparing groups, studying relationships, or making predictions.\nQuantify uncertainty. Report estimates along with measures of uncertainty, such as confidence intervals and (when appropriate) p-values. Always distinguish statistical significance from practical significance. These tools will be developed in Chapters 9 and 10.\nDecide and communicate. Interpret results in context. Explain what the findings mean, what they do not imply, and what actions—if any—are justified based on the evidence.\nMake it reproducible. Ensure that another person could follow your steps and reproduce your results from the same data. Reproducibility is a cornerstone of trustworthy science.\n\nIn practice, this process is rarely a straight line. Insights from exploratory analysis often send you back to refine the research question or adjust the design. That looping is not a failure—it is a sign of careful, thoughtful science guided by statistical reasoning.\n\n\nWorking in JMP\nJMP is built for this process:\n\nDocument the plan and steps. Use File → New → Journal to create a running research notebook. Paste screenshots, notes, and output as you go.\nSave scripts to reproduce output. In most reports, click the red triangle ► Save Script → To Data Table (or To Journal). This stores a runnable recipe with the data.\nExplore first. Start with Graph → Graph Builder and Analyze → Distribution to profile variables and check data quality.\nFit models with assumptions in view. Use Analyze → Fit Y by X for two-variable comparisons and Analyze → Fit Model for multiple predictors. Residual and diagnostic tools are right there in the platform menus.\nShare and rerun. Bundle data, scripts, and notes with Projects or send a Journal so collaborators can reproduce your analysis.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nResearch question\nThe plain-language question your study seeks to answer.\n\n\nTarget parameter\nThe specific population quantity you aim to learn (e.g., mean difference, proportion).\n\n\nStudy design\nThe plan for collecting data (sampling, measurement, assignment) to answer the question.\n\n\nObservational study\nA design that observes existing groups without assigning treatments.\n\n\nExperiment\nA design that assigns treatments (often at random) and compares outcomes.\n\n\nTreatment\nThe condition or intervention applied in an experiment.\n\n\nConfounding\nA third factor related to both treatment and outcome that distorts comparisons.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nA hospital asks: “Does a new scheduling system reduce ER wait times compared to the current system?”\n\nState a suitable target parameter.\nName a design choice (observational vs. experimental) and one reason for your choice.\n\nA university compares GPA between students who use tutoring and those who don’t, with no assignment or randomization. Name a potential confounder and how an experiment would address it.\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n(a)Target parameter: the mean difference in ER wait time (new minus current) for all eligible ER visits. (b) Design: a randomized experiment (e.g., randomize days or shifts to “new” vs. “current”), so groups differ only by scheduling system on average, improving causal interpretation.\nPotential confounder: prior academic preparation (e.g., incoming math placement). Students who seek tutoring might differ systematically. An experiment could randomly assign eligible students to tutoring or control (or randomize access), balancing confounders by design.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "01.html#footnotes",
    "href": "01.html#footnotes",
    "title": "1  Introduction to Statistics",
    "section": "",
    "text": "We will not discuss how to find these “chances” here. The chances of these two scenarios can be found using the binomial distribution discussed in Section 6.4.↩︎\nIn real world applications, you are usually limited by how you measure. For instance, you may be measuring the length of insects and you measure to the nearest millimeter. This limitation should not play a role in determining continuous RVs. So in theory, insects could measure between 10 and 20 millimeters. You only measure in millimeters but an insect could be 10.1, 10.114, 10.675, 10.000004, etc. Since there are infinite number of values in 10 to 20 that insects could measure in theory, we say the length is continuous.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "02.html",
    "href": "02.html",
    "title": "2  Collecting Data",
    "section": "",
    "text": "2.1 Sampling Methods\nA sample is a subset of a population, but not every subset is equally informative. A representative sample mirrors the key characteristics of the population closely enough that analyzing the sample leads to the same conclusions you would reach by studying the entire population. Put differently, it is a smaller group whose results accurately reflect those of the larger group. Designing studies that produce representative samples lies at the core of statistics-it allows us to save time and resources without compromising the credibility of our conclusions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "02.html#sec-sampling_methods",
    "href": "02.html#sec-sampling_methods",
    "title": "2  Collecting Data",
    "section": "",
    "text": "“And I knew exactly what to do. But in a much more real sense, I had no idea what to do.” - Michael Scott\n\n\n\nProbability vs. non-probability sampling\nThere are two big approaches to sampling:\n\nIn probability sampling, you use random mechanisms so that every member of the population has a known chance of being selected. This ensures that any differences between the sample and the population are due to random sampling error, not researcher choice, and makes it possible to quantify uncertainty in estimates.\nIn non-probability sampling, you choose participants based on convenience, volunteer responses, judgement or quotas. These methods are cheaper and often necessary for exploratory or qualitative work, but they make it hard to know whether the sample really reflects the population. Lack of a representative sample reduces the validity of conclusions and can introduce sampling bias.\n\nWhenever you want to generalize, probability sampling is the gold standard.\n\n\nSimple random sampling (SRS)\nIn a simple random sample (SRS), every individual in the population has an equal probability of being selected, and every possible sample of a given size is equally likely. To carry out an SRS, you must first have a sampling frame, which is a complete list of all units in the population. You then use a random mechanism-such as a random number generator or random number table-to choose the sample.\nIn practice, simple random sampling can be done with replacement or without replacement. Sampling without replacement is far more common in real studies, since it prevents the same individual from being selected more than once. Sampling with replacement is sometimes useful in theoretical settings, but it is rarely appropriate when collecting data from people or physical units.\nThe main advantage of an SRS is its fairness and transparency: because selection is purely random, it avoids systematic bias in who is included. However, SRS also has limitations. It requires a complete and accurate sampling frame, which may not exist or may be difficult to obtain. In addition, when a population is geographically dispersed or hard to reach, implementing an SRS can be costly or impractical.\n\n\n\n\n\n\nExample 2.1\n\n\n\nSuppose you want to survey 100 employees of a social media marketing company out of 1,000. You assign each employee a number from 1 to 1,000 and use a random number generator to select 100 distinct numbers. The employees corresponding to those numbers form your sample. Because each employee had an equal chance of being selected, the resulting sample is likely to be representative, especially if the sample size is sufficiently large.\nTo do this in JMP, suppose you have the list of 1,000 names like you see below.\n\nIf we wanted to get a random sample of 20, we would just click on Tables→Subset.\n\nThis will open up the following window.\n\nSelecting ‘Random - sample size’, entering the desired sample size in the box next to it, and then clicking ‘OK’ will make a new data table with 20 of the names selected at random.\nNote the option above labeled ‘Random - sampling rate’ will allow you to choose a proportion of the population to randomly sample instead of choosing a sample size.\n\n\n\n\n\n\n\n\nExample 2.2\n\n\n\nA university wants to estimate the average number of hours per week that undergraduates spend studying. The registrar has a list of all 12,000 enrolled undergraduates. Each student is assigned an ID number, and a computer randomly selects 400 unique IDs. Those students are contacted and asked to report their weekly study hours. This procedure produces a simple random sample because every student had the same chance of being included.\n\n\n\n\nSystematic sampling\nSystematic sampling is a practical alternative to simple random sampling that reduces logistical effort while preserving much of the randomness. Instead of selecting units entirely at random, you begin with an ordered sampling frame and choose every \\(k\\)-th unit, where \\(k\\) is the sampling interval. The value of \\(k\\) is typically determined by dividing the population size (\\(N\\)) by the desired sample size (\\(n\\)) (so \\(k \\approx N/n\\)). To introduce randomness, you first select a random starting point between 1 and \\(k\\), then continue by selecting units at fixed intervals.\nWhen the ordering of the list is unrelated to the variable being studied, systematic sampling closely approximates a simple random sample. It is often faster, easier to explain, and simpler to implement-especially in field settings or large administrative lists-because it avoids repeated random number generation.\nHowever, systematic sampling carries an important risk: periodicity. If the list has a repeating structure or is sorted in a way that is correlated with the outcome of interest, the method can unintentionally overrepresent some types of units and underrepresent others. For example, if employees are grouped by department, shift, or seniority, selecting every \\(k\\)-th name may systematically favor or exclude certain groups. In such cases, the resulting sample may be biased, even though the selection rule appears objective.\n\n\n\n\n\n\nExample 2.3\n\n\n\nAll employees of a company are listed in alphabetical order. You want to sample 100 employees from a list of 1,000, so you set \\(k = 10\\). You randomly select a starting position among the first ten names-say, the 6th person-and then select every 10th person on the list (6, 16, 26, 36, …) until 100 employees are chosen. This approach is easy to carry out and avoids duplicates, but it may be problematic if the alphabetical ordering aligns with job roles, seniority, or family relationships, which could distort the representativeness of the sample.\n\n\n\n\nStratified sampling\nIn stratified sampling, the population is first divided into distinct, non-overlapping subgroups called strata, based on a characteristic that is relevant to the study (such as gender, age group, geographic region, or job level). Each individual in the population belongs to exactly one stratum. After forming the strata, a random sample is drawn within each stratum, rather than from the population as a whole.\nThe most common approach is proportional stratified sampling, where the number of units sampled from each stratum is proportional to that stratum’s share of the population. This guarantees that the composition of the sample mirrors the composition of the population with respect to the stratifying variable. In some studies, researchers may intentionally oversample smaller or especially important strata (called disproportionate stratified sampling) to ensure enough data for detailed subgroup analysis, adjusting for this later in the analysis.\nStratified sampling has several advantages. By forcing representation from each stratum, it prevents small but important subgroups from being underrepresented due to random chance. In addition, when individuals within a stratum are relatively similar to one another but different across strata, stratification can substantially reduce sampling variability, leading to more precise estimates than a simple random sample of the same size.\nThe main drawback is increased complexity. Stratified sampling requires reliable information to classify individuals into strata before sampling begins, as well as separate sampling procedures within each stratum. This can increase administrative effort, cost, and the potential for implementation errors if strata are poorly defined or misclassified.\n\n\n\n\n\n\nExample 2.4\n\n\n\nA company has 800 junior employees and 200 senior employees. Because job seniority is expected to influence workplace satisfaction, the company divides employees into two strata: junior and senior. To draw a sample of 100 employees that reflects the population structure, the company randomly selects 80 junior employees and 20 senior employees. The resulting sample preserves the original 80/20 split, ensuring that both groups are appropriately represented and that comparisons between junior and senior employees are meaningful.\n\n\n\n\nCluster sampling\nCluster sampling also divides the population into groups, but the logic is fundamentally different from stratified sampling. In cluster sampling, each cluster is intended to be a small-scale version of the entire population-a mini-population that contains a diverse mix of individuals. Rather than ensuring representation from every group, the goal is to reduce cost by limiting how many groups are studied.\nTo implement cluster sampling, the population is first partitioned into clusters, often based on geography, organizational units, or naturally occurring groupings (such as schools, offices, or neighborhoods). A random sample of clusters is then selected, and data are collected from all individuals within the selected clusters (one-stage cluster sampling) or from a random subset of individuals within those clusters (two-stage cluster sampling).\nThe primary advantage of cluster sampling is efficiency. When populations are large and geographically dispersed, it is often impractical or prohibitively expensive to reach individuals scattered across many locations. By concentrating data collection within a small number of clusters, researchers can dramatically reduce travel, coordination, and administrative costs. For this reason, cluster sampling is commonly used in large-scale surveys, public health studies, and educational assessments.\nThe main drawback is increased variability. If individuals within a cluster tend to be similar to one another-and clusters differ meaningfully from one another-then sampling only a few clusters may fail to capture the full diversity of the population. This intra-cluster similarity reduces the effective amount of information in the sample and can lead to larger sampling variance than a simple random sample of the same size. In practice, this means cluster sampling often requires a larger total sample size to achieve comparable precision.\n\n\n\n\n\n\nExample 2.5\n\n\n\nA company operates offices in 10 cities across the country, each employing roughly the same number of workers and performing similar roles. To conduct an employee satisfaction survey, the company randomly selects 3 offices and surveys every employee in those locations. This approach greatly reduces travel and administrative effort. However, it relies on the assumption that offices are reasonably similar; if workplace culture or management practices vary substantially by city, the survey results may be less precise or even misleading.\n\n\n\n\nNon-probability methods\nIn some research settings, random selection is impossible, impractical, or unnecessary. Non-probability sampling methods rely on researcher judgment or participant availability rather than chance. Because individuals do not have known or equal probabilities of selection, these methods cannot guarantee that the sample is representative of the population. As a result, conclusions drawn from non-probability samples should be interpreted cautiously and generally should not be generalized to a broader population.\nDespite this limitation, non-probability methods are widely used in practice. They are common in exploratory studies, pilot research, qualitative work, and studies involving hard-to-reach populations, where constructing a complete sampling frame or implementing random selection is unrealistic. In such cases, the goal is often to generate insights, identify patterns, or refine research questions rather than to make precise population-level estimates.\n\nConvenience sampling\nA convenience sample consists of individuals who are easiest for the researcher to access. Participants are selected simply because they are readily available, willing, or nearby. This approach is inexpensive, fast, and easy to implement, which makes it appealing for classroom projects, preliminary studies, and early stages of research.\nHowever, convenience sampling comes with serious limitations. Because selection is driven by accessibility rather than randomness, certain groups may be systematically overrepresented while others are excluded entirely. This can introduce substantial bias, meaning the results may reflect the characteristics of the convenience group rather than those of the population of interest. For this reason, findings from convenience samples should not be treated as broadly generalizable.\n\n\n\n\n\n\nExample 2.6\n\n\n\nYou want to learn about student perceptions of campus support services. After each of your classes, you ask students in the room to complete a short survey. While this approach is quick and easy, it only captures the views of students enrolled in your classes. These students may differ from the broader student body in major, year, motivation, or academic engagement, so the resulting sample is not representative of all students at the university.\n\n\n\n\nVoluntary response sampling\nA voluntary response sample is formed when individuals choose for themselves whether to participate in a study. Invitations are typically broad-such as open surveys, online polls, or public feedback forms-but participation is entirely self-selected. Because respondents are not randomly chosen, the resulting sample often reflects the views of those who feel most strongly about the topic.\nThe key issue with voluntary response sampling is self-selection bias. People with intense opinions, strong grievances, or high levels of engagement are much more likely to respond than those who are indifferent or moderately affected. As a result, voluntary response samples tend to exaggerate extremes and provide a distorted picture of average attitudes or behaviors.\nAlthough voluntary response samples are easy and inexpensive to collect, they are inappropriate for drawing conclusions about a population as a whole. They are best used for gathering feedback, identifying potential concerns, or generating hypotheses rather than for making quantitative estimates.\n\n\n\n\n\n\nExample 2.7\n\n\n\nYou email a survey to the entire student body asking for opinions about a new campus policy. Only a small fraction of students respond, and those responses come primarily from students who are either strongly supportive or strongly opposed. Students with neutral or mildly held views are far less likely to participate, so the results cannot be trusted to represent the typical student’s perspective.\n\n\n\n\nPurposive sampling\nIn purposive sampling, participants are deliberately selected by the researcher because they possess characteristics, experiences, or knowledge that are especially relevant to the research question. Rather than aiming for representativeness, the goal is to obtain information-rich cases that can provide deep insight into a particular phenomenon.\nThis method is common in qualitative research, case studies, and applied settings where understanding how or why something occurs is more important than estimating its prevalence. Researchers often aim for diversity within the purposive sample-such as selecting participants with different backgrounds or experiences-to capture a range of perspectives.\nThe main limitation of purposive sampling is that selection is subjective and non-random. While the resulting data can be rich and informative, it cannot support population-level inference or precise numerical claims.\n\n\n\n\n\n\nExample 2.8\n\n\n\nYou want to understand how well university services support students with disabilities. You intentionally recruit students with different types of disabilities and varying levels of accommodation needs. This allows you to explore common challenges and contrasts across experiences, but it does not allow you to estimate what proportion of all disabled students share those experiences.\n\n\n\n\nSnowball sampling\nSnowball sampling is a recruitment method in which existing participants help identify and recruit additional participants. The process typically begins with a small number of initial subjects, who then refer others they know who meet the study criteria. Over time, the sample grows through these social connections, much like a snowball rolling downhill.\nSnowball sampling is especially useful for studying hidden, stigmatized, or hard-to-reach populations, such as individuals experiencing homelessness, undocumented workers, or members of informal networks. In these cases, traditional sampling frames may not exist, and trust is often essential for participation.\nHowever, snowball sampling provides little control over who is included. Because referrals occur within social networks, the sample may overrepresent individuals who are more connected or similar to the initial participants. This can introduce substantial bias and limit the generalizability of findings.\n\n\n\n\n\n\nExample 2.9\n\n\n\nTo study experiences of homelessness, you begin by interviewing one participant who agrees to take part in the study. She then introduces you to others she knows who are also homeless, and those participants provide additional referrals. While this approach allows you to access a difficult-to-reach population, the resulting sample may reflect a narrow subset of experiences shaped by shared social connections.\n\n\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nRepresentative sample\nA sample that accurately reflects key characteristics of the population.\n\n\nProbability sampling\nSampling technique using random selection so each unit has a known chance of inclusion.\n\n\nNon-probability sampling\nSampling techniques based on convenience or judgement without randomisation.\n\n\nSimple random sampling\nEvery unit has an equal chance of selection; implemented via random number generators.\n\n\nSystematic sampling\nSelecting every \\(k\\)-th unit from an ordered list after a random start.\n\n\nStratified sampling\nDividing the population into subgroups and randomly sampling within each subgroup.\n\n\nCluster sampling\nRandomly selecting entire groups (clusters) and studying all units within them.\n\n\nConvenience sampling\nIncluding the most accessible units; prone to sampling and selection bias.\n\n\nVoluntary response\nSampling based on participants who choose to respond, often those with strong opinions.\n\n\nPurposive sampling\nSelecting cases based on researcher judgement of what is most informative.\n\n\nSnowball sampling\nRecruiting participants via referrals from initial subjects, often for hidden populations.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nYou want to estimate the average GPA of all first-year students at your university.\n\nName two probability sampling methods you could use.\nBriefly explain why a convenience sample of your friends might mislead you.\n\nA researcher selects every 5th name from a sorted list of patients to survey. What sampling method is this? Under what circumstance might this method introduce bias?\nCompare stratified sampling and cluster sampling. Give an example of a scenario where each would be appropriate.\nExplain why voluntary response samples often yield extreme views and cannot be trusted for generalizing to a population.\nIn JMP, how could you create a simple random sample of 150 observations from a data table with 2,000 rows?\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n(a)Simple random sampling (assign each first-year student a number and randomly select using a random number generator); stratified sampling (divide students by major or residence hall and sample proportionally within each group). (b) Your friends are likely from similar classes or social circles, so they may have similar study habits; they might not reflect the broader student body.\nThis is systematic sampling. It works well if the list has no pattern related to the outcome. If patients are sorted by appointment time, every 5th patient might always be a morning appointment, which could bias results if morning and afternoon patients differ.\nStratified sampling divides the population into meaningful groups and samples within each (e.g., sampling men and women separately when studying height). It ensures each subgroup is represented. Cluster sampling selects whole groups (e.g., choosing three hospitals at random and surveying all nurses within them) to save cost when the population is geographically spread out.\nPeople with strong positive or negative feelings are more likely to volunteer, while those who are neutral remain silent. This self-selection skews the sample, so the responses do not reflect the average opinion in the population.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "02.html#experimental-design",
    "href": "02.html#experimental-design",
    "title": "2  Collecting Data",
    "section": "2.2 Experimental Design",
    "text": "2.2 Experimental Design\n\n“All life is an experiment. The more experiments you make the better.” -Ralph Waldo Emerson\n\nStatistics provides two complementary approaches for gathering evidence: surveys and experiments. In a survey, we select individuals from a population, collect information by asking questions, and summarize the resulting data. In an experiment, we actively assign treatments to units and observe their responses. In both cases, sound inference depends on thoughtful sampling and careful study design. While other methods of data collection exist, surveys and experiments are the approaches most people have in mind when they think about gathering data.\n\nPrinciples of good experimental design\nThe goal of an experiment is to isolate the causal effect of a treatment by systematically controlling other sources of variation. Experiments give the researcher direct control over how treatments are assigned. To ensure that any observed differences in outcomes can be attributed to the treatment itself-and not to hidden biases or confounding factors-well-designed experiments rely on four core principles.\n\nRandomization\nRandomization is the foundation of experimental design. Assigning experimental units to treatment conditions by chance ensures that, on average, the groups being compared are similar with respect to both observed characteristics (such as age or prior experience) and unobserved characteristics (such as motivation or genetic differences). This balance allows differences in outcomes to be interpreted as causal effects of the treatment rather than artifacts of preexisting differences.\nIn a completely randomized design, each unit is assigned to a treatment independently of the others. For example, in a sleep study, students might be randomly assigned to different cell phone usage limits before bedtime. This approach is simple and effective when experimental units are fairly homogeneous.\nIn a randomized block design, units are first grouped into blocks based on a characteristic known to influence the response, and treatments are then randomly assigned within each block. For instance, agricultural plots might be blocked by rainfall zone before assigning fertilizer types. Blocking removes predictable variation due to the blocking variable, allowing randomization to work more efficiently within each group.\nRandomization also applies beyond treatment assignment. In industrial or laboratory settings, the order of experimental runs should be randomized to prevent time-related factors-such as equipment warming, operator fatigue, or ambient temperature-from becoming confounded with treatment effects.\n\n\n\n\n\n\nExample 2.10\n\n\n\nIn a drug study with 20 mice and two test kits (A and B), you might randomly assign 10 mice to kit A and the remaining 10 to kit B. Differences in age, weight, or health are then spread randomly across the two groups, preventing these factors from systematically favoring one treatment over the other.\n\n\n\n\nControl and placebo\nA well-designed experiment includes a meaningful comparison, typically between a treatment group and a control group. The control group provides a baseline against which the treatment effect is measured. Depending on the context, the control may receive no treatment, standard care, or a placebo-a treatment designed to mimic the experience of receiving the real intervention without containing the active ingredient.\nPlacebo controls are especially important in medical and psychological studies because participants’ expectations alone can influence outcomes. Improvements due to belief, attention, or the act of being treated are known as placebo effects. By giving both groups identical experiences except for the active component, researchers can attribute differences in outcomes specifically to the treatment.\n\n\nReplication\nReplication refers to applying each treatment to multiple experimental units. Replication is essential because outcomes naturally vary from unit to unit, even under identical conditions. By observing this variability, researchers can estimate the amount of random noise in the data and determine whether observed differences between treatments are larger than would be expected by chance alone.\nReplication increases the precision of estimated treatment effects and provides the information needed for statistical inference. A single observation per treatment cannot distinguish a genuine effect from an unusual outcome.\n\n\n\n\n\n\nExample 2.11\n\n\n\nMeasuring battery life under a specific charging condition using several batteries gives a far more reliable estimate than testing just one battery, which might be unusually good or unusually poor.\n\n\n\n\nBlocking\nBlocking is used when a nuisance variable-one that is not of primary interest but is known to affect the response-can be identified in advance. In a block design, experimental units are grouped into relatively homogeneous blocks based on this variable, and treatments are randomized within each block.\nBlocking reduces unexplained variability by accounting for known sources of variation. This leads to more precise estimates of treatment effects without increasing sample size.\nNatural blocking structures often arise in practice. In biological experiments, animals may be grouped by litter; in chemical experiments, samples may come from different batches; in education studies, students may be grouped by classroom. Treating these groupings as blocks and randomizing within them prevents block-level differences from obscuring the treatment effect.\n\n\n\nPutting the principles together\nStrong experimental designs typically combine these principles rather than relying on just one. For example, an agricultural study might block plots by rainfall zone, randomly assign fertilizer treatments within each block, and replicate measurements across multiple growing seasons.\nExperiments also differ in structure. In between-subjects designs, each unit receives exactly one treatment condition. In within-subjects (or repeated-measures) designs, each unit experiences all treatment conditions, usually in a randomized or counterbalanced order. Within-subjects designs reduce variability by allowing each unit to serve as its own control, but they require careful attention to order effects, such as learning, fatigue, or carryover. Randomizing or counterbalancing the order of treatments is essential to preserve validity.\nTogether, randomization, control, replication, and blocking form the backbone of experimental design, enabling researchers to make credible causal claims from data.\n\n\nDesigning unbiased survey questions\nHigh-quality surveys require care in two distinct areas: how respondents are selected and how questions are written. Sampling determines who provides data (see Section 2.1); question design determines what information those respondents actually provide. Even a perfectly representative sample can produce misleading results if the questions themselves are biased, confusing, or poorly structured.\nWell-designed questions are clear, neutral, and interpretable in the same way by all respondents. Poorly designed questions can systematically distort responses, introduce bias, and lead decision-makers to draw incorrect conclusions. Below are several common pitfalls in survey question design, along with examples and revisions.\n\nLeading questions\nLeading questions subtly (or not so subtly) push respondents toward a particular answer by framing one response as more reasonable, popular, or desirable than others. This can inflate support for a policy, product, or opinion simply through wording rather than genuine sentiment.\nBiased:\n\n“Don’t you agree that our new app is much easier to use?”\n\nThis wording assumes agreement and pressures respondents to conform.\nUnbiased:\n\n“How would you rate the ease of use of our new app?”\n\nAnother example: Biased: “Most students think this course is well organized. Do you agree?” Unbiased: “How would you rate the organization of this course?”\n\n\nLoaded questions\nLoaded questions embed an assumption-often a controversial or emotionally charged one-into the question itself. Respondents are forced to accept the premise in order to answer, even if they disagree with it.\nBiased:\n\n“When did you stop wasting time on your phone?”\n\nThis question assumes the respondent wastes time on their phone and that they have already stopped.\nUnbiased:\n\n“How much time do you spend on your phone each day for non-work activities?”\n\nAnother example: Biased: “Why do you support unfair tuition increases?” Unbiased: “What is your opinion on recent tuition increases?”\n\n\nDouble-barreled questions\nDouble-barreled questions ask about two (or more) distinct issues but allow only a single response. Because respondents may have different opinions about each component, the resulting data are ambiguous and difficult-or impossible-to interpret.\nBiased:\n\n“Do you intend to leave work and return to full-time study this year?”\n\nA respondent might plan to leave work but not return to school, or vice versa.\nUnbiased:\n\n“Do you intend to leave your current job this year?” “Do you intend to return to full-time study this year?”\n\nAnother example: Biased: “How would you rate our products and level of service?” Unbiased:\n\n“How would you rate the quality of our products?” “How would you rate the quality of our customer service?”\n\n\n\nAmbiguous wording\nAmbiguous wording occurs when a question uses vague terms or phrases that different respondents may interpret differently. When this happens, people may answer different questions even though they are responding to the same survey item.\nBiased:\n\n“How do we compare to our competitors?”\n\nRespondents may interpret this as referring to price, quality, customer service, innovation, or brand reputation.\nUnbiased:\n\n“Compared to our competitors, how would you rate our prices?” “Compared to our competitors, how would you rate our customer service?”\n\nAnother example: Biased: “How often do you exercise regularly?” Unbiased: “On how many days per week do you engage in at least 30 minutes of physical activity?”\n\n\n\nWhy this matters\nSurvey questions shape the data you collect. Poorly worded questions can introduce bias just as surely as a flawed sampling method. Clear, neutral, and focused questions help ensure that responses reflect what respondents truly believe or experience, rather than how the question guided them. In practice, careful question design often requires multiple drafts, pilot testing, and revision-but the payoff is data that support credible, defensible conclusions.\nTo craft unbiased and effective survey questions, researchers should follow several key principles. These guidelines help ensure that respondents interpret questions consistently and feel free to answer honestly, leading to data that support meaningful conclusions.\n\nUse neutral language. Questions should be phrased without emotionally charged words, value judgments, or implied “correct” answers. Neutral wording allows respondents to express their true opinions rather than reacting to the tone of the question. Even subtle cues-such as describing a policy as “beneficial” or “harmful”-can influence responses. Replacing evaluative language with descriptive phrasing helps minimize this source of bias.\nBe specific and clear. Vague terms can mean different things to different respondents. Whenever possible, define key concepts, specify time frames, and avoid shorthand that assumes shared understanding. For example, instead of asking “How often do you use the library?” a clearer question would be “How many times have you visited the library in the past month?” Specific wording improves consistency and makes responses easier to interpret and analyze.\nAsk one thing at a time. Each survey item should measure a single concept. When a question combines multiple ideas, respondents may agree with one part but not the other, producing answers that are difficult to interpret. Splitting complex questions into separate items allows each concept to be measured cleanly and reduces confusion for respondents.\nBalance response options. Response scales should be symmetrical and evenly spaced, offering a full range of plausible choices. For example, a 5-point Likert scale ranging from “Strongly disagree” to “Strongly agree” treats positive and negative responses equally and includes a neutral midpoint if appropriate. Unbalanced or uneven scales can push respondents toward certain answers and distort the results.\nPilot test your survey. Even well-intentioned questions can be misunderstood. Pilot testing the survey with a small group of respondents helps identify ambiguous wording, confusing response options, or unintended interpretations. Feedback from pilot tests often reveals issues that are not obvious to the survey designer but can significantly affect data quality if left unaddressed.\n\nTogether, these principles promote clarity, neutrality, and fairness in survey design. By reducing confusion and bias, they help respondents provide thoughtful, accurate answers-and help researchers draw conclusions that are trustworthy and defensible.\n\n\nRecap\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nRandom assignment\nAssigning sampled units to treatment conditions by chance to create comparable groups.\n\n\nTreatment group / control group\nGroups receiving the experimental intervention and baseline comparison, respectively.\n\n\nPlacebo\nAn inert treatment used to mimic the experience of the intervention to control for expectations.\n\n\nReplication\nRepeating the same treatment on multiple experimental units to estimate variability.\n\n\nBlocking\nGrouping similar units and randomizing within each group to control a nuisance factor.\n\n\nBetween-subjects design\nEach unit experiences only one condition; comparisons are across subjects.\n\n\nWithin-subjects design\nEach unit experiences all conditions in random order.\n\n\nLeading question\nA survey question that suggests a particular answer.\n\n\nLoaded question\nA survey question containing an assumption or implication.\n\n\nDouble-barreled question\nA single question that asks about two things.\n\n\nAmbiguous wording\nVague terms that can be interpreted differently by different respondents.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nExplain the difference between random sampling and random assignment. Why are both important, and in what contexts do they apply?\nName the four principles of good experimental design and give a brief example of each.\nConsider this survey question: “How satisfied are you with the cost and quality of your textbooks?” Identify the problem and rewrite the question.\nIn a study of exam performance, 60 students volunteer for tutoring and 60 do not. The volunteer group has a higher average GPA than the non-volunteer group. Explain why this study may not show that tutoring causes better performance. How could you redesign it?\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nRandom sampling determines who gets into the study. Every member of the population has a known chance of selection, improving generalizability. Random assignment determines which condition participants experience, creating comparable groups and allowing causal conclusions. Surveys rely on random sampling; experiments rely on random assignment.\nRandomization: assign units by chance (e.g., randomize phone use levels to study sleep). Control/placebo: include a baseline or placebo condition to isolate the treatment effect. Replication: repeat treatments on multiple units, like testing several batteries under the same condition. Blocking: group units by a nuisance factor (e.g., soil type) and randomize within blocks.\nThe question is double-barreled-it asks about cost and quality. Rewrite as two separate questions (e.g., “How satisfied are you with the cost of your textbooks?” and “How satisfied are you with the quality of your textbooks?”).\nVolunteers may differ systematically from non-volunteers (e.g., motivation or prior GPA). Random assignment is missing. To infer causality, randomly assign students to tutoring or control groups and compare outcomes, possibly blocking on prior GPA.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "02.html#observational-studies-vs.-experiments",
    "href": "02.html#observational-studies-vs.-experiments",
    "title": "2  Collecting Data",
    "section": "2.3 Observational Studies vs. Experiments",
    "text": "2.3 Observational Studies vs. Experiments\n\n“You can observe a lot by just watching.” - Yogi Berra\n\nAt first glance, collecting data looks the same whether you’re watching what happens or deliberately changing something. But how you gather the data matters tremendously for what you can conclude.\n\nObservational studies: watching without intervening\nIn an observational study, researchers record what happens without actively assigning exposures or treatments. Common types include:\n\nCohort Studies\nCohort studies, where a group of people linked by a characteristic (e.g., birth year) is followed over time. Researchers compare outcomes between those exposed to some factor and those not exposed.\nA classic example of a cohort study is the Framingham Heart Study (FHS), which has been described in detail in the International Journal of Epidemiology. In 1948 the National Heart Institute recruited a community-based cohort of 5,209 adults aged 30–59 years from Framingham, Massachusetts, to investigate causes of cardiovascular disease (CVD). Two of every three families in the town were randomly sampled and invited; 4,494 (about 69%) agreed to participate, and an additional 715 volunteers joined. This initial prospective cohort has been followed every two to four years with detailed medical histories, physical examinations, electrocardiograms and laboratory tests. By following participants longitudinally for decades, the FHS identified major risk factors for CVD-such as high blood pressure, cholesterol, and smoking-helping to shape modern cardiovascular prevention guidelines.\n\n\nCase-Control Studies\nCase–control studies, where people with a condition (“cases”) are compared to similar people without it (“controls”) to look for differences in past exposures.\nFor example, a German study1 compared 118 patients with a rare form of eye cancer called uveal melanoma to 475 healthy patients who did not have this eye cancer. The patients’ cell phone use was measured using a questionnaire. On average, the eye cancer patients used cell phones more often. The cases were those who had developed uveal melanoma and the controls were those who did not uveal melanoma. The cell phone use was compared between the two groups.\nBecause participants choose their own behaviors, observational data reflect the real world and are often the only ethical way to study harmful exposures. For example, you can’t ethically assign people to smoke or not smoke, so the long-term effects of smoking are studied by tracking smokers and non-smokers over time.\nObservational studies are usually quicker and cheaper than experiments and have high ecological validity (they mirror everyday life). But they have a critical limitation: you can’t be sure whether differences in outcomes are caused by the exposure or by other factors that differ between groups. For example, a highly publicized 1985 study from Johns Hopkins University linked coffee consumption to an increased risk of heart disease, especially for heavy drinkers. The study’s findings, published in the American Journal of Epidemiology, were later challenged by other research that pointed out the failure to adequately control for the effect of cigarette smoking. Once smoking was controlled for, the link between coffee consumption and increased risk of heart disease was no longer significant.\n\n\n\nExperiments: deliberately changing something\nAs discussed previously, in an experiment, researchers assign treatments or interventions to units and observe the effects. Randomization-assigning units by chance-ensures that, on average, the groups are comparable on both observed and unobserved characteristics. The classic experimental design is the completely randomized design: participants are randomly allocated to receive a new drug, a placebo, or no treatment, and outcomes are compared. Experiments are considered the gold standard for establishing causality because randomization eliminates systematic differences between groups.\nExperiments also offer a controlled environment, making it easier to isolate the effect of a single factor. However, they can be expensive, time-consuming, or unethical to conduct.\nFor example, suppose we were interested in the association between eye cancer and smart phone use. Suppose we conduct an experiment, such as the following:\n\nPick half the students from your school at random and tell them to use a smart phone each day for the next 50 years.\nTell the other half of the student body not to ever use smart phones.\nFifty years from now, analyze whether cancer was more common for those who used smart phones.\n\nThere are obvious difficulties with such an experiment:\n\nIt’s not ethical for a study to expose over a long period of time some of the subjects to something (such as smart phone radiation) that we suspect may be harmful.\nIt’s difficult in practice to make sure that the subjects behave as told. How can you monitor them to ensure that they adhere to their treatment assignment over the 50-year experimental period?\nWho wants to wait 50 years to get an answer?\n\nThus, an observational study would be preferred over an experiment.\n\n\nWhy observational studies can mislead\nObservational data are susceptible to confounding-a situation where a third factor influences both the exposure and the outcome, creating a spurious association. For example, an observational cohort might find that people who meditate have lower rates of heart disease. But meditators may also exercise more and eat healthier diets, making it unclear whether meditation or lifestyle explains the difference. Similarly, people who choose to take daily vitamins might generally have healthier habits, so observed vitamin benefits may reflect those habits rather than the vitamins themselves.\nRandomization is the only method that can eliminate potential confounders by balancing both measured and unmeasured factors across treatment groups. In observational research, statistical methods like stratification, regression adjustment and propensity score matching can reduce bias, but they depend on untestable assumptions: all confounders must be measured correctly and modeled properly. Many important confounders may be unknown or infeasible to measure. Even meticulously controlled observational studies cannot remove all confounding. As a result, observational evidence alone “cannot support conclusions of causation”.\nBelow is a simple simulation illustrating confounding. Shoe size and reading ability appear positively related, but both are driven by age. When age is not controlled, a misleading association emerges.\n\n\n\n\n\n\n\n\n\nThe scatterplot shows a strong correlation between shoe size and reading, even though neither directly affects the other. The common cause is age. Observational studies must always consider whether a hidden variable like age could be responsible for an observed association.\n\n\nRecap\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nObservational study\nA study in which researchers record exposures and outcomes without assigning treatments or interventions.\n\n\nCohort study\nObservational design where a group is followed over time to compare outcomes between exposed and unexposed members.\n\n\nCase–control study\nObservational design where people with a condition (“cases”) are compared to similar people without the condition (“controls”) to look for differences in past exposures.\n\n\nExperiment\nA study where researchers introduce an intervention and randomly assign subjects to treatment or control groups.\n\n\nConfounding\nA situation where a third factor influences both the exposure and the outcome, potentially creating a spurious association.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nA nutrition researcher recruits people who already take daily multivitamins and compares their health outcomes to people who do not.\n\nIs this an observational study or an experiment?\nName at least two potential confounding variables.\n\nIn a randomized trial, half the participants are assigned to eat a Mediterranean diet and half to continue their usual diet. After a year, the first group shows lower cholesterol. Explain why randomization strengthens the causal interpretation.\nA cohort study finds that people who bike to work have lower rates of depression than those who drive. Suggest two reasons why this association may not reflect a causal effect of biking.\nDescribe a research question that would be unethical or impractical to answer via experiment but could be studied observationally. Explain why.\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\na)This is an observational study because participants choose whether or not to take multivitamins. b) Potential confounders include diet quality, exercise habits, socioeconomic status, access to healthcare, smoking status and other health behaviors.\nRandomization assigns diets by chance, so, on average, both known and unknown factors (age, lifestyle, genetics) are balanced across the groups. Therefore, differences in cholesterol are likely due to the diet rather than pre-existing differences.\nPeople who bike may have higher baseline fitness and better mental health; they might live in neighborhoods with better infrastructure or community support; they may also have lifestyles that promote well-being (e.g., more time outdoors). Any of these confounders could explain the observed association.\nStudying the long-term effects of smoking is unethical to do experimentally, because you can’t randomly assign people to smoke. Instead, researchers observe smokers and non-smokers and compare outcomes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "02.html#sources-of-bias",
    "href": "02.html#sources-of-bias",
    "title": "2  Collecting Data",
    "section": "2.4 Sources of Bias",
    "text": "2.4 Sources of Bias\n\n“Normally if given a choice between doing something and nothing, I’d choose to do nothing. But I would do something if it helps someone do nothing. I’d work all night if it meant nothing got done.” - Ron Swanson\n\nWhen we talk about bias in statistics, we mean a systematic error built into the way we select or measure our data. Bias is different from random sampling error. Sampling error comes from the natural variability you get when you observe only part of the population and tends to shrink as sample sizes grow. Bias, by contrast, does not go away with bigger samples; a flawed design simply produces more confident wrong answers. That makes it important to understand the different ways bias sneaks into our studies.\n\nHow bias differs from sampling error\nWhenever we select a sample, the numbers we compute (like the mean or proportion) will vary from one sample to the next. This variability is called sampling error. If we repeated our survey many times with different random samples, the average of those sample statistics would be the true population value, and the spread among them would reflect sampling error. Increasing the sample size reduces sampling error, but it does not correct for systematic flaws in how the sample was chosen. When the method of collecting or measuring data systematically favors some outcomes over others, we call it bias. A biased sample can be huge and still be wrong because its error is baked into the design.\nTo illustrate the difference, imagine a population with a true average income of $8 (in arbitrary units), made up of 70% low earners (income of 5) and 30% high earners (income of 15). Below we simulate two ways of sampling from this population: a fair simple random sample and a biased sample that over-selects high earners (80% high, 20% low). As the sample size grows, the random sample mean settles near the true average, while the biased sample mean stays high. This shows that increasing the sample size reduces random error but does not fix bias.\n\n\n\n\n\n\n\n\n\n\n\nCommon sources of bias\nBias can enter at many points in the data-collection process. Here are some of the most common culprits:\n\nCoverage (undercoverage) bias\nA coverage bias occurs when some members of the population are not included in the sampling frame. The Literary Digest’s famous 1936 presidential poll relied on telephone directories and car registration lists, thereby missing less affluent voters who tended to support Franklin Roosevelt. Because those voters were excluded, the sample favored wealthier respondents and overpredicted Alfred Landon’s support.\n\n\n\n\n\nNonresponse bias\nA nonresponse bias arises when selected individuals choose not to participate and the responders differ systematically from nonresponders. In the same 1936 survey only 25% of those sampled returned the mail-in ballot. Landon supporters were more likely to return the survey, so the results overestimated his popularity.\n\n\nVoluntary response bias\nWhen people opt into a survey on their own-like call-in radio polls about controversial topics-the sample disproportionately includes individuals with strong opinions. This voluntary response bias can produce extreme results because moderate voices remain silent.\n\n\nConvenience sampling bias\nA convenience sample chooses whoever is easiest to reach. If you stand outside a gym to survey “all adults in the city,” your sample will overrepresent health-conscious people. Convenience sampling often leads to coverage problems.\n\n\nResponse (measurement) bias\nEven if we select the right people, the way we ask questions or record data can introduce response bias. Response bias occurs when the measurement process influences the answer: leading questions or unbalanced answer choices can nudge respondents toward particular responses. Social desirability bias occurs when people underreport socially undesirable behaviors or overreport virtuous ones.\n\n\nSurvivorship bias\nWhen we only observe “survivors” and ignore those that dropped out or failed, we can mistake success for the rule.\n\n\n\nThe most famous example of this is the WWII bomber problem. During WWII, analysts tallied bullet holes on returning Allied bombers and saw clusters on wings and fuselage, with relatively few in engines and cockpit. The intuitive fix was to add armor where the holes were densest. Statistician Abraham Wald pointed out the trap: these data come only from planes that survived. Holes on the survivors mark places a plane can be hit and still make it home. The missing planes-those that didn’t return-are precisely the ones likely hit in the “clean” areas (e.g., engines). So Wald recommended reinforcing the areas with the fewest holes on the survivors, not the most. That’s survivorship bias: drawing conclusions from only the observed “winners” and ignoring the unseen “failures.”\n\n\nRecall bias\nIn retrospective studies, participants may not remember past events accurately. People who have developed an illness might recall exposures differently than healthy controls, leading to systematic differences.\n\n\nInterviewer bias\nThe interviewer’s tone, appearance or expectations can subtly influence responses. Neutral wording and training can reduce this effect.\n\n\nHealthy-user bias and attrition bias\nPeople who choose to participate in certain programs or who remain in a study for its duration often differ from those who do not, leading to biased estimates.\nEach of these biases stems from the way participants are chosen or how data are measured; they cannot be “averaged out” by larger samples.\n\n\n\nMitigating bias\nTo minimize bias:\n\nUse probability sampling whenever you want to generalize to a population. Random sampling helps guard against undercoverage and voluntary response bias.\nEnsure your sampling frame matches your target population. Consider oversampling underrepresented groups and weighting responses to reflect their true proportion.\nFollow up with nonresponders and offer multiple modes of participation to reduce nonresponse bias.\nDesign neutral, balanced questions and offer anonymity to reduce measurement and social desirability bias.\nDocument who was invited and who actually participated so you can assess potential biases.\nIn observational studies, adjust for measured differences between participants and nonparticipants using weighting or modeling; but remember that unmeasured biases may remain.\n\n\n\nRecap\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nCoverage bias\nSystematic error that arises when part of the population is missing from the sampling frame.\n\n\nNonresponse bias\nBias introduced when individuals who do not respond differ meaningfully from those who do respond.\n\n\nVoluntary response bias\nBias caused by allowing people to opt into a survey; respondents with strong opinions dominate the sample.\n\n\nResponse bias\nBias that arises from flaws in the measurement process, such as leading questions or social desirability.\n\n\nSampling error\nNatural variability in statistics from sample to sample; decreases with larger samples.\n\n\nBias\nSystematic error due to design or measurement; does not diminish with larger samples.\n\n\nSurvivorship bias\nFocusing only on observed “survivors” and ignoring those that failed, leading to overly optimistic conclusions.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nA tech company sends an email survey to customers using its premium service. Over half of the recipients do not respond. The company concludes that 85 % of its customers are satisfied.\n\nIdentify two potential sources of bias.\n\nSuggest one way to mitigate each bias.\n\nA political action group hosts an online poll on its website asking visitors whether they support a proposed tax increase. Seventy-five percent say “no.” What type of bias is most likely, and why does this poll not reflect general public opinion?\nSuppose you draw a simple random sample of 1,000 Baylor students from a roster and send them a questionnaire. Only 200 students respond. How could you use follow-ups or weighting to reduce bias? Explain your reasoning.\nExplain the difference between sampling error and bias in your own words. Why can a huge sample still give a wrong answer?\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n(a)Coverage bias and nonresponse bias. The company sampled only premium users (ignoring basic or free users) and most of the sampled customers did not respond, so respondents may differ from nonrespondents. (b) To reduce coverage bias, draw a sample from all customers or weight responses to reflect the full user base. To reduce nonresponse bias, send reminders, offer incentives or provide alternative modes (e.g., phone, mail).\nThis is voluntary response bias: only visitors who care enough to vote participate, and they may have strong opinions. A poll embedded on a partisan website cannot be generalized because participants are self-selected and not representative of the broader population.\nThe low response rate introduces nonresponse bias. You could send follow-up reminders, offer incentives, or contact nonrespondents by phone to increase participation. If demographic data are available for all sampled students, you can apply weights so that the 200 responders reflect the distribution of the 1,000 sampled students (and thus the target population).\nSampling error is the random fluctuation you see from one sample to the next; it decreases with larger samples. Bias is a systematic error built into the design or measurement; it doesn’t shrink with bigger samples. A huge convenience or volunteer sample can still give a wrong answer if it systematically excludes part of the population or asks questions in a biased way.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "02.html#footnotes",
    "href": "02.html#footnotes",
    "title": "2  Collecting Data",
    "section": "",
    "text": "Stang, A., Anastassiou, G., Ahrens, W., Bromen, K., Bornfeld, N., & Jöckel, K. H. (2001). The possible role of radio frequency radiation in the development of uveal melanoma. Epidemiology, 7-12.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "03.html",
    "href": "03.html",
    "title": "3  Describing Data with Tables and Graphs",
    "section": "",
    "text": "3.1 Organizing Categorical Data\nWhen you collect data that fall into groups—like preferred streaming service, political affiliation, or type of pet—the first step is to count how many observations fall into each category. Those counts form the backbone of both tables and graphs for categorical data. In this section we’ll learn how to build simple frequency tables, translate them into proportions or percentages, and organize two categorical variables together in a two‑way table. Along the way we’ll see when different visual summaries make sense and preview bar and pie charts (covered in detail in Section 3.2).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Describing Data with Tables and Graphs</span>"
    ]
  },
  {
    "objectID": "03.html#sec-03_01",
    "href": "03.html#sec-03_01",
    "title": "3  Describing Data with Tables and Graphs",
    "section": "",
    "text": "“The greatest value of a picture is when it forces us to notice what we never expected to see.” – John Tukey\n\n\n\nFrequency and relative frequency tables\nA frequency table is a basic but powerful tool for summarizing a categorical variable. It lists each distinct category and records how many observations fall into that category. By organizing raw data into counts, a frequency table provides an immediate snapshot of how the data are distributed across categories, making patterns and imbalances easy to see.\nA typical frequency table has two columns: one for the category labels and one for their frequencies (counts). Constructing a frequency table involves identifying all possible categories and tallying the number of observations in each. This process is often the first step in data analysis because it reduces a potentially large and messy dataset into a clear and interpretable summary.\nWhile frequency tables report absolute counts, these counts can be misleading when sample sizes differ. For example, a category with 50 observations may seem large, but its importance depends on whether the total sample size is 100 or 1,000. To address this, we often compute relative frequencies.\nA relative frequency expresses the count for each category as a proportion of the total number of observations. When a column of relative frequencies—or percentages—is added to a frequency table, the result is called a relative frequency table. Relative frequencies are calculated by dividing each category’s count by the total sample size. Because they represent proportions of the whole, relative frequencies always sum to 1 (or 100% when expressed as percentages).\nRelative frequency tables are especially useful for comparing distributions across different samples or populations. By focusing on proportions rather than raw counts, they allow meaningful comparisons even when the underlying sample sizes are not the same. For this reason, relative frequencies are commonly used in reports, visualizations, and summaries intended for broad audiences.\nIn practice, frequency and relative frequency tables often serve as the foundation for graphical displays such as bar charts and pie charts, providing a clear numerical summary that complements visual representations of categorical data.\n\n\n\n\n\n\nExample 3.1: Common symptoms in a clinic\n\n\n\nImagine you survey 30 patients at a local clinic about the primary symptom that brought them in. You record four categories: “Headache,” “Back pain,” “Fatigue,” and “Nausea.” We can organize the responses in a simple table of counts and proportions. Below we simulate such a survey and display the results.\n\n\n\n\n\nresponse\nfrequency\nrelative_frequency\n\n\n\n\nBack pain\n9\n0.3000000\n\n\nFatigue\n9\n0.3000000\n\n\nHeadache\n8\n0.2666667\n\n\nNausea\n4\n0.1333333\n\n\n\n\n\n\n\n\n\n\n\n\nThe table lists the four categories in alphabetical order with their counts and relative frequencies. For instance, if 8 of the 30 patients reported “Headache,” the relative frequency of “Headache” is \\(8/30 \\approx 0.27\\). The accompanying bar chart gives a visual sense of the same information: each bar’s height corresponds to a category’s frequency, and the bars are separated to emphasize that the categories have no inherent order. In practice you might reorder the bars to make the graph easier to read—perhaps putting the largest category first.\n\n\n\n\n\n\n\n\nPareto charts\n\n\n\n\n\nSometimes you want to highlight the few categories that account for most of the observations. A Pareto chart is a bar chart arranged in descending order of frequency and often paired with a cumulative percentage line. It helps you identify the “vital few and trivial many” in quality control and business applications. Pareto charts are useful when there are many categories and you want to focus attention on the most common causes or responses.\n\n\n\n\n\n\n\n\n\nTip:\n\n\n\n\n\nIn JMP you can create a frequency table by selecting Analyze → Distribution, assigning your categorical variable to the X role, and examining the resulting counts. To add relative frequencies, use the red triangle menu (▸) to choose Display Options → Show Percent. JMP’s Graph Builder will automatically construct a bar chart when you drag a categorical variable to the X‑axis and the count statistic to the Y‑axis.\n\n\n\n\n\nTwo-way (contingency) tables\nWhen data include two categorical variables, a natural question is whether—and how—those variables are related. A two-way table, also called a contingency table, provides a clear way to summarize and explore this relationship. Rather than listing categories separately, a contingency table displays the counts for every combination of categories across the two variables.\nIn a contingency table, one categorical variable defines the rows and the other defines the columns. Each cell in the table shows how many observations fall into the corresponding pair of categories. The table also often includes row totals, column totals, and a grand total, which help place the individual cell counts in context.\nContingency tables are especially useful for examining associations between variables. By comparing counts or proportions across rows or columns, you can look for patterns such as whether certain categories tend to occur together more often than would be expected by chance. For example, you might ask whether voting preference differs by age group, or whether product satisfaction varies by subscription type.\nTo aid interpretation, contingency tables are often converted to conditional distributions by computing row or column percentages. Row percentages answer questions like “Given this row category, how are observations distributed across the columns?” Column percentages reverse the conditioning. Choosing which percentages to compute depends on the research question and which variable is considered explanatory versus response.\nContingency tables form the foundation for formal statistical analysis of categorical data. In later chapters, these tables will provide the structure for chi-square tests of independence, which assess whether the observed association between two categorical variables is stronger than would be expected from random variation alone. Even before formal testing, however, contingency tables offer a powerful descriptive tool for uncovering relationships in categorical data.\n\n\n\n\n\n\nExample 3.2: symptom by age group\n\n\n\nSuppose we collect data on the same symptom question but also record each patient’s age group: “Under 30,” “30–50,” or “Over 50.” We can summarize the joint distribution in a two‑way table.\n\n\n\n\n\nage_group\nBack pain\nFatigue\nHeadache\nNausea\n\n\n\n\n30–50\n3\n4\n5\n3\n\n\nOver 50\n2\n3\n1\n0\n\n\nUnder 30\n4\n2\n2\n1\n\n\n\n\n\nEach cell in the table shows the number of patients who fall into the corresponding combination of age group and symptom. We can also compute row or column relative frequencies to see percentages within each group; for example, dividing each row by its total gives the distribution of symptoms within each age group. Contingency tables allow us to see whether symptom patterns differ across age groups and serve as input for clustered or stacked bar charts (discussed in Section 3.2).\n\n\n\n\nWhy percentages matter\nBecause categorical variables can have different numbers of levels and sample sizes can vary, relative frequencies are essential for fair comparisons. Reporting only counts can be misleading: 20 supporters of a movie genre in a survey of 50 people represent a large fraction, while 20 supporters in a survey of 500 people represent a much smaller fraction. Percentages standardize the scale.\nWhen displaying percentages, make sure they add to 100%. In a pie chart (a circular graph we’ll describe in the next section), each slice represents a category’s percentage of the whole. Pie charts are useful for showing how the total is divided among categories, but they become cluttered with too many slices. Bar charts are more flexible: you can reorder the bars, show counts or percentages, and compare multiple groups using side‑by‑side or stacked bars.\n\n\nWorking in JMP\nIn JMP, tables and graphs for categorical variables are straightforward:\n\nTo create a frequency table, go to Analyze → Distribution, assign your categorical variable to X, and click OK. The report shows counts and percentages; use the red triangle (▸) menu to toggle percentages, counts, or both.\nFor two categorical variables, use Analyze → Fit Y by X and assign one variable to Y and the other to X. Choose Contingency Table from the platform to see the two‑way counts and associated statistics.\nTo visualize categorical distributions, open Graph Builder, drag the categorical variable to the X‑axis, and drop the N summary statistic onto the Y‑axis. You can change the chart type to “Bar” or “Pie.” Dragging a second categorical variable onto the Group drop zone will create clustered or stacked bars.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nFrequency table\nA table that lists each category of a variable and the number of observations in that category.\n\n\nRelative frequency\nThe proportion or percentage of observations in a category, equal to the category’s count divided by the total count.\n\n\nRelative frequency table\nA frequency table with an additional column showing the relative frequency of each category.\n\n\nTwo‑way (contingency) table\nA table that displays the counts for each combination of levels of two categorical variable.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nIn a survey of 80 households, 32 own a dog, 20 own a cat, 12 own both, and the remainder own no pets. Construct a frequency table that shows the number and percentage of households in each pet ownership category (Dog only, Cat only, Both, None). Which visualization—a bar chart or a pie chart—would you choose, and why?\nExplain the difference between a frequency table and a relative frequency table. In what situations is it more informative to look at relative frequencies rather than absolute frequencies?\nWhat is a two‑way (contingency) table? Describe a scenario where a two‑way table could help you explore the relationship between two categorical variables.\nBar charts have spaces between bars and can be drawn in any order. Why are these design choices appropriate for categorical variables? What might go wrong if you drew the bars touching or forced them into a numerical order?\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nPet ownership table. The four categories and their counts are: Dog only (20), Cat only (8), Both (12), None (40). The total number of households is 80. The relative frequencies are 25% dog only, 10% cat only, 15% both, and 50% none. A bar chart would be preferable here because it allows you to order the bars from most to least common and makes it easy to compare magnitudes. A pie chart could work for four categories, but it becomes harder to read when slices are similar in size or when there are many categories.\nFrequency vs. relative frequency. A frequency table reports the counts of observations in each category. A relative frequency table adds a column showing the proportion or percentage of observations in each category. Relative frequencies are more informative when comparing groups of different sizes or when you want to focus on the distribution rather than the sample size—for example, comparing survey results from two classes of different sizes.\nContingency table example. A two‑way table displays counts for each combination of levels of two categorical variable. For instance, you could record whether each patient in a clinic has insurance (Yes/No) and whether they arrived on time (On time/Late). A contingency table would show how many patients fall into each combination (e.g., insured & on time, insured & late, uninsured & on time, uninsured & late), helping you explore whether punctuality differs by insurance status.\nDesign choices. Categories have no intrinsic numeric order, so bars in a bar chart can be arranged in any order without misrepresenting the data. Leaving space between bars reinforces that the categories are distinct and unordered. If you drew the bars touching, it might suggest a continuous scale (like a histogram), which could confuse readers. Forcing categories into a numerical order might imply ranking where none exists.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Describing Data with Tables and Graphs</span>"
    ]
  },
  {
    "objectID": "03.html#sec-03_02",
    "href": "03.html#sec-03_02",
    "title": "3  Describing Data with Tables and Graphs",
    "section": "3.2 Bar Charts and Pie Charts",
    "text": "3.2 Bar Charts and Pie Charts\n\n“Normally if given a choice between doing something and nothing, I’d choose to do nothing. But I would do something if it helps someone do nothing. I’d work all night if it meant nothing got done.” – Ron Swanson\n\nWhen you’ve tallied the counts of a categorical variable, your next job is to turn those numbers into a picture. Two of the simplest pictures—bar charts and pie charts—seem deceptively alike: each shows categories and their sizes. But as we’ll see, they serve different purposes and come with different design rules.\n\nWhat is a bar chart?\nAs we have already seen in Section 3.1, a bar chart is one of the most common and effective ways to visualize categorical data. It displays distinct categories along one axis and represents a numerical value for each category—such as a count, proportion, or percentage—using the length of a bar along the other axis. Bar charts can be drawn with bars oriented vertically (often called column charts) or horizontally, depending on which layout best supports readability.\nA defining feature of bar charts is that the bars are separated by gaps. These gaps signal that the categories are discrete and unordered, distinguishing bar charts from histograms, which display continuous data with adjacent bins. The axis along which the bars extend typically starts at zero, ensuring that bar lengths accurately reflect the magnitudes being compared.\nBar charts are especially effective because humans are very good at comparing lengths that share a common baseline. This makes it easy to see which categories are larger or smaller, to compare differences across groups, and to identify patterns or outliers at a glance. For this reason, bar charts are a natural choice for summarizing frequency tables, relative frequency tables, and summary statistics across categories.\nBar charts are also flexible. They can be used to display raw counts, percentages, averages, or other summary measures, as long as the underlying variable is categorical. When categories have long labels or there are many of them, horizontal bar charts often improve readability. In more advanced settings, grouped or stacked bar charts can be used to compare categories across additional variables.\nBecause of their clarity and versatility, bar charts are often the first visualization used to explore categorical data and to communicate results to a broad audience.\n\n\n\n\n\n\nExample 3.3: Distribution of blood types\n\n\n\nSuppose a hospital records the blood type (A, B, AB or O) of 200 randomly chosen donors. The counts are shown in the table below along with a bar chart. Notice that the bars are separated and can be reordered to make patterns easy to see.\n\n\n\n\n\ntype\ncount\nprop\n\n\n\n\nA\n66\n0.330\n\n\nAB\n9\n0.045\n\n\nB\n31\n0.155\n\n\nO\n94\n0.470\n\n\n\n\n\n\n\n\n\n\n\n\nThe vertical bar chart emphasizes how common type O is relative to the others. You could flip the axes to make a horizontal bar chart if your category names are long or if you prefer to read labels on the y‑axis.\n\n\n\n\nDesign tips for bar charts\nBar charts are powerful because they encode values using length, which humans perceive accurately when bars share a common baseline. Poor design choices can undermine that strength and unintentionally (or intentionally) mislead the reader. The following guidelines help ensure that bar charts communicate information honestly and clearly.\n\nStart the axis at zero\nBecause bar charts represent magnitude through bar length, the axis should almost always start at zero. Truncating the axis exaggerates differences by making small changes appear visually dramatic. While truncated axes are sometimes acceptable for line charts, they are almost always misleading for bar charts.\nWhat goes wrong if you don’t: Two categories with similar values may appear drastically different, leading readers to overestimate the importance of the difference.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeep consistent spacing\nBars should be evenly spaced, with visible gaps between them. The gaps reinforce that categories are discrete, not continuous, and help the eye separate groups cleanly.\nWhat goes wrong if you don’t: Inconsistent or missing gaps can make bars blend together, confusing the chart with a histogram or suggesting unintended relationships.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSort deliberately\nThe order of bars should help the reader understand the data. Common choices include alphabetical order, chronological order, or ordering by value. A deliberate ordering allows patterns and comparisons to emerge naturally.\nWhat goes wrong if you don’t: Random or arbitrary ordering forces the reader to work harder and can hide meaningful trends.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAvoid clutter and gimmicks\nDecorative elements—such as 3-D bars, shadows, gradients, icons, or excessive labels—do not add information. Instead, they distort perception, make values harder to compare, and distract from the data itself.\nWhat goes wrong if you don’t: 3-D effects change apparent bar lengths depending on viewing angle, and visual clutter overwhelms the message.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy these rules matter\nEach of these principles protects the integrity of the visual message. Bar charts are most effective when they are simple, proportional, and transparent. Violating these guidelines can exaggerate differences, obscure patterns, or mislead readers—sometimes unintentionally, sometimes not.\nFollowing these design tips ensures that your bar charts support accurate interpretation and reinforce, rather than undermine, the credibility of your analysis.\n\n\nWhat is a pie chart?\nA pie chart is a graphical display used to show how a whole is divided into parts. The entire circle represents the total—typically 100% of the observations—and each slice corresponds to a category within a categorical variable. The size of each slice is proportional to the category’s share of the total, with larger proportions occupying larger angles and areas of the circle.\nPie charts are widely recognized and easy to interpret at a glance. Their circular shape immediately signals a “part-of-a-whole” relationship, making them intuitive for audiences without formal statistical training. For this reason, pie charts are often used in presentations, reports, and media when the goal is to convey how a total is allocated across a small number of categories.\nHowever, pie charts have important limitations. Human perception is better at comparing lengths along a common baseline than comparing angles or areas. As a result, it can be difficult to judge small differences between slices, especially when categories have similar proportions or when the chart contains many slices. Labels can also become crowded as the number of categories increases.\nBecause of these limitations, pie charts work best when:\n\nThere are only a few categories,\nThe proportions differ substantially,\nThe goal is to emphasize the overall composition rather than precise comparisons.\n\nIn situations where accurate comparison across categories is important, a bar chart often provides a clearer and more informative alternative. Nonetheless, when used sparingly and appropriately, pie charts can be an effective way to communicate simple part-whole relationships.\n\n\n\n\n\n\nExample 3.4: Reasons for missing an appointment\n\n\n\nA dental clinic tracks why patients miss scheduled cleanings. Out of 100 missed appointments, 50 were due to forgetfulness, 20 to fear, 15 to cost, and 15 to other reasons. A pie chart makes the share of each reason obvious.\n\n\n\n\n\n\n\n\n\nThe slices emphasize that half of the missed appointments were simply forgotten. However, imagine adding three more reasons of similar size. The slices would become crowded and hard to compare. Pie charts work only when the categories sum to a meaningful whole and there are no more than a few slices.\n\n\n\n\nWhen to use bar charts vs. pie charts\nAlthough the same categorical data can often be displayed using either a bar chart or a pie chart, the two serve different analytical purposes and are not interchangeable. Choosing the right chart depends on what you want the reader to notice and compare.\n\nUse a bar chart when you want to:\n\nCompare values across categories or between groups. Bar charts place values along a common baseline, making differences in magnitude easy to see. This is especially important when categories have similar values or when precise comparisons matter, such as comparing approval ratings across departments or test scores across teaching methods.\nDisplay statistics that do not form a meaningful whole. Many summary statistics—such as averages, medians, rates, or scores—do not add up to 100% and therefore do not represent parts of a single total. A pie chart would be inappropriate in these cases, while a bar chart can display these values clearly and honestly.\nShow many categories, even if some are small. Bar charts remain readable with a large number of categories, particularly when sorted or displayed horizontally. Small values can still be compared accurately, whereas tiny pie slices are difficult to see and label.\n\nBecause bar charts rely on length comparisons, they align well with how people naturally judge quantity, making them the default choice for most categorical comparisons.\n\n\nUse a pie chart only when:\n\nThe values represent parts of a whole that add up to 100%. Pie charts are specifically designed to show how a total is divided among categories. If the data do not represent proportions of a single whole, a pie chart is misleading.\nThe number of categories is small. Pie charts work best with a limited number of slices—ideally no more than four or five. Too many slices make the chart cluttered and difficult to interpret.\nThe goal is to communicate the overall composition, not precise differences. Pie charts are effective for conveying the big picture—such as showing that one category dominates the total—rather than for comparing closely sized categories.\n\n\n\n\nA practical rule of thumb\nIf you find yourself squinting at a pie chart to decide which slice is larger, that’s a sign the chart is doing too much. In those cases, switch to a bar chart. Because humans are far better at judging lengths than angles or areas, bar charts almost always provide clearer and more accurate comparisons.\nIn practice, bar charts should be your default choice, with pie charts reserved for simple, high-level part-of-a-whole messages.\n\n\nClustered and stacked bar charts\nSometimes you have two categorical variables and want to see how their categories interact. We introduced two‑way tables in Section 3.1; here’s how to graph them.\nA clustered (side‑by‑side) bar chart groups bars for each level of a second variable next to each other so you can compare across groups. For example, imagine you survey 120 patients about how satisfied they were with a new physical therapy program (satisfied, neutral, dissatisfied) and record whether they were in the treatment or control group. A clustered bar chart shows differences in satisfaction between the two groups.\n\n\n\n\n\n\n\n\n\nIn a stacked bar chart, bars for each category are stacked atop one another. This emphasizes the total size of each category but makes it harder to compare the segments across stacks. You might use a stacked chart to show how types of injuries (sprain, fracture, other) contribute to emergency visits across departments; if you convert each bar to 100% of its height, you get a 100% stacked bar chart that highlights composition within each group.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautions with stacked bars\nStacked bars are useful when you care about the total across categories, but they hide patterns in the middle segments. In the last plot, you can easily compare the overall emergency visits across departments and the share of fractures, but it’s harder to compare the “other” injuries across departments because their segments float at different heights. If your goal is to compare subgroups, a clustered bar chart is usually better.\n\n\n\nWorking in JMP\nIn JMP, bar and pie charts live in the Graph Builder. Drag your categorical variable to the X‑axis and drop the N or % statistic onto the Y‑axis to create a bar chart. To cluster by a second categorical variable, drop it in the Group or Overlay zone, and choose Bar (Horizontal) or Bar (Vertical) from the chart palette. To stack, use the Stack option in the legend. To make a pie chart, drag the categorical variable to a blank canvas and choose Pie; JMP will automatically convert counts to percentages and label the slices. Use the red triangle (▸) menu to display data labels, reorder slices, or combine small categories into an “Other” slice.\n\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition or note\n\n\n\n\nBar chart\nGraph that displays categories along one axis and uses the length of bars to represent numeric values; great for comparing counts or percentages across categories.\n\n\nPie chart\nCircular chart in which slices represent how a total is divided among categories; appropriate only when values sum to a meaningful whole and the number of categories is small.\n\n\nClustered (side‑by‑side) bar chart\nBar chart where categories are grouped side by side for levels of a second variable; useful for comparing groups across categories.\n\n\nStacked bar chart\nBar chart where bars for each subgroup are stacked; shows composition and totals but makes it harder to compare individual segments.\n\n\n\n\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nA clinical trial reports the average pain score (on a 0–10 scale) for three physical therapy programs. Should you use a bar chart or a pie chart? Explain your reasoning.\nA nutritionist surveys 500 patients about their preferred breakfast type: cereal, fruit, eggs, or none. The counts are 150, 120, 80, and 150. Sketch how you would display this information with a pie chart. When might a bar chart be preferable?\nIn a mental health study, participants are classified into stress levels (low, moderate, high) and whether they attended counseling (Yes/No). Which type of bar chart would you use to compare stress levels between counseling and non‑counseling participants? What pattern would indicate that counseling is associated with lower stress?\nA bar chart shows the average number of cavities per patient in three dental clinics: 1.2, 1.3 and 1.4 cavities. The y‑axis starts at 1.0. Explain why this design might mislead and how to fix it.\nGive two reasons why pie charts often make it harder to compare categories than bar charts.\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nYou should use a bar chart because the numbers represent average pain scores and do not sum to a meaningful whole. Pie charts imply a part‑to‑whole relationship and would be misleading here.\nThe four breakfast types form parts of the whole sample (500 patients). In a pie chart, the slices would be 30% cereal, 24% fruit, 16% eggs, and 30% none. However, a bar chart may be preferable because it allows you to order the categories from most to least common and makes it easier to compare the cereal and none categories, which are equal in size.\nA clustered (side‑by‑side) bar chart would let you compare the counts (or proportions) of low, moderate, and high stress within the counseling and non‑counseling groups. If counseling is associated with lower stress, you would expect the “low” bar to be taller (or the “high” bar shorter) in the counseling group than in the non‑counseling group.\nStarting the y‑axis at 1.0 truncates the bars and exaggerates small difference. To avoid misleading readers, start the axis at zero. Alternatively, use a dot plot or annotate the differences directly if the differences are small but meaningful.\nFirst, people judge lengths more accurately than angles; in a pie chart it is hard to gauge the exact size of a slice. Second, when slices are similar in size or there are many categories, comparing slices becomes difficult and the chart becomes cluttered. Bar charts avoid these issues by using a common baseline and allowing many bars.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Describing Data with Tables and Graphs</span>"
    ]
  },
  {
    "objectID": "03.html#sec-03_03",
    "href": "03.html#sec-03_03",
    "title": "3  Describing Data with Tables and Graphs",
    "section": "3.3 Organizing Quantitative Data",
    "text": "3.3 Organizing Quantitative Data\n\n“It’s not what you look at that matters, it’s what you see.” - Henry David Thoreau\n\nCategorical variables describe membership in distinct groups, but quantitative variables operate on an entirely different scale. Measurements such as height, blood pressure, cholesterol level, income, or reaction time can take on a wide range of numerical values along a continuum. When presented as a raw list, these numbers are often difficult to interpret on their own.\nTo extract meaning from quantitative data, we must organize and summarize it in a systematic way. One of the most effective approaches is to group numerical values into intervals and display the results in tables. These tables provide a structured summary of the data and serve as the foundation for graphical displays such as histograms, stem-and-leaf plots, and dotplots.\nWell-constructed tables help reveal key features of a distribution. They show where values tend to cluster, how widely the data are spread, and how observations accumulate across the range of possible values. By organizing quantitative data before visualizing it, we gain insight into its overall shape and structure that raw numbers alone rarely provide.\n\nConstructing frequency distributions\nA frequency distribution for quantitative data is a table that organizes numerical values into a set of non-overlapping intervals, called classes, and records how many observations fall into each class. Instead of examining individual data points one by one, a frequency distribution provides a structured summary that reveals the overall pattern of the data.\nEach class is defined by a lower limit and an upper limit, and together the classes cover the entire range of the dataset. The class width is the distance between consecutive class boundaries (or lower limits), and the range of the data is the difference between the maximum and minimum observed values. Thoughtful choices about the number of classes and their widths are essential: too few classes hide important structure, while too many classes make the table overly detailed and difficult to interpret.\nThe purpose of a frequency distribution is not to preserve every detail, but to highlight the shape, center, and spread of the data in a compact and interpretable form.\n\nSteps for constructing a frequency distribution\n\nDecide on the number of classes. A common guideline is to use between 5 and 20 classes. Fewer classes may oversimplify the distribution, masking important features such as skewness or multiple peaks. Too many classes, on the other hand, can make the distribution appear noisy and obscure the overall pattern.\nCompute the class width. Subtract the minimum value from the maximum value to obtain the range. Divide this range by the chosen number of classes, then round up to a convenient number. Rounding up ensures that all observations fit neatly into the defined classes without overlap.\nDetermine class limits. Choose a starting point at or slightly below the minimum value, then add the class width repeatedly to establish the remaining class boundaries. Consistent class widths make the table easier to read and interpret.\nTally the frequencies. Count the number of observations falling into each class. Each observation should belong to exactly one class.\n\n\n\n\n\n\n\nExample 3.5: Number of cavities filled (discrete data)\n\n\n\nSuppose a dental researcher records the number of cavities filled for 40 patients in a clinic over the past month. The raw data is shown below.\n\n\n3, 2, 2, 2, 3, 2, 4, 0, 2, 2, 2, 5, 2, 0, 2, 4, 1, 1, 3, 0, 1, 3, 0, 3, 1, 4, 3, 1, 3, 1, 2, 1, 2, 2, 2, 0, 0, 1, 6, 4\n\n\nWe want to summarize the distribution of cavities per patient.\n\n\n# A tibble: 7 × 2\n  class      frequency\n  &lt;fct&gt;          &lt;int&gt;\n1 (-0.5,0.5]         6\n2 (0.5,1.5]          8\n3 (1.5,2.5]         13\n4 (2.5,3.5]          7\n5 (3.5,4.5]          4\n6 (4.5,5.5]          1\n7 (5.5,6.5]          1\n\n\nBecause the data are counts, classes of width 1 are natural and easy to interpret. Each class corresponds to an exact number of cavities (0, 1, 2, 3, and so on). The frequency distribution shows how common each count is and immediately reveals whether most patients have few cavities or whether higher counts are common.\n\n\n\n\n\n\n\n\nExample 3.6: Systolic blood pressure (continuous data)\n\n\n\nNow consider a study measuring systolic blood pressure (in mmHg) for 60 adults. Unlike cavity counts, blood pressure is a continuous variable, so it makes sense to group values into wider intervals.\nThe 60 measurements are shown below.\n\n\n132.8, 108.8, 127.1, 123.7, 115, 87.3, 114, 109.7, 126.7, 117.9, 118.9, 114, 121.7, 121.6, 86.8, 145.2, 134.2, 128.3, 112.9, 135.3, 120.1, 122.5, 104.1, 147, 125.7, 153.6, 151, 125.9, 134.7, 150.9, 117.1, 127.5, 121.2, 130, 127.7, 142.5, 133.9, 111.6, 133.7, 112.6, 107.6, 136.7, 106.9, 129.6, 112.5, 146.3, 135.7, 119, 137, 131.4, 107.5, 121.9, 111, 131.7, 115.3, 121.5, 106.5, 110.6, 127, 110\n\n\nThe minimum measurement is 86.8 and the max is 153.6 which gives us a range of \\[\n\\text{max} - \\text{min} = 153.6 - 86.8 = 66.8\n\\]\nIf we choose seven classes, then we find the class width to be \\[\n\\frac{\\text{range}}{7}=\\frac{66.8}{7}=9.5428571\n\\]\nwhich we will round up to 10. We use 85 as the starting point since it is slightly below the minimum measurement. Now we can construct the intervals and find the frequencies for each.\n\n\n# A tibble: 7 × 2\n  class     frequency\n  &lt;fct&gt;         &lt;int&gt;\n1 [85,95)           2\n2 [95,105)          1\n3 [105,115)        15\n4 [115,125)        14\n5 [125,135)        17\n6 [135,145)         5\n7 [145,155)         6\n\n\nHere, the classes span 10 mmHg intervals (e.g., 85–95, 95-105, etc.). This grouping smooths out small measurement differences and highlights the overall distribution, where most values cluster, how spread out the measurements are, and whether unusually high or low values appear.\n\n\n\n\n\nRelative and cumulative frequencies\nRaw counts are helpful, but sometimes we want to know what proportion of observations fall into each class. The relative frequency of a class is the class frequency divided by the total number of observations.\nIf you add successive relative frequencies as you move across the classes, you get the cumulative frequency, which tells you the proportion of observations less than or equal to a given class boundary. Each entry is the sum of the current class’s relative frequency and all those below it. Because it represents a running total of proportions, the cumulative frequency increases monotonically and must end at 1 (or 100%) in the final class, reflecting that all observations have been accounted for.\nA graphical display of cumulative frequencies is called an ogive (pronounced “O-jive”). An ogive plots the cumulative proportion (or cumulative percentage) on the vertical axis against the class boundaries on the horizontal axis. Each point on the ogive shows the proportion of observations that are less than or equal to a given value.\n\n\n\n\n\n\nExample 3.7: Relative and cumulative frequencies for cavity data\n\n\n\nWe can extend our cavities example to compute relative and cumulative frequencies.\n\n\n\n\n\nclass\nfrequency\nrelative_frequency\ncumulative_frequency\n\n\n\n\n(-0.5,0.5]\n6\n0.150\n0.150\n\n\n(0.5,1.5]\n8\n0.200\n0.350\n\n\n(1.5,2.5]\n13\n0.325\n0.675\n\n\n(2.5,3.5]\n7\n0.175\n0.850\n\n\n(3.5,4.5]\n4\n0.100\n0.950\n\n\n(4.5,5.5]\n1\n0.025\n0.975\n\n\n(5.5,6.5]\n1\n0.025\n1.000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 3.8: Relative and cumulative frequencies for blood pressure data\n\n\n\n\n\n\n\n\nclass\nfrequency\nrelative_frequency\ncumulative_frequency\n\n\n\n\n[85,95)\n2\n0.0333333\n0.0333333\n\n\n[95,105)\n1\n0.0166667\n0.0500000\n\n\n[105,115)\n15\n0.2500000\n0.3000000\n\n\n[115,125)\n14\n0.2333333\n0.5333333\n\n\n[125,135)\n17\n0.2833333\n0.8166667\n\n\n[135,145)\n5\n0.0833333\n0.9000000\n\n\n[145,155)\n6\n0.1000000\n1.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing the number of classes\nSelecting an appropriate number of classes is more art than science, but a few guidelines help. Many textbooks suggest between 5 and 20 classes; an alternative rule of thumb for histograms is to use 5 to 15 bars. As the sample size grows, more classes allow finer detail, but too many classes produce a sparse table. In practice, try several class widths and see which one provides a clear picture of the data. Software like JMP and R will automatically suggest class widths, but you can adjust them manually.\n\n\nWorking in JMP\nTo organize quantitative data in JMP:\n\nUse Analyze → Distribution and assign your quantitative variable to the Y‑axis. JMP will display a histogram along with a frequency table showing counts, percentages, and cumulative percentages. The red triangle (▸) menu lets you change the number of bins or show the cumulative distribution.\nTo extract a frequency table without a histogram, go to Analyze → Tabulate, drag your quantitative variable to the Drop Zone for columns, and select a statistic such as N or Percent. JMP’s Tabulate platform can create grouped summaries by another variable (e.g., cavities by gender).\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nClass\nAn interval into which quantitative data are grouped in a frequency distribution; each class has lower and upper limits.\n\n\nFrequency distribution\nA table that lists classes of quantitative data and the number of observations in each class.\n\n\nClass width\nThe difference between consecutive lower class limits, computed by dividing the data range by the number of classes.\n\n\nRelative frequency\nThe fraction or percentage of observations in a class, equal to the class frequency divided by the total sample size.\n\n\nCumulative frequency\nThe running total of frequencies (or relative frequencies) up to a given class, used to construct an ogive.\n\n\nOgive\nA graph of cumulative frequencies or cumulative relative frequencies versus the upper class boundary, useful for identifying percentiles.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nThe heights (in centimeters) of 50 adolescents undergoing orthodontic treatment are recorded. Explain how you would construct a frequency distribution for these heights. How might your choice of the number of classes affect your ability to see the shape of the distribution?\nDifferentiate between a relative frequency distribution and a cumulative frequency distribution. In what situations would you prefer to look at cumulative frequencies instead of simple frequencies?\nSuppose a hospital’s quality control team records the time (in minutes) it takes to triage 100 emergency room patients. The times range from 1 to 28 minutes. If you decide to use eight classes in your frequency distribution, what is the class width? After building the frequency table, describe how you would create an ogive for the data.\nA dataset contains the number of decayed teeth per child in a dental study. Why might a class width of 1 be appropriate for this frequency distribution? When might you choose a wider class width?\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nConstructing a frequency distribution. List the minimum and maximum heights, decide on the number of classes (e.g., between 5 and 15), compute the class width by dividing the range by the number of classes, and set class limits starting at or below the minimum. Tally the number of heights falling into each class. Fewer classes smooth out details; more classes reveal fine structure but may result in many empty or low‑frequency intervals. Experimenting with different numbers of classes helps you see the underlying shape.\nRelative vs. cumulative frequency. A relative frequency distribution reports the proportion of observations in each class. A cumulative frequency distribution adds the proportions successively to show the total proportion up to each class. Cumulative frequencies are useful when you care about percentiles or thresholds—for example, determining the percentage of patients whose heights are below a certain value.\nCalculating class width and drawing an ogive. The range is 28 − 1 = 27 minutes. Dividing by eight classes gives 3.375 minutes; rounding up to 4 minutes yields a class width of 4. You would start your first class at or below 1 minute and create successive 4‑minute intervals (e.g., 0–4, 4–8, … ). After tallying frequencies, compute cumulative frequencies and plot them against the upper class boundaries to form the ogive.\nChoosing class width for count data. Decayed teeth are counted in whole numbers, so using a class width of 1 preserves each distinct count (0, 1, 2, …) and yields an easy‑to‑interpret distribution. If the counts range widely or if some counts are rare, a wider class width (e.g., grouping 4–6 teeth together) could reduce sparsity and simplify the table.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Describing Data with Tables and Graphs</span>"
    ]
  },
  {
    "objectID": "03.html#sec-03_4",
    "href": "03.html#sec-03_4",
    "title": "3  Describing Data with Tables and Graphs",
    "section": "3.4 Histograms, Stem‑and‑Leaf Plots, and Dotplots",
    "text": "3.4 Histograms, Stem‑and‑Leaf Plots, and Dotplots\n\nThe purpose of [data] display is comparison (recognition of phenomena), not numbers… The phenomena are the main actors, numbers are the supporting cast. - John Tukey\n\nOnce quantitative data are grouped into classes, we can visualize the distribution using several complementary plots. Histograms show how the data are distributed across intervals; stem‑and‑leaf plots preserve individual values while offering a quick visual summary; and dotplots display each observation as a dot. Choosing the right tool depends on your sample size, measurement scale, and the story you want to tell.\n\nHistograms\nA histogram is a graphical display for quantitative data that shows how values are distributed across a range. Like a bar chart, it uses rectangular bars, but the interpretation is fundamentally different. Each bar in a histogram represents a class interval of numerical values, and the height of the bar shows how many observations fall within that interval (or the proportion of observations, if relative frequencies are used).\nBecause quantitative data are measured on a continuous scale, the bars in a histogram touch. This visual continuity emphasizes that values flow smoothly from one class to the next, unlike categorical bar charts where gaps indicate distinct groups. The horizontal axis is measured in the original units of the data, while the vertical axis displays either frequency, relative frequency, or density.\nHistograms are especially useful for revealing the shape, center, and spread of a distribution. They allow us to see whether data are symmetric or skewed, whether there are multiple peaks, and whether unusually large or small values (outliers) are present.\n\nConstructing a histogram\nA histogram is built directly from a frequency distribution. The steps are:\n\nConstruct a frequency distribution using classes of equal width (see Section 3.3).\nLabel the horizontal axis with the class boundaries and the vertical axis with frequencies or relative frequencies.\nDraw adjacent rectangles whose widths equal the class widths and whose heights equal the corresponding frequencies.\n\nIn a density histogram, the bar heights are scaled so that the area of each bar equals the class’s relative frequency. In that case, the total area of the histogram equals 1.\nThe number of classes (bins) strongly affects the appearance of a histogram. Too few bins oversmooth the data and hide structure; too many bins create a jagged, noisy plot. A common guideline is to use between 5 and 15 bins, adjusting as needed to highlight meaningful patterns rather than random variation. As with bar charts, decorative effects such as 3-D bars should be avoided, they distort perception without adding information.\n\n\n\n\n\n\nExample 3.9: Resting heart rates\n\n\n\nConsider a study of the resting heart rates (in beats per minute) of 100 individuals undergoing physical therapy. Resting heart rate is a continuous quantitative variable, so grouping values into intervals is appropriate.\nThe 100 measurements are shown below.\n\n\n75, 70, 76, 80, 73, 69, 73, 69, 67, 76, 67, 56, 67, 76, 79, 68, 70, 73, 59, 89, 68, 76, 93, 75, 69, 60, 63, 70, 67, 76, 62, 85, 60, 75, 65, 60, 72, 58, 77, 78, 80, 82, 65, 67, 58, 79, 66, 71, 77, 74, 73, 60, 69, 79, 73, 79, 75, 63, 64, 70, 75, 75, 74, 81, 61, 50, 63, 67, 84, 67, 69, 64, 63, 51, 66, 78, 66, 82, 64, 65, 61, 82, 72, 62, 70, 70, 80, 62, 62, 78, 75, 63, 84, 71, 67, 62, 55, 58, 63, 72\n\n\nThe minimum heart rate is 50 and the maximum is 93, giving a range of \\[\n\\text{range} = 93 - 50 = 43.\n\\]\nSuppose we choose 10 classes. The class width is \\[\n\\frac{\\text{range}}{10} = \\frac{43}{10}\n\\] which we round up to a convenient value of 5 beats per minute. We start slightly below the minimum value at 47.5 and construct the class intervals.\n\n\n# A tibble: 10 × 2\n   class       frequency\n   &lt;fct&gt;           &lt;int&gt;\n 1 [47.5,52.5)         2\n 2 [52.5,57.5)         2\n 3 [57.5,62.5)        15\n 4 [62.5,67.5)        23\n 5 [67.5,72.5)        18\n 6 [72.5,77.5)        21\n 7 [77.5,82.5)        14\n 8 [82.5,87.5)         3\n 9 [87.5,92.5)         1\n10 [92.5,97.5)         1\n\n\nThis frequency distribution shows how heart rates accumulate across the range. From this table, we construct the histogram by drawing adjacent bars whose heights correspond to these frequencies.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 3.10: Daily delivery times\n\n\n\nA logistics company records the delivery times (in minutes) for 75 packages delivered within a city on a given day. Delivery time is a continuous quantitative variable, so we again begin by constructing a frequency distribution.\n\n\n31.5, 31.3, 27.3, 41, 46.5, 49.9, 38.1, 38.3, 50, 29.1, 28.8, 32.8, 31.8, 39, 40.7, 35.8, 47.4, 58.6, 28.8, 46.4, 37.5, 45.7, 42.3, 44.3, 51.8, 22.8, 44.5, 24.6, 41.9, 36.4, 40.7, 27.9, 52.6, 38.4, 39, 54.6, 47.5, 25.4, 52.7, 39.8, 42.5, 52.8, 34.4, 41.9, 40.1, 48.7, 54.8, 25.1, 37.5, 26.8, 61.9, 37.9, 41.3, 42.4, 52.2, 53.5, 60.7, 45.5, 42.7, 17.7, 43.6, 34.3, 43.9, 50.9, 47.6, 56.3, 21.9, 57.9, 61.5, 44.3, 39.4, 44, 49, 40, 39.2\n\n\nThe minimum delivery time is 17.7 minutes and the maximum is 61.9 minutes, giving a range of \\[\n61.9 - 17.7 = 44.2.\n\\]\nSuppose we choose 8 classes. The class width is \\[\n\\frac{\\text{range}}{8} = \\frac{44.2}{8},\n\\] which we round up to 6 minutes. We start at 17 minutes and construct the intervals.\n\n\n# A tibble: 8 × 2\n  class   frequency\n  &lt;fct&gt;       &lt;int&gt;\n1 [17,23)         3\n2 [23,29)         8\n3 [29,35)         7\n4 [35,41)        17\n5 [41,47)        18\n6 [47,53)        13\n7 [53,59)         6\n8 [59,65)         3\n\n\nUsing this frequency distribution, we now draw the histogram.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy histograms matter\nHistograms transform a frequency distribution into a visual summary that highlights the shape of the data. They help us see where observations cluster, how variable the data are, and whether unusual values are present. Because many statistical methods rely on assumptions about distributional shape, histograms play a central role in both exploratory analysis and model checking.\n\nCaution about number of classes\nWhen constructing a histogram, the choice of the number of classes (bins) requires care. Too few classes oversmooth the data, hiding important features such as skewness, gaps, or multiple peaks. In contrast, too many classes make the histogram appear jagged and noisy, emphasizing random variation rather than meaningful structure. A well-chosen number of classes balances detail with clarity, revealing the overall shape of the distribution without distracting from its main features.\nFrom Example 3.10, suppose we chose 40 classes instead of 8. The resulting histogram would look like:\n\n\n\n\n\n\n\n\n\nFrom this histogram, we cannot determine much about the structure due to the jagged shape from too many bars.\nSuppose we chose 4 classes instead:\n\n\n\n\n\n\n\n\n\nHere, some of the features of the data is lost since the bars span too large of a class interval. We may want to know more on how the values are distributed in these large intervals. Fortunately, most software like JMP do a good job in finding a right balance in determining the number of classes.\n\n\n\nStem-and-leaf plots\nA stem-and-leaf plot is a simple display for quantitative data that preserves the original numerical values while providing a visual summary of the distribution. Each observation is split into two parts: a stem, consisting of the leading digit(s), and a leaf, consisting of the final digit. The stems are listed in increasing order in a column, and the leaves corresponding to each stem are written to the right.\nBecause stem-and-leaf plots retain every data value, they allow you to see the shape, center, and spread of the distribution and recover the original dataset if needed. This makes them especially useful for small to moderate sample sizes (typically fewer than about 100 observations), where displaying individual values is still manageable.\nUnlike histograms, which group data into intervals and sacrifice some detail, stem-and-leaf plots show exactly where each observation falls. At the same time, the ordered stems and leaves create a visual structure that reveals clustering, gaps, skewness, and extreme values.\n\n\n\n\n\n\nExample 3.11: Recovery times after knee surgery\n\n\n\nSuppose an orthopedic clinic records the time (in days) for 20 patients to regain full range of motion after knee surgery. We can construct a stem-and-leaf plot as follows:\n\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  0 | 88899\n  1 | 000002224\n  1 | 5578\n  2 | 14\n\n\n\n\nIn a hand-drawn stem-and-leaf plot, the stems would represent the tens of days (e.g., 1, 2, 3), and the leaves would represent the ones digit. Many statistical software packages split each stem into two rows, one for leaves 0–4 and another for leaves 5–9, to make patterns easier to see.\nFrom the plot, it is easy to see where most recovery times cluster and to identify unusually long recovery periods. Because the stems are ordered, the plot also highlights the overall shape of the distribution and any extreme values.\n\n\n\n\n\n\n\nExample 3.12:  Quiz scores in a class\n\n\n\nSuppose an instructor records the quiz scores (out of 100) for 80 students in an algebra course.\n\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  5 | 4\n  5 | 9\n  6 | 11234\n  6 | 5556799\n  7 | 0001223344\n  7 | 55566666666667788888999\n  8 | 000111222233334\n  8 | 555777789\n  9 | 00122\n  9 | 5689\n\n\nHere, the stems represent the tens digit of the score (5, 6, 7, 8, 9), and the leaves represent the ones digit. The stem-and-leaf plot shows that most scores fall in the 70s and 80s, with a few higher scores in the 90s and possibly some lower scores in the 50s.\n\n\n\n\nWhen stem-and-leaf plots are most useful\nStem-and-leaf plots are ideal when:\n\nThe dataset is small enough to display individual values clearly,\nYou want both a numerical listing and a graphical summary,\nPreserving the original data values is important.\n\nFor larger datasets, stem-and-leaf plots become cluttered, and histograms or dotplots are usually more effective. However, for exploratory analysis of modest-sized samples, stem-and-leaf plots provide a powerful blend of detail and visualization.\n\n\nDotplots\nA dotplot is a simple graphical display for quantitative data in which each observation is represented by a dot placed above a number line. When multiple observations have the same value, the dots are stacked vertically. This stacking makes frequencies visible while still preserving the individual data points.\nDotplots are especially useful for small to moderate datasets, where displaying every observation is feasible and desirable. Unlike histograms, which group values into bins, dotplots show the exact values observed. This makes it easy to identify clusters, gaps, and outliers, as well as to see the overall shape of the distribution.\nBecause dotplots work directly with individual values, they are flexible enough to handle both discrete variables (such as counts) and continuous variables (especially when the data are rounded). They are also quick to construct and intuitive to read, making them a popular choice for exploratory analysis and classroom demonstrations.\n\n\n\n\n\n\nExample 3.13:  Yearly clinic visits\n\n\n\nContinuing the dental theme, suppose researchers record the number of clinic visits per patient during a year for 25 orthodontic patients. The variable of interest—the number of visits—is a discrete quantitative variable.\n\n\n\n\n\n\n\n\n\nThe dotplot shows that most patients had between one and three visits, while a few had no visits or as many as 13 visits. Because each dot represents a patient, the plot makes it easy to see both the concentration of values and the less common outcomes.\n\n\n\n\n\n\n\n\nExample 3.14:  Daily coffee consumption\n\n\n\nSuppose a workplace survey records the number of cups of coffee consumed per day by 30 employees.\n\n\n\n\n\n\n\n\n\nThis dotplot reveals that most employees drink one to three cups per day, with a smaller number drinking four or more cups and a one employee drinking none at all. The stacked dots make it easy to compare frequencies while still seeing each individual’s value.\n\n\n\n\nWhen dotplots are most useful\nDotplots are an excellent choice when:\n\nThe dataset is small enough to display individual observations clearly,\nExact values matter,\nYou want to quickly identify clusters, gaps, and outliers.\n\nAs datasets grow larger, dotplots can become cluttered and difficult to read. In those cases, histograms provide a more effective summary. For small to moderate datasets, however, dotplots strike a valuable balance between numerical precision and visual insight.\n\n\nChoosing the right display\n\nHistograms reveal the overall shape of a distribution and are most useful for moderate to large samples. They sacrifice individual data values to provide a smooth picture of the distribution.\nStem‑and‑leaf plots preserve actual data values and work well for small to moderate samples. They quickly show the shape and allow you to recover the raw data.\nDotplots also preserve individual values and are easy to interpret for small datasets; they can handle discrete counts and continuous measurements.\n\n\n\nWorking in JMP\nIn JMP:\n\nTo create a histogram or stem‑and‑leaf plot, use Analyze → Distribution. For stem‑and‑leaf plots, JMP automatically splits stems if needed and prints the “leaf unit.” The red triangle (▸) menu lets you toggle between histogram and stem‑and‑leaf views.\nTo build a dotplot, open Graph Builder, drag your quantitative variable to the X‑axis, and choose “Dot Plot” from the element palette.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nHistogram\nA graph of adjacent rectangles representing classes of a quantitative variable; the horizontal axis is numeric and the bars touch.\n\n\nRelative frequency histogram\nA histogram in which the height of each bar shows the relative frequency (proportion) in that class, so the sum of the bar heights equals 1.\n\n\nStem‑and‑leaf plot\nA display that splits each data value into a stem (leading digits) and a leaf (trailing digit), preserving the original data.\n\n\nDotplot\nA plot that places a dot above each data value on a number line, with stacked dots representing repeated values.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nExplain why a histogram is drawn with adjacent bars touching. How does this design reinforce the type of variable being plotted?\nFor a set of 15 recovery times (in days) after a dental procedure, would you prefer a histogram, a stem‑and‑leaf plot, or a dotplot? Justify your choice.\nIn a stem‑and‑leaf plot, what do the stems and leaves represent? Why might you choose to split the stems into two rows for certain datasets?\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nWhy bars touch. The bars in a histogram are adjacent because the underlying variable is continuous, and there are no gaps between the classes. Drawing the bars together emphasizes that each class covers an interval on the number line; a gap would incorrectly suggest a break between values.\nChoosing a plot for 15 observations. For a sample of 15 recovery times, a stem‑and‑leaf plot or dotplot would preserve the individual values and make it easy to see exact times. A histogram could also work, but with so few observations its appearance would depend heavily on the chosen bin width. Stem‑and‑leaf plots are especially useful here because they reveal the shape and extremes while retaining the data.\nInterpreting stems and leaves. In a stem‑and‑leaf plot, the stem contains the leading digits of each number and the leaf is the final digit. Splitting stems into two rows—one for leaves 0–4 and one for leaves 5–9—provides more detail and smooths out long runs of leaves, making the distribution easier to read.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Describing Data with Tables and Graphs</span>"
    ]
  },
  {
    "objectID": "04.html",
    "href": "04.html",
    "title": "4  Describing Data with Numbers",
    "section": "",
    "text": "4.1 Measures of Center (Mean, Median, Mode)\nOne of the first things we usually want to know about a numerical variable is where it centers. When we describe a value as “typical,” we are really asking a practical question: If we had to summarize this entire group with a single number, which number would best represent it?\nIn this section, we explore three common measures of center -the mean, median, and mode -and consider when each provides a sensible description of what is typical.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Describing Data with Numbers</span>"
    ]
  },
  {
    "objectID": "04.html#sec-measures_of_center",
    "href": "04.html#sec-measures_of_center",
    "title": "4  Describing Data with Numbers",
    "section": "",
    "text": "“You keep using that word. I do not think it means what you think it means.” - Inigo Montoya\n\n\n\n\nWhat do we mean by “typical”?\nA good measure of center should capture the central tendency of the data without being overly influenced by extreme values. However, no single summary works best in every situation. The “best” notion of typical depends on the shape of the distribution, the presence of outliers, and the context of the data.\nThe three most common measures of center, mean, median, and mode, answer the question of “typical” in different ways.\n\n\nThe mean (arithmetic average)\nThe mean of a sample, often denoted \\(\\bar{x}\\), is the sum of all observations divided by the number of observations. If you imagine placing each data value as a weight on a number line, the mean is the balance point -the location where the line would balance perfectly.\nFormally, for sample values \\(x_1, x_2, \\dots, x_n\\), the mean is \\[\n\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i.\n\\]\nBecause the mean uses every observation, it incorporates all available information and is mathematically convenient. However, this same feature makes the mean sensitive to outliers. A single unusually large or small value can pull the mean away from where most of the data lie, making it a poor description of what is typical.\n\n\n\n\n\n\nExample 4.1: Scutari data\n\n\n\nFlorence Nightingale is well known for her work in improving hospital conditions for British soldiers during the Crimean War. One dataset she examined recorded outcomes for soldiers treated in hospitals in Crimea and in Scutari, Turkey1.\nThe file scutari.jmp contains the number of soldiers hospitalized in various regiments and the number who died. Consider the number of soldiers who died in Scutari for the first five regiments:\n\n\n93, 96, 42, 155, 94\n\n\nThe mean is \\[\n\\begin{aligned}\n\\bar{x} &= \\frac{1}{5}(93 + 96 + 42 + 155 + 94) \\\\\n&= \\frac{480}{5} \\\\\n&= 96\n\\end{aligned}\n\\]\nAt first glance, 96 seems like a reasonable “typical” value for these data.\nNow suppose a sixth regiment is added with 500 deaths. The data become:\n\n\n93, 96, 42, 155, 94, 500\n\n\nThe new mean is \\[\n\\begin{aligned}\n\\bar{x} &= \\frac{1}{6}(980) \\\\\n&= 163.3\n\\end{aligned}\n\\]\nDoes 163.3 feel typical for this group? Clearly not. Most regiments had death counts well below 200, but the single extreme value dramatically inflates the mean. This example illustrates how outliers can make the mean misleading as a measure of center.\n\n\n\n\n\n\n\n\nExample 4.2: Household income\n\n\n\nConsider annual household incomes (in thousands of dollars) for a small neighborhood:\n\\[\n45,\\, 48,\\, 50,\\, 52,\\, 55,\\, 58,\\, 60,\\, 62,\\, 65,\\, 420\n\\]\nMost households earn between $45,000 and $65,000 per year, but one household earns $420,000.\nThe mean income is \\[\n\\bar{x} = \\frac{45 + 48 + 50 + 52 + 55 + 58 + 60 + 62 + 65 + 420}{10}\n= 91.5\n\\]\nA “typical” income of $91,500 does not reflect the reality for most households in this neighborhood. The single high-income household pulls the mean far above where the majority of incomes lie. This is why average income statistics are often criticized.\n\n\n\n\nAdjusting the mean: trimmed means\nBecause of its sensitivity to extremes, the mean is sometimes modified to reduce the influence of outliers. A trimmed mean removes a fixed percentage of the smallest and largest observations before computing the average.\nFor example, a 10% trimmed mean drops the lowest 10% and highest 10% of values, then averages the remaining data. This approach provides a compromise: it retains much of the information used by the mean while reducing the impact of extreme values.\nTrimmed means are especially useful when data are moderately skewed or when a small number of outliers are present but not of primary interest.\n\nKey takeaway\nThe mean is a powerful and widely used measure of center, but it answers the question of “typical” only when extreme values are not overly influential. When outliers are present -or when the distribution is strongly skewed -the mean can give a distorted picture, motivating the use of alternative summaries such as the median or trimmed mean.\n\n\n\nThe median (middle value)\nThe median is the value that lies at the center of an ordered dataset. When the observations are arranged from smallest to largest, the median divides the data into two equal halves: 50% of the observations are at or below the median, and 50% are at or above it.\nHow the median is computed depends on whether the number of observations is odd or even:\n\nFor an odd number of observations, the median is the single middle value.\nFor an even number of observations, the median is the average of the two middle values.\n\nBecause the median depends only on the order of the data and not on their numerical magnitudes, it is said to be resistant to extreme values. Large outliers may change the mean substantially, but they have little or no effect on the median unless they alter the ordering of the central observations.\nThis resistance makes the median a particularly useful measure of “typical” when distributions are skewed or contain outliers.\n\n\n\n\n\n\nExample 4.3: Scutari data again\n\n\n\nRecall the Scutari death counts after adding an extreme value:\n\n\n42, 93, 94, 96, 155, 500\n\n\nThere are six observations, so the median is the average of the third and fourth values in the ordered list: \\[\n\\text{median} = \\frac{94 + 96}{2} = 95.\n\\]\nUnlike the mean, which was pulled upward by the extreme value of 500, the median remains close to where most of the data lie. This robustness is why medians are often preferred for skewed data such as incomes, house prices, or medical costs, where a small number of unusually large values are common.\n\n\n\n\n\n\n\n\nExample 4.4: Fuel efficiency of cars\n\n\n\nNow consider a dataset that contains fuel efficiency (in miles per gallon) for various automobiles tested in the 1970s.\nSuppose we examine the fuel efficiency of the first 11 cars in the dataset (an odd number of observations):\n\n\n14.3, 17.8, 18.1, 18.7, 19.2, 21, 21, 21.4, 22.8, 22.8, 24.4\n\n\nBecause there are 11 observations, the median is the 6th value in the ordered list:\n\n\n21\n\n\nThis value represents the fuel efficiency of a “typical” car in this subset. Even if one of the cars had extremely poor or extremely high fuel efficiency, the median would remain stable unless that extreme value crossed the center of the ordered data.\nThis example illustrates why the median is often reported for performance measures like fuel economy, commute times, or household expenses -contexts where a few extreme cases should not define what is typical.\n\n\nThe median answers the question of “typical” by focusing on position rather than magnitude. Its resistance to extreme values makes it a reliable measure of center for skewed distributions and data sets where outliers are common.\n\n\nThe mode (most frequent value)\nThe mode of a dataset is the value that occurs most frequently. Unlike the mean and median, which describe the numerical center of a distribution, the mode describes what is most common. Because it depends only on frequency, the mode can be used with categorical, ordinal, and quantitative data.\nA dataset may have:\n\nOne mode (unimodal) - a single most frequent value,\nTwo modes (bimodal) - often indicating two distinct groups,\nMore than two modes (multimodal) - suggesting additional structure or heterogeneity.\n\nThe mode is especially useful for categorical variables (such as favorite color or blood type) and discrete quantitative variables (such as number of visits or number of children), where identifying the most common outcome is meaningful. In these contexts, the mean may not even be defined, and the median may be less informative.\nFor continuous variables measured with fine precision, however, exact ties are uncommon. In such cases, the numerical mode may not be meaningful or may not exist at all. Instead, we often identify the mode visually as the peak of a histogram, representing the most densely populated region of the data rather than a single repeated value.\n\n\n\n\n\n\nExample 4.5: Clinic visit counts (discrete data)\n\n\n\nSuppose a dental clinic records the number of visits made by each of 30 patients over the course of a year. The variable is a discrete quantitative count.\n\n\n0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 5, 5, 8\n\n\nWe can find the mode by identifying the most frequently occurring value.\n\n\n Value Count\n     0     1\n     1     6\n     2    12\n     3     6\n     4     2\n     5     2\n     8     1\n\n\nIn this dataset, the most common number of visits is 3, making 3 the mode. Interpreted in context, this tells us that the typical patient visits the clinic about three times per year -not in the sense of an average, but in the sense of what happens most often.\nThis is a situation where the mode is particularly informative: patients and administrators may care most about the most common pattern of usage rather than a numerical balance point.\n\n\n\n\n\n\n\n\nExample 4.6: Highway fuel efficiency (continuous data)\n\n\n\nNow consider a dataset on fuel efficiency measurements for many car models.\nWe examine highway miles per gallon (hwy) for a subset of vehicles:\nBecause highway fuel efficiency is measured on a continuous scale, exact repeated values are not especially meaningful. Instead, we identify the mode as the peak of the distribution, using a histogram.\n\n\n\n\n\n\n\n\n\nFrom the histogram, we see that the highest bar occurs around 28-32 mpg, indicating the modal region of the distribution. Rather than a single numerical value, the mode here represents the most common range of highway fuel efficiency among these vehicles.\nThis approach is typical for continuous data: the mode is interpreted as a region of highest density rather than a single repeated observation.\n\n\n\n\nKey takeaway\nThe mode answers the question: What value occurs most often? It is indispensable for categorical data, useful for discrete counts, and informative for continuous data when interpreted through histogram peaks. While it does not use all the information in the data the way the mean does, the mode often aligns closely with how people intuitively think about what is “typical.”\n\n\nThe shape of a distribution\nWhen examining a data distribution of a quantitative variable, whether portrayed by a frequency table or by a graph such as a histogram, we should look for clear peaks. Does the distribution have a single mound? A distribution of such data is called unimodal.\nA distribution with two distinct mounds is called bimodal. A bimodal distribution can result, for example, when a population is polarized on a controversial issue. Suppose each subject is presented with ten scenarios in which a person found guilty of murder may be given the death penalty. If we count the number of those scenarios in which subjects feel the death penalty would be just, many responses would be close to 0 (for subjects who oppose the death penalty generally) and many would be close to 10 (for subjects who think it’s always or usually warranted for murder).\nA bimodal distribution can also result when the observations come from two different groups. For instance, a histogram of the height of students at a university might show two peaks, one for females and one for males.\n\n\n\n\n\n\nExample 4.7: Highway fuel efficiency (bimodal)\n\n\n\nBelow is another subset of vehicles from the data described in Example 4.6.\n\n\n\n\n\n\n\n\n\nWe see here that there are two peaks thus we call this distribution bimodal. In Example 4.6, the subset of vehicles were just 4 cylinder (smaller engine) vehicles. The data in this histogram includes vehicles with 4, 6, and 8 cylinders (larger engines). Vehicles with larger engines tend to have less highway miles per gallon than those with smaller engines. The distributions of both of these groups (small and large engines) are mixed together in this histogram giving us the bimodal shape.\n\n\n\nDistribution Shapes\nWhat is the shape of the distribution? The shape of the distribution is often described as symmetric or skewed.\nA distribution is symmetric if the side of the distribution below a central value is a mirror image of the side above that central value. The distribution is skewed if one side of the distribution stretches out longer than the other side.\nTo skew means to stretch in one direction. A distribution is skewed to the left if the left tail2 is longer than the right tail. A distribution is skewed to the right if the right tail is longer than the left tail.\n\n\n\nChoosing the right measure\nHow do you decide which summary to use? A few guidelines can help:\n\nUse the mean when the distribution is roughly symmetric without outliers. The mean connects nicely to many statistical models and formulas.\nUse the median when the distribution is skewed or contains outliers. The median provides a better sense of the typical case when extremes are present.\nUse the mode when describing the most common category or when the data are naturally discrete. For continuous variables, speak of the “modal class” (the bin with the highest frequency).\n\nYou can also look at the relationship among mean, median, and mode to diagnose shape. In a symmetric distribution these summaries coincide. In a right‑skewed distribution the mean typically lies to the right of the median, and the mode is the smallest of the three; in a left‑skewed distribution the order reverses.\n\n\n\n\n\n\nExample 4.8: Illustration: skewness and the mean–median comparison\n\n\n\nThe following simulates data from a right‑skewed distribution and a symmetric distribution and plots them side by side. Notice how the mean and median behave.\n\n\n\n\n\n\n\n\n\nIn the right‑skewed distribution, the mean lies to the right of the median, reflecting the pull of larger values. In the nearly symmetric distribution, the mean and median are close together.\n\n\n\n\nWorking in JMP\nJMP makes it easy to compute and compare these summaries.\n\nUse Analyze→Distribution on a single numeric column. The report shows the Mean and Median under the “Summary Statistics” section, and a “Quantiles” table lists the median explicitly. The bar under the histogram marks the median with a vertical line.\nTo find the mode, examine the histogram (bins with the highest bars) or create a Tabulate table. Because continuous measurements rarely tie exactly, the notion of a mode is approximate.\nFor a trimmed mean, click the red triangle ▶ next to the variable name in the Distribution platform, choose Nonparametric→TrimmedMean, and select the trimming proportion.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nMean\nThe arithmetic average: sum of all observations divided by the number of observations; sensitive to extreme values.\n\n\nMedian\nThe middle value when data are ordered; half the observations are at or below it; resistant to outliers.\n\n\nMode\nThe most frequently occurring value (or class) in the data; useful for categorical or discrete variables.\n\n\nRight-skewed\nA distribution where the tail extends to the right; typically mean&gt;median&gt;mode.\n\n\nLeft-skewed\nA distribution where the tail extends to the left; typically mean&lt;median&lt;mode.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nA sample of commuting times (in minutes) for twelve workers is \\[\n10,\\ 12,\\ 12,\\ 15,\\ 16,\\ 17,\\ 18,\\ 18,\\ 18,\\ 20,\\ 22,\\ 90.\n\\] Use these value for the following:\n\nFind the median commuting time.\nFind the mean commuting time. How does the outlier affect the mean?\nIf you were advising a city planner about what most commuters experience, which summary (mean or median) would you report? Why?\n\nFor which of the following situations would the mode be a more informative summary than the mean or median? Explain your reasoning.\n\nThe test scores (0–100) from a class of 200 students.\nThe favorite ice-cream flavors (chocolate, vanilla, strawberry, etc.) of 150 customers.\nThe heights of 30 professional basketball players.\n\nThe median of five numbers is 8 and the mean is 10. If four of the numbers are 3,7,8, and 20, what is the fifth number? (Hint: Use the definitions of median and mean.)\nDescribe a situation in which the trimmed mean would be preferred over both the mean and the median.\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\na)Ordering the commuting times yields 10,12,12,15,16,17,18,18,18,20,22,90. With 12 observations, the median is the average of the 6th and 7th values: \\((17 + 18)/2 = 17.5\\) minutes. b)The mean is \\((10 + 12 + 12 + 15 + 16 + 17 + 18 + 18 + 18 + 20 + 22 + 90)/12 = 268/12 \\approx 22.33\\) minutes. The 90‑minute commute pulls the mean upward by about 5minutes compared to the median. c)For summarizing what most commuters experience, report the median (17.5minutes). It better represents the typical commute and is not inflated by the one long commute.\na)Mean or median. Test scores on a bounded 0–100 scale often form a roughly bell‑shaped distribution; either mean or median can represent typical performance. The mode might be less stable because exact scores can vary. b)Mode. Favorite flavors are categorical; reporting the most popular flavor (mode) is more meaningful than attempting to average flavors. c)Mean or median. Heights are quantitative; the mean or median conveys typical height. The mode is less useful because exact duplicates are rare and height is nearly continuous.\nWith five numbers, the median is the third when ordered. Because the median is 8, the third value (when the numbers are sorted) must be 8. The numbers we know are 3,7,8,20 and an unknown \\(x\\). After sorting them, the median (middle value) must be 8, so the sorted list must be 3,7,8,\\(x\\),20 (if \\(x \\leq 20\\)) or 3,7,8,20,\\(x\\) (if \\(x \\geq 20\\)). To satisfy a mean of 10, the sum of all five numbers is 5×10=50. The known sum is 3+7+8+20=38, so \\(x = 12\\). Check the ordering: 3,7,8,12,20 has median 8 and mean 10. Thus the fifth number is 12.\nTrimmed means are useful when you expect a few extreme observations in both tails but still want to use most of the data. For example, in judging gymnastics or diving, a panel of judges gives scores; to guard against unusually high or low scores (perhaps due to bias), competitions often drop the highest and lowest score and average the rest. A trimmed mean removes these extremes, producing a fairer overall score.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Describing Data with Numbers</span>"
    ]
  },
  {
    "objectID": "04.html#measures-of-variability",
    "href": "04.html#measures-of-variability",
    "title": "4  Describing Data with Numbers",
    "section": "4.2 Measures of Variability",
    "text": "4.2 Measures of Variability\n\n“In statistics, variation is the name of the game – without it there would be nothing to explain.” – David Salsburg\n\nIf measures of center describe what is typical, measures of variability describe how much the data deviate from that typical value. Two datasets can share the same mean or median yet behave very differently in practice. One may be tightly clustered around the center, while the other is widely scattered, with values far from typical. Without a measure of spread, a measure of center alone can give a misleading sense of consistency or predictability.\nVariability provides essential context. Knowing that the average commute time is 30 minutes is far less informative if commute times regularly range from 10 to 90 minutes than if they typically fall between 25 and 35 minutes. Measures of variability quantify this uncertainty and help us understand how much individual observations are likely to differ from the center.\n\nWhy variability matters\nUnderstanding variability is critical for assessing reliability, risk, and precision.\nIn everyday settings, variability affects decision-making. If daily commute times vary widely, planning becomes difficult and the “typical” commute time offers little reassurance. If test scores in a class are highly variable, a single average score may not accurately reflect most students’ experiences.\nIn applied settings, variability is often more important than the center. In manufacturing and quality control, low variability in part dimensions indicates a stable and well-controlled process, even if the average dimension is slightly off target. High variability, on the other hand, signals inconsistency and potential defects. Similarly, in finance, two investments with the same average return may carry very different levels of risk if one fluctuates much more than the other.\nVariability also lies at the heart of inferential statistics. Measures such as variance and standard deviation quantify the natural randomness in data and determine how much confidence we can place in estimates, predictions, and conclusions. Larger variability generally means greater uncertainty, wider confidence intervals, and less precise inferences.\n\nMeasuring variability\nThere is no single “best” way to describe spread. Different measures emphasize different aspects of variability and are useful in different contexts. In this section, we will examine four commonly used summaries:\n\nThe range, which captures the total spread from the smallest to the largest value,\nThe interquartile range (IQR), which focuses on the spread of the middle 50% of the data and is resistant to outliers,\nThe variance, which measures average squared deviation from the mean,\nThe standard deviation, which expresses typical deviation from the mean in the original units of the data.\n\nTogether, these measures provide a toolkit for describing and comparing variability across datasets and for understanding how much individual observations tend to differ from what is typical.\n\n\n\nThe range\nThe range is the simplest measure of spread: it’s the difference between the largest and smallest values.\n\\[\n\\text{Range} = \\max(x) - \\min(x).\n\\]\nThe range is easy to compute and understand, but it depends only on two data points -making it very sensitive to outliers. If one observation is extreme, the range may exaggerate typical variability.\nTwo data sets can have the same range and be vastly different with respect to data variation.\nFor Example, Consider the data set A: \\[1, 3, 5, 6, 8, 9, 10, 15\\] and data set B: \\[1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 15\\]\nBoth data sets have the same range but are very different in how the data are spread out.\n\n\nQuartiles and the interquartile range (IQR)\nWhen describing variability, we often want a measure of spread that is not overly influenced by extreme values. One way to achieve this is by focusing on quantiles, which divide an ordered dataset into portions containing equal fractions of the data.\nThe first quartile (\\(Q_1\\)) is the value below which approximately 25% of the observations fall, and the third quartile (\\(Q_3\\)) is the value below which approximately 75% of the observations fall. Recall from Section 4.1, the median separates the data into two equal halves. Thus, the median is also the second quartile (\\(Q_2\\)).\nThe interquartile range (IQR) measures the spread of the middle half of the data and is defined as \\[\n\\text{IQR} = Q_3 - Q_1.\n\\]\nBecause the IQR depends only on the central 50% of the data, it is a resistant measure of spread. Extremely large or small observations in the tails have little effect on it. For this reason, the IQR is especially useful for skewed distributions and data sets that contain outliers.\nThe IQR plays a central role in boxplots and in common rules for identifying potential outliers, which we will study in the next section.\n\n\nA note on quartile definitions\nUnlike the mean or median, quartiles do not have a single universally accepted method for finding them. Different textbooks, software packages, and statistical traditions use slightly different rules for determining \\(Q_1\\) and \\(Q_3\\), particularly when the sample size is not divisible by four.\nSome common approaches include:\n\nComputing quartiles as specific percentiles using interpolation,\nDefining quartiles as the medians of the lower and upper halves of the data,\nUsing weighted averages when the quartile position falls between two observations.\n\nAs a result, you may see small numerical differences in reported quartiles depending on the method used. These differences are usually minor and do not change the overall interpretation, but they are important to be aware of when comparing results across software.\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nJMP determines quartiles based on the following steps:\n\nSort the \\(n\\) values in ascending order.\nCompute the location index for the \\(p\\text{th}\\) percentile as \\[\nr=(n+1)\\frac{p}{100}\n\\]\nIf \\(r\\ge n\\), then the \\(p\\text{th}\\) percentile is the maximum value in the data.\nIf \\(r\\le 1\\), then the \\(p\\text{th}\\) percentile is the minimum value in the data.\nIf \\(r\\) is an integer, then the \\(p\\text{th}\\) percentile is the \\(r\\text{th}\\) value in the ordered data.\nIf \\(r\\) is not an integer, then the \\(p\\text{th}\\) percentile is found using the formula \\[\np\\text{th percentile} = (1-f)y_i+(f)y_{i+1}\n\\] where \\(i\\) is the integer part of \\(r\\) and \\(f\\) is the fractional (decimal) part of \\(r\\).\n\nThe JMP documentation provides an example where \\(n\\) is 15 and the 75th and 90th percentiles are found. The value of \\(r\\) for each is calculated to be \\[\n\\begin{align*}\nr=(15+1)\\frac{75}{100}=12\\qquad\\qquad r=(15+1)\\frac{90}{100}=14.4\n\\end{align*}\n\\]\nSince \\(r\\) is an integer, then the 12th value in the sorted dataset is the 75th percentile.\nSuppose the values are\n\n\n11, 86, 33, 77, 5, 16, 52, 42, 50, 6, 89, 82, 100, 31, 64\n\n\nThese values are sorted in ascending order:\n\n\n5, 6, 11, 16, 31, 33, 42, 50, 52, 64, 77, 82, 86, 89, 100\n\n\nThe 12th value in this ordered dataset is 82. Thus the 75th percentile is 82.\nFor the 90th percentile, the value of \\(r\\) is not an integer. Therefore, the 95th percentile is found by finding \\(i=14\\) (since 14 is the integer part of 14.4) and \\(f=0.4\\) (since 0.4 is the decimal part of 14.4). The 90 percentile is then calculated as\n\\[\n\\begin{align*}\n90\\text{th percentile} &= (1-0.4)y_{14}+(0.4)y_{14+1}\\\\\n&= (0.6)89+(0.4)(100)\\\\\n&=93.4\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\nExample 4.9: Fuel efficiency of cars\n\n\n\nConsider the fuel efficiency (miles per gallon) of cars first examined in Example 4.4. This time, we will use all 32 observations.\n\n\n10.4, 10.4, 13.3, 14.3, 14.7, 15, 15.2, 15.2, 15.5, 15.8, 16.4, 17.3, 17.8, 18.1, 18.7, 19.2, 19.2, 19.7, 21, 21, 21.4, 21.4, 21.5, 22.8, 22.8, 24.4, 26, 27.3, 30.4, 30.4, 32.4, 33.9\n\n\nWe compute the first quartile, third quartile, and IQR using JMP:\n\nFrom the JMP output, we see that \\(Q_1=15.275\\) and \\(Q_3=22.8\\). This leads to an interquartile range of \\[\n\\text{IQR}=22.8-15.275=7.525\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy the IQR matters\nThe interquartile range focuses attention on the core of the distribution, where most observations lie. Its resistance to outliers makes it a natural companion to the median, particularly for skewed data. Together, the median and IQR provide a robust summary of center and spread that remains meaningful even when extreme values are present.\nAs we move forward, the IQR will play a key role in graphical summaries and in formal rules for detecting outliers.\n\nVariance and standard deviation\nFor a different measure of variability, we can calculate the distance and direction from the mean for each individual measurement. This is known as the deviation of the measurement. \\[\n\\text{deviation} = x - \\bar x\n\\]\nUsing these deviations, we can construct a more sensitive (as compared to the range) measure of variation.\n\n\n\n\n\n\nExample 4.10\n\n\n\nData set 1:\n1, 2, 3, 4, 5\nData set 2:\n2, 3, 3, 3, 4\nBoth datasets have a mean of 3.\nThe deviations for data set 1 are: \\[\n\\begin{align*}\n    (1-3), (2-3), (3-3), (4-3), (5-3) \\Longrightarrow -2, -1, 0, 1, 2\n\\end{align*}\n\\]\nThe deviations for data set 2 are: \\[\n\\begin{align*}\n    (2-3), (3-3), (3-3), (3-3), (4-3) \\Longrightarrow -1, 0, 0, 0, 1\n\\end{align*}\n\\]\n\n\nWhat information do these deviations contain?\nIf they tend to be large in magnitude, as in data set 1, the data are spread out, or highly variable.\nIf the deviations are mostly small, as in data set 2, the data are clustered around the mean, \\(\\bar x\\) , and therefore do not exhibit much variability.\nThe next step is to condense the information in these distances into a single numerical measure of variability.\nWhile not just average these values?\nYou see from the two example datasets above that some of the values are below the mean making the deviation negative. Other values are above the mean making the deviation positive. The negative deviations cancel out the positive deviations when you sum them up. In fact, the sum of deviations of values from the mean will always be zero.\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\n\\[\n\\begin{align*}\n    \\frac{1}{n}\\sum^n_{i=1}(x_i - \\bar x) &= {\\frac{1}{n}\\sum^n_{i=1}x_i - \\frac{1}{n}\\sum^n_{i=1}\\bar x}\\\\\n    &{ = \\bar x - \\frac{1}{n}n\\bar x}\\\\\n    & {= \\bar x - \\bar x}\\\\\n    &{ = 0}\n\\end{align*}\n\\]\n\n\n\nSo this won’t work. What we can do instead is square the deviations. By doing this and averaging3 them out, we will obtain what is known as the sample variance.\n\\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\n\\]\nBecause the variance uses the square of the units of measurement for the original data, its square root is easier to interpret. This is called the sample standard deviation.\n\\[\n\\begin{align*}\ns =& \\sqrt{s^2}\\\\\n=& \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2}\n\\end{align*}\n\\]\nSquaring the deviations ensures that positive and negative deviations don’t cancel out and gives more weight to larger deviations. Taking the square root returns the measure to the original units of the data. Standard deviation answers the question: on average, how far do observations fall from the mean?\nUnlike the IQR, the standard deviation is sensitive to outliers, because every deviation is squared.\n\n\n\n\n\n\nExample 4.11: Illustration comparing spreads\n\n\n\nConsider two small sets of exam scores. Group A scores are tightly clustered, and Group B scores vary widely even though their means are similar.\n\n\n\nExam scores for the two groups\n\n\nGroupA\n70\n72\n73\n74\n75\n76\n78\n\n\nGroupB\n60\n65\n70\n75\n80\n85\n90\n\n\n\n\n\nWe’ll compute the range, IQR, and standard deviation for each group.\n\n\n\nComparing measures of spread for two groups\n\n\n\nmean\nrange\nIQR\nsd\n\n\n\n\nGroup_A\n74\n8\n3\n2.65\n\n\nGroup_B\n75\n30\n15\n10.80\n\n\n\n\n\nIn this example, both groups have means around the mid‑70s (\\(\\bar{x}\\approx 74\\)). However, Group A’s range is 8 points and its IQR is 3 points, while Group B’s range is 30 points and its IQR is 15 points. The standard deviation for Group B is more than triple that of Group A. Even without a graph you can see that Group B’s scores are much more spread out.\n\n\n\n\n\nWorking in JMP\nTo examine variability in JMP:\n\nRange and IQR. In the Distribution platform, click the red triangle ▶ next to the variable name and select Save Quantiles or Quantiles to see \\(Q_1\\), \\(Q_2\\), \\(Q_3\\) and compute the IQR (\\(Q_3 - Q_1\\)). The range is visible from the minimum and maximum shown in the summary.\nVariance and standard deviation. These appear automatically in the “Summary Statistics” section of the Distribution report. Standard deviation is labeled “Std Dev,” and variance is its square.\nMultiple groups. To compare groups, use Analyze→Fit Y by X with a continuous response and a categorical factor. The side‑by‑side boxplots display medians, quartiles, and potential outliers, and the “Means and Std Dev” table summarizes each group’s mean and standard deviation.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nRange\nThe difference between the maximum and minimum values in a dataset; very sensitive to outliers.\n\n\nQuartile\nA value that divides ordered data into four equal parts; \\(Q_1\\) is the first quartile (25% mark) and \\(Q_3\\) is the third quartile (75% mark).\n\n\nInterquartile range (IQR)\nThe difference \\(Q_3 - Q_1\\); measures the spread of the middle half of the data; resistant to outliers.\n\n\nVariance\nThe average squared deviation from the mean; units are squared.\n\n\nStandard deviation\nThe square root of variance; a typical distance from the mean; sensitive to outliers but commonly used in many statistical formulas.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nFor the exam score data above (Group A and Group B), interpret the differences in standard deviation. Which group has more variability and why?\nA dataset of weekly hours spent exercising (in hours) for eight people is 2, 3, 3, 4, 4, 4, 5, 15.\n\nCompute the range, IQR, and standard deviation.\nHow does the outlier of 15 hours affect each measure of variability?\nIf you wanted to describe the spread for most people in this group, which measure would you report? Explain.\n\nExplain in your own words why we square deviations when computing variance. What would go wrong if we didn’t square them?\nTwo companies have average delivery times of 3 days. Company A has a standard deviation of 0.5 days, while Company B has a standard deviation of 2 days. Describe what this tells you about customer experiences with each company.\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nGroupB has a standard deviation of about 10.808 points, whereas GroupA’s standard deviation is about 2.65 points (as computed in the example). The larger standard deviation means GroupB’s scores are more spread out around the mean -students in GroupB vary widely in performance compared to the tight clustering of GroupA.\na)The ordered data are 2,3,3,4,4,4,5,15. The range is 15−2=13. \\(Q_1\\) is halfway between the 2nd and 3rd observations (3 and 3), so \\(Q_1 = 3\\); \\(Q_3\\) is halfway between the 6th and 7th observations (4 and 5), so \\(Q_3 = 4.5\\). The IQR is 4.5−3=1.5 hours. The mean is \\((2+3+3+4+4+4+5+15)/8 = 40/8 = 5\\) hours. The standard deviation (using \\(n-1=7\\) in the denominator) is about 4.14 hours. b) The outlier of 15 hours has a huge impact on the range (it becomes 13) and the standard deviation (4.14), both of which rise substantially. The IQR remains 1.5 because quartiles ignore the extreme values -so the IQR is more robust. c) To describe the spread for most people, the IQR is most appropriate because it captures the middle 50% of values and is not distorted by the one extreme exerciser.\nIf we summed deviations from the mean without squaring them, positive and negative deviations would cancel out, giving zero. Squaring each deviation ensures that all contributions to variability are positive and that larger deviations are weighted more heavily, which reflects their greater contribution to the overall spread.\nAlthough the average delivery time is the same for both companies, Company A’s small standard deviation means most deliveries take close to 3 days -customers can expect consistent service. Company B’s larger standard deviation indicates delivery times vary widely; some packages may arrive much sooner or much later than 3 days. Customers may perceive Company A as more reliable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Describing Data with Numbers</span>"
    ]
  },
  {
    "objectID": "04.html#identifying-outliers",
    "href": "04.html#identifying-outliers",
    "title": "4  Describing Data with Numbers",
    "section": "4.3 Identifying Outliers",
    "text": "4.3 Identifying Outliers\n\n“Would I rather be feared or loved? Easy, both. I want people to be afraid of how much they love me.” – Michael Scott\n\nIn any real dataset, not all observations behave nicely. Some values sit far away from the bulk of the data-much larger or much smaller than what we would consider typical. These observations are called outliers. Identifying outliers is a crucial step in data analysis because they can strongly influence summaries, visualizations, and statistical conclusions.\nOutliers arise for many reasons. Some are the result of errors, such as data entry mistakes (an extra zero added to a value) or instrument malfunctions. Others reflect rare but legitimate variation, such as unusually long hospital stays or extremely high incomes. In some cases, outliers are the most interesting observations in the dataset, pointing to unusual events, special populations, or meaningful departures from expected behavior.\nFor these reasons, outliers should never be ignored automatically or removed without justification. Instead, they deserve careful attention. The goal is not simply to label points as “bad,” but to detect them systematically, investigate their source, and decide-based on context-how they should be handled in the analysis.\n\nWhat counts as an outlier?\nThere is no single, absolute definition of an outlier. What counts as “far away” depends on the scale, shape, and purpose of the data. As a result, statisticians rely on rules of thumb that flag potential outliers -values that merit closer examination rather than automatic exclusion.\nTwo of the most commonly used approaches are:\n\nThe interquartile range (IQR) rule, which identifies outliers based on their distance from the middle 50% of the data and is resistant to skewness and extreme values.\nThe z-score rule, which measures how many standard deviations an observation lies from the mean and is most appropriate for roughly symmetric distributions.\n\nBoth methods provide objective criteria for highlighting unusual observations, but neither replaces judgment. In the sections that follow, we will explore how each rule works, when it is appropriate to use, and how to interpret flagged values responsibly.\nUltimately, identifying outliers is as much about understanding the data-generating process as it is about applying formulas.\n\nThe IQR rule\nOne of the most widely used methods for identifying potential outliers is the interquartile range (IQR) rule. This approach is based on the idea that the middle 50% of the data provides a stable reference for what is typical, even when extreme values are present.\nRecall that the interquartile range is defined as \\[\n\\text{IQR} = Q_3 - Q_1,\n\\] where \\(Q_1\\) is the first quartile and \\(Q_3\\) is the third quartile. The IQR measures the spread of the central half of the data and intentionally ignores the tails of the distribution.\nThe IQR rule defines two cutoff values, often called fences, that mark unusually low or unusually high observations:\n\nThe lower fence: \\(Q_1 - 1.5 \\times \\text{IQR}\\),\nThe upper fence: \\(Q_3 + 1.5 \\times \\text{IQR}\\).\n\nAny observation that falls below the lower fence or above the upper fence is flagged as a potential outlier. The factor of 1.5 is a convention rather than a strict law; it represents a balance between being too sensitive (flagging many ordinary values) and too conservative (missing truly unusual observations).\nA key advantage of the IQR rule is its resistance. Because \\(Q_1\\) and \\(Q_3\\) are determined by the ranks of the data rather than their magnitudes, extreme values have little influence on the fences themselves. This makes the IQR rule especially effective for skewed distributions or datasets with heavy tails, where methods based on the mean and standard deviation can be distorted.\nIt is important to emphasize that observations identified by the IQR rule are not automatically errors. They are simply values that merit closer inspection. In some contexts, these points may reflect data entry mistakes or measurement problems; in others, they may represent rare but meaningful cases that are central to the story the data are telling.\nFor this reason, the IQR rule is best viewed as a diagnostic tool-a systematic way to highlight unusual observations-rather than as a mechanical rule for deleting data.\n\n\n\n\n\n\nExample 4.12: Identifying outliers in fuel efficiency\n\n\n\nRecall in Example 4.9 that we found \\(Q_1=15.275\\), \\(Q_3=22.8\\), and \\(IQR=7.525\\) for the fuel efficiency data. We will use the IQR rule to check whether any cars have unusually low or high fuel efficiency compared to the rest.\nWe calculate the lower and upper fences as \\[\n\\text{Lower fence} = 15.275 - 1.5 \\times 7.525=3.9875, \\quad\n\\text{Upper fence} = 22.8 + 1.5 \\times 7.525=26.5625\n\\]\nRecall the data in ascending order shown in Example 4.9\n\n\n10.4, 10.4, 13.3, 14.3, 14.7, 15, 15.2, 15.2, 15.5, 15.8, 16.4, 17.3, 17.8, 18.1, 18.7, 19.2, 19.2, 19.7, 21, 21, 21.4, 21.4, 21.5, 22.8, 22.8, 24.4, 26, 27.3, 30.4, 30.4, 32.4, 33.9\n\n\nWe see there are no vehicles below the lower fence but there are five vehicles above the upper fence. If we were to examine these potential outliers, we would see tht these five vehicles are very light, fuel-efficient cars. These observations are not errors-they reflect genuine differences in vehicle design-but they are unusual relative to the bulk of the cars in the dataset.\n\n\n\n\nThe z‑score rule\nWe can place individual observations on a common, unit-free scale by standardizing them. Standardization uses the sample mean \\(\\bar{x}\\) and sample standard deviation \\(s\\) to convert each observation \\(x_i\\) into a z-score: \\[\nz = \\frac{x_i - \\bar{x}}{s}\n\\]\nA z-score tells us how far an observation is from the mean, measured in units of standard deviations rather than in the original units of the data. This makes z-scores especially useful for comparing values that come from different scales or contexts.\n\n\nInterpreting z-scores\n\nA positive z-score means the observation is above the mean.\nA negative z-score means the observation is below the mean.\nA z-score of 0 corresponds exactly to the mean.\nThe magnitude of the z-score indicates how unusual the observation is:\n\n\\(z = 1\\) means one standard deviation above the mean,\n\\(z = -2\\) means two standard deviations below the mean, and so on.\n\n\nBecause standard deviation reflects typical variability in the data, z-scores provide a natural way to judge whether a value is close to typical or unusually far from the center.\n\n\nWhy standardization is useful\nStandardization removes the original units of measurement and rescales the data so that the mean becomes 0 and the standard deviation becomes 1. This has several advantages:\n\nIt allows fair comparisons across variables measured on different scales (for example, test scores vs. reaction times).\nIt helps identify potential outliers, since values with large absolute z-scores lie far from the mean relative to the overall spread.\nIt forms the basis for many methods in inferential statistics, including confidence intervals, hypothesis tests, and normal probability calculations.\n\n\n\nCaution\nZ-scores rely on the mean and standard deviation, which are sensitive to outliers. As a result, standardization works best for roughly symmetric distributions without extreme values. In highly skewed datasets, z-scores may give a misleading impression of how unusual an observation truly is. In those cases, resistant measures such as the median and IQR may be more appropriate for assessing extremeness.\n\n\nThe Empirical Rule\nFor distributions that area approximately bell-shaped,\n\nabout 68% of the observations fall within 1 standard deviation of the mean, that is, between \\(\\bar x -s\\) and \\(\\bar x +s\\) (denoted \\(\\bar x \\pm s\\))\nabout 95% of the observations fall within 2 standard deviations of the mean \\((\\bar x \\pm 2s)\\)\nAll or nearly all (about 99.7%) observations fall within 3 standard deviations of the mean \\((\\bar x \\pm 3s)\\)\n\nThese results are known as the Empirical Rule.\n\n\n\n\n\n\nExample 4.13: Female heights\n\n\n\nMany human physical characteristics have bell-shaped distributions. Let’s explore height. The file heights.jmp contains heights of of 261 female students at the University of Georgia. Below is a histogram and summary statistics for these heights. Note that a height of 92 inches was not included below.\n\nThe histogram has approximately a bell shape. From the summary statistics, the mean and median are close, about 65 inches, which reflects an approximately symmetric distribution. The empirical rule is applicable. - About 68% of the observations fall between \\[\n\\begin{align*}\n\\bar x \\pm s &= 65.3 \\pm 3.0\\\\\n& = {(62.3, 68.3)}\n\\end{align*}\n\\]\n\nAbout 95% of the observations fall between \\[\n\\begin{align*}\n\\bar x \\pm 2s &= 65.3 \\pm 2(3.0)\\\\\n& = {(59.3, 71.3)}\n\\end{align*}\n\\]\nAbout 99.7% of the observations fall between \\[\n\\begin{align*}\n\\bar x \\pm 3s &= 65.3 \\pm 3(3.0)\\\\\n& = {(56.3, 74.3)}\n\\end{align*}\n\\]\n\nWe can examine the dataset and count how many observations fall in these intervals:\n\n187 observations, 72%, fall within (62.3, 68.3).\n248 observations, 95%, fall within (59.3, 71.3).\n258 observations, 99%, fall within (56.3, 74.3).\n\nIn summary, the percentages predicted by the empirical rule are near the actual ones.\n\n\n\n\nUsing the Empirical Rule to find outliers\nBased on the Empirical Rule, an observation with \\(|z| &gt; 3\\) might be considered an outlier. However, this method is not resistant-outliers inflate \\(s\\), which can mask their own detection -and it should be used only when the distribution is close to bell-shaped.\n\n\n\nWhat to do with outliers\nIdentifying an outlier is not the same as discarding it. Consider these steps:\n\nInvestigate. Check the raw data entry, measurement units, and collection context. Could the value be a typo or a mis-recorded unit? Is there an instrument calibration issue?\nUnderstand context. Sometimes extreme values are genuine and carry important information (e.g., rare species sightings, extreme weather events). Domain knowledge helps decide whether to keep them.\nReport and compare. It’s often useful to perform analyses both with and without the outlier to see how much it influences results. If conclusions change dramatically, acknowledge this in reporting.\nUse robust methods. When outliers are present and valid, robust statistics like the median, IQR, trimmed mean, or non‑parametric tests help mitigate their influence.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking in JMP\n\nBoxplots and outliers. Use Graph→GraphBuilder to create a boxplot. JMP plots points beyond 1.5×IQR as small circles by default. Hover over a point to see its value; right‑click to exclude or include it.\nDistribution platform. In Analyze→Distribution, the boxplot includes the fences and outlier points. You can click the red triangle ▶ and choose Nonparametric→Outlier Box Plot for additional options.\nExploring z‑scores. To compute z‑scores in JMP, create a new column and use Formula→Standardize. Then filter rows with |z|&gt;3 to flag potential outliers.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nIQR rule\nPoints more than 1.5×IQR below \\(Q_1\\) or above \\(Q_3\\) are flagged as potential outliers\n\n\nLower/Upper fence\nThresholds used in the IQR rule: \\(Q_1 - 1.5 \\times \\text{IQR}\\)\n\n\nz‑score\nStandardized value \\(z = (x - \\bar{x})/s\\); indicates how many standard deviations a point is from the mean.\n\n\nEmpirical Rule\nIn a normal distribution, about 68% of values lie within 1 standard deviation of the mean, 95% within 2 standard deviations, and 99.7% within 3 standard deviations.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nFor the dataset \\(3, 4, 5, 6, 7, 8, 9, 100\\):\n\nCompute the quartiles, IQR, and the 1.5×IQR fences.\nIdentify any outliers using the IQR rule.\nCompute z‑scores for each observation. Which values, if any, have |z|&gt;3?\n\nUsing GroupB’s exam scores from earlier (60, 65, 70, 75, 80, 85, 90), apply the IQR rule to determine whether any scores are outliers. Explain why or why not.\nSuppose heights of adult men follow a normal distribution with mean 70 inches and standard deviation 3 inches. Using z‑scores, what heights would be considered extreme outliers (|z|&gt;3)?\nDescribe two different actions you might take after flagging an outlier in a dataset. When would each be appropriate?\nExplain why the z‑score rule may fail to detect outliers in skewed distributions.\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\na)Ordering the data gives 3, 4, 5, 6, 7, 8, 9, 100. There are 8 values, so the median is the average of the 4th and 5th values: \\((6 + 7)/2 = 6.5\\). The lower half (3,4,5,6) has median \\((4 + 5)/2 = 4.5\\), so \\(Q_1 = 4.5\\). The upper half (7,8,9,100) has median \\((8 + 9)/2 = 8.5\\), so \\(Q_3 = 8.5\\). The IQR is 8.5−4.5=4. The fences are \\(Q_1 - 1.5\\times\\text{IQR} = 4.5 - 6 = -1.5\\) and \\(Q_3 + 1.5\\times\\text{IQR} = 8.5 + 6 = 14.5\\). b) Values below −1.5 or above 14.5 would be considered outliers. In this dataset, 100 &gt; 14.5, so 100 is an outlier. No value is below −1.5, so there are no lower outliers. c) The mean is \\((3+4+5+6+7+8+9+100)/8 = 142/8 = 17.75\\) and the standard deviation is about 31.63. The z‑score for 100 is \\((100 - 17.75)/31.63 \\approx 2.59\\). Because |2.59| is less than 3, the z‑score rule does not flag 100 as an outlier. This illustrates how outliers inflate the standard deviation and hide themselves.\nGroupB’s scores (60, 65, 70, 75, 80, 85, 90) have \\(Q_1 = 67.5\\) and \\(Q_3 = 82.5\\) (see the previous section’s example). The IQR is 15. The fences are 67.5−22.5=45 and 82.5+22.5=105. All scores lie between 45 and 105, so there are no outliers under the IQR rule.\nHeights more than 3 standard deviations from the mean are below 70−9=61inches or above 70+9=79inches. Thus, heights less than 61inches or greater than 79inches would be considered extreme outliers by the z‑score rule.\nPossible actions include: (1) Correcting or removing the outlier if it is due to a data entry or measurement error. This is appropriate when you have verified the value is incorrect. (2) Reporting results with and without the outlier and using robust methods. This is appropriate when the outlier is genuine but influential; you can present both analyses or use methods less sensitive to extreme values.\nIn skewed distributions, the standard deviation can be inflated by the skew, so the threshold of 3 standard deviations may be too high. Outliers can inflate \\(s\\) and thus produce smaller z‑scores than expected. The z‑score rule also assumes symmetry; in a skewed distribution the tail on one side is longer, so using a symmetric cutoff like |z| &gt; 3 may miss unusual observations in the longer tail.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Describing Data with Numbers</span>"
    ]
  },
  {
    "objectID": "04.html#the-fivenumber-summary-and-boxplots",
    "href": "04.html#the-fivenumber-summary-and-boxplots",
    "title": "4  Describing Data with Numbers",
    "section": "4.4 The Five‑Number Summary and Boxplots",
    "text": "4.4 The Five‑Number Summary and Boxplots\n\n“If it’s green or wriggles, it’s biology. If it stinks, it’s chemistry. If it doesn’t work, it’s physics or engineering. If it’s green and wiggles and stinks and still doesn’t work, it’s psychology. If it’s incomprehensible, it’s mathematics. If it puts you to sleep, it’s statistics.” - Anonymous (in Journal of the South African Institute of Mining and Metallurgy (1978))\n\nThe five-number summary provides a compact yet informative description of a numerical dataset by reducing it to five key values:\n\nthe minimum,\nthe first quartile (\\(Q_1\\)),\nthe median (\\(Q_2\\)),\nthe third quartile (\\(Q_3\\)), and\nthe maximum.\n\nTogether, these values describe the dataset’s location, spread, and extremes. The minimum and maximum indicate the overall range of the data, while \\(Q_1\\), the median, and \\(Q_3\\) describe how the data are distributed through the middle. In particular, the distance between \\(Q_1\\) and \\(Q_3\\) (the interquartile range) captures the spread of the central 50% of the observations and is resistant to outliers.\nThe five-number summary is especially useful because it does not rely on assumptions about the shape of the distribution. It works well for symmetric data, skewed data, and data containing outliers, making it a robust descriptive tool.\n\nThe Boxplot: A Visual Summary\nA boxplot, also called a box-and-whisker plot, provides a compact graphical summary of a numerical dataset based on the five-number summary. Rather than displaying every data point, a boxplot highlights key features of the distribution—center, spread, skewness, and potential outliers—in a clean and interpretable way.\nAt the heart of the boxplot is the box, which spans from the first quartile \\(Q_1\\) to the third quartile \\(Q_3\\). The length of this box is the interquartile range (IQR), representing the spread of the middle 50% of the data. A line drawn inside the box marks the median, providing a clear indicator of the dataset’s center.\nExtending outward from the box are the whiskers, which summarize the range of typical values. Under a common and widely used convention, the whiskers extend to the smallest and largest observations that are within \\(1.5 \\times \\text{IQR}\\) of the quartiles. Observations that fall beyond these limits are not included in the whiskers; instead, they are plotted individually, often as dots or small symbols, and labeled as potential outliers. This approach keeps extreme values from distorting the visual scale while still drawing attention to them.\nBecause a boxplot is built from quartiles, it is a resistant summary. The positions of the box and the median are largely unaffected by a small number of extreme observations, making boxplots especially useful for skewed distributions or data sets with outliers.\nA boxplot also conveys information about the shape of the distribution. If the median lies near the center of the box and the whiskers are of similar length, the distribution is roughly symmetric. If the median is closer to one end of the box or if one whisker is noticeably longer than the other, this suggests skewness. While boxplots do not show fine detail, they provide a quick diagnostic view of distributional features.\nOne of the greatest strengths of boxplots is their ability to facilitate comparisons across groups. By placing multiple boxplots side by side on a common scale, you can quickly compare centers, variability, skewness, and the presence of outliers across different populations or experimental conditions. For this reason, boxplots are a staple of exploratory data analysis and an essential tool for comparing numerical data across categories.\n\n\n\n\n\n\nExample 4.14: Illustration: boxplots for two groups\n\n\n\nLet’s revisit the exam score groups from the previous section. We’ll compute their five‑number summaries and draw side‑by‑side boxplots.\n\n\n\nFive-number summaries for Groups A and B\n\n\nStatistic\nGroup_A\nGroup_B\n\n\n\n\nMin\n70.0\n60.0\n\n\nQ1\n72.5\n67.5\n\n\nMedian\n74.0\n75.0\n\n\nQ3\n75.5\n82.5\n\n\nMax\n78.0\n90.0\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the table you can see that Group A’s scores run from 70 to 78 with a median of 74, while Group B’s scores run from 60 to 90 with a median of 75. The much longer whiskers and broader box of Group B reveal its greater variability.\n\n\n\n\nWorking in JMP\nYou can build boxplots quickly in JMP:\n\nOpen Graph→Graph Builder and drag your variable to the Y axis. Drag a grouping variable (if any) to the X axis. From the gallery of plots, select the boxplot icon (a box with whiskers) to overlay a boxplot on the graph. JMP uses the 1.5×IQR rule to draw whiskers and will display outliers as separate points.\nThe five‑number summary is visible in the Analyze→Distribution output. Click the red triangle ▶ next to the variable and choose Nonparametric→Quantiles to list the quartiles. JMP labels the minimum and maximum directly in the summary.\nRight‑click on the boxplot to toggle “Show Outliers” or adjust the whisker definition if needed.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nFive‑number summary\nThe five values \\(\\min, Q_1, Q_2, Q_3, \\max\\) summarizing the distribution’s location and spread.\n\n\nBoxplot\nA graphical display of the five‑number summary: a box from \\(Q_1\\) to \\(Q_3\\) with a line at the median, and whiskers extending to the min/max (or to 1.5×IQR).\n\n\nWhisker\nThe line segment extending from the box to the smallest or largest non‑outlier value.\n\n\n\n\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nConsider the dataset \\(5, 7, 8, 10, 12, 15, 18, 20, 25\\).\n\n\nCompute the five‑number summary (min, \\(Q_1\\), median, \\(Q_3\\), max).\nConstruct the corresponding boxplot. Describe its shape. Is the distribution symmetric or skewed?\n\n\nA boxplot shows \\(Q_1 = 60\\), \\(Q_2 = 75\\), \\(Q_3 = 90\\) with minimum 50 and maximum 120. The right whisker is much longer than the left whisker, and there are several points plotted individually above 105.\n\n\nWhat does the long right whisker indicate about the distribution’s skewness?\nUsing the 1.5×IQR rule, which observations (approximately) would be considered outliers?\n\n\nFor the dataset \\(40, 42, 44, 45, 48, 50, 51, 52, 90\\):\n\n\nCompute the five‑number summary.\nCalculate the IQR and determine the lower and upper fences for outliers (using 1.5×IQR).\nIdentify any outliers and explain how they would appear on a boxplot.\n\n\nExplain why boxplots are particularly useful for comparing multiple groups on the same scale. What aspects of the distributions can you quickly compare?\nDescribe at least one limitation of boxplots. When might they hide important features of the data?\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\na)Ordering the data gives 5,7,8,10,12,15,18,20,25. The minimum is 5 and the maximum is 25. The median (the fifth value) is 12. The lower half (5,7,8,10) has median \\((7 + 8)/2 = 7.5\\), so \\(Q_1 = 7.5\\). The upper half (15,18,20,25) has median \\((18 + 20)/2 = 19\\), so \\(Q_3 = 19\\). The five‑number summary is (5, 7.5, 12, 19, 25). b) The box would span from 7.5 to 19 with a line at 12. The whiskers would extend to 5 and 25. The distribution is slightly right‑skewed because the upper whisker (19→25) is longer than the lower whisker (7.5→5).\na)A long right whisker suggests a right‑skewed distribution—there are some relatively high values pulling the upper tail outward. b) The IQR is \\(Q_3 - Q_1 = 90 - 60 = 30\\). The upper fence is \\(Q_3 + 1.5\\times\\text{IQR} = 90 + 45 = 135\\). Any observation above 135 would be considered an outlier. Since the maximum is 120 (below 135), the individual points above 105 are unusual but do not meet the 1.5×IQR rule; they are “mild” outliers by some conventions. If the maximum were above 135, those values would be flagged as outliers.\na)The ordered data are 40,42,44,45,48,50,51,52,90. The minimum is 40 and the maximum is 90. The median (the fifth value) is 48. The lower half (40,42,44,45) has median \\((42 + 44)/2 = 43\\), so \\(Q_1 = 43\\). The upper half (50,51,52,90) has median \\((51 + 52)/2 = 51.5\\), so \\(Q_3 = 51.5\\). The five‑number summary is (40, 43, 48, 51.5, 90). b) The IQR is 51.5−43=8.5. The lower fence is \\(Q_1 - 1.5 \\times \\text{IQR} = 43 - 12.75 = 30.25\\). The upper fence is \\(Q_3 + 1.5 \\times \\text{IQR} = 51.5 + 12.75 = 64.25\\). c) Any observation below 30.25 or above 64.25 would be flagged as an outlier. In this dataset, only 90 exceeds 64.25, so 90 is an outlier. On a boxplot, the upper whisker would extend to 52 (the largest non‑outlier), and the value 90 would appear as a separate point above the whisker.\nBoxplots align multiple groups on the same axis, allowing you to compare medians, the spread of the middle half (box length), the overall range (whisker length), and skewness (relative whisker lengths) at a glance. This makes it easy to see which group has a higher central tendency, more variability, or more extreme values.\nBoxplots summarize distributions succinctly but omit details like multimodality or the exact shape of the tails. Two very different distributions can share the same five‑number summary. When the sample size is small or when you need to see the full distribution (e.g., to spot bimodality), a histogram or dotplot may be more informative.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Describing Data with Numbers</span>"
    ]
  },
  {
    "objectID": "04.html#choosing-appropriate-summary-statistics",
    "href": "04.html#choosing-appropriate-summary-statistics",
    "title": "4  Describing Data with Numbers",
    "section": "4.5 Choosing Appropriate Summary Statistics",
    "text": "4.5 Choosing Appropriate Summary Statistics\n\n“It is not known how many office robberies occur every second because there is no Wikipedia entry for office robbery statistics.” -Michael Scott\n\nNot all variables are created equal, and as a result, no single summary statistic is universally appropriate. The usefulness of a summary depends on two key features of the data: the type of variable being measured and the shape of its distribution. Choosing summaries that ignore these features can lead to misleading or meaningless conclusions.\n\nCategorical and binary data\nFor categorical variables the values represent labels or group membership rather than numerical magnitude. Because the categories have no inherent numerical meaning, arithmetic operations such as addition or averaging are not meaningful. As a result, summaries like the mean or standard deviation do not make sense for categorical data.\nInstead, categorical variables are best summarized using counts (frequencies), proportions (or percentages), and the mode, which identifies the most common category. These summaries answer natural questions such as How many?, What fraction?, and Which category occurs most often?\nFor example, if we record people’s favorite ice-cream flavors, a sensible summary would list how many respondents chose chocolate, vanilla, strawberry, and other flavors, along with the corresponding percentages. Reporting a “mean flavor” would be meaningless, because the categories do not lie on a numerical scale.\nGraphical displays for categorical data—such as bar charts or pie charts—are built directly from these counts or proportions and reinforce the idea that categorical data are about group membership rather than numerical measurement.\nA binary variable is a special case of a categorical variable with exactly two categories, such as yes/no, success/failure, pass/fail, or disease/no disease. Binary variables are common in practice and often arise as outcomes of interest.\nIt is common to code binary categories numerically, typically using 1 for “yes” or “success” and 0 for “no” or “failure.” While this coding does not turn the variable into a truly quantitative one, it allows for convenient calculation of proportions. In particular, the mean of a 0/1-coded binary variable equals the proportion of observations coded as 1.\nFor example, suppose 30 out of 100 students pass an exam. If we code “pass” as 1 and “fail” as 0, the mean of this variable is \\[\n\\frac{30 \\times 1 + 70 \\times 0}{100} = 0.30,\n\\] which is exactly the pass rate. In this context, the mean has a clear and useful interpretation—not as an average score, but as a proportion.\nThis dual interpretation makes binary variables especially important in statistics. While they are fundamentally categorical, their proportions can be analyzed using tools that resemble those for quantitative data.\n\n\nOrdinal data\nOrdinal variables consist of categories that have a natural order, but where the distances between categories are not known or not equal. Common examples include Likert-scale responses (such as Strongly disagree to Strongly agree), education levels, class ranks, or satisfaction ratings. The ordering conveys meaningful information—one category is clearly higher or lower than another—but the scale itself does not support precise numerical differences.\nBecause the spacing between categories is undefined, arithmetic operations can be problematic. For instance, consider a five-point Likert scale. While Agree is higher than Neutral, it is not clear that the “distance” from Neutral to Agree is the same as from Disagree to Neutral. As a result, computing a mean can be misleading, since it implicitly assumes equal spacing between levels.\nA safer and more interpretable approach is to summarize ordinal data using the mode, which identifies the most common response, along with the distribution of responses. Reporting counts or percentages for each category preserves the ordering without imposing questionable numerical assumptions. Bar charts are especially effective for visualizing ordinal data because they clearly show how responses are spread across the ordered categories.\nIn some contexts, researchers also report the median for ordinal data, since it depends only on order rather than numerical distance. This can be useful when the categories represent a clear progression. However, even then, the interpretation should remain tied to category labels rather than numerical values.\nIn practice, ordinal data require careful handling. Treating them as purely categorical ignores their ordering, while treating them as fully quantitative risks overstating precision. Thoughtful summaries—focused on modes, medians, and response distributions—strike a balance that respects the structure of the data and communicates results honestly.\n\n\nQuantitative data: symmetric vs. skewed\nFor quantitative variables, the choice of summary statistics depends critically on the shape of the distribution and the presence (or absence) of outliers. Unlike categorical or ordinal data, quantitative data support a wide range of numerical summaries—but not all summaries are equally informative in every situation. The same dataset can tell very different stories depending on whether the center and spread are described using non-resistant or resistant measures.\nA good first step is always to examine the distribution—using a histogram, dotplot, or boxplot—before deciding how to summarize it.\n\nSymmetric distributions with no outliers\nWhen a distribution is approximately symmetric and does not contain extreme values, the mean and standard deviation provide an efficient and interpretable summary.\n\nThe mean captures the balance point of the distribution and represents a natural notion of “typical.”\nThe standard deviation (or variance) describes the typical distance of observations from the mean.\n\nIn this setting, the mean and median are usually close, and no single observation dominates the summary. Because the standard deviation measures variability relative to the mean, it is most meaningful when the mean itself is representative of the data.\n\n\nSkewed distributions or data with outliers\nWhen a distribution is skewed or contains outliers, non-resistant measures like the mean and standard deviation can be misleading. A few extreme values may pull the mean far away from where most observations lie and inflate the standard deviation.\nIn these situations, resistant summaries are preferred:\n\nThe median provides a more stable description of a typical value because it depends only on order, not magnitude.\nThe interquartile range (IQR) summarizes variability in the middle 50% of the data and is not affected by extreme observations.\nA trimmed mean (such as a 10% trimmed mean) offers a compromise: it removes the most extreme values before averaging, reducing the influence of outliers while still using much of the data.\n\n\n\nQuantitative counts (discrete variables)\nQuantitative count data—such as the number of doctor visits, customer complaints, or emails received per day—occupy a middle ground. While counts are numerical and support means, they are often skewed, with many small values and a few large ones.\nFor count data, it is especially important to:\n\nExamine the distribution’s shape,\nBe cautious about relying solely on the mean,\nConsider reporting the median and IQR alongside or instead of the mean.\n\n\n\nA guiding principle\nFor quantitative data, there is no one-size-fits-all summary.\n\nSymmetric, well-behaved data → mean and standard deviation\nSkewed data or data with outliers → median and IQR (or trimmed mean)\n\nChoosing summaries that align with the distribution ensures that reported statistics are both meaningful and honest representations of the data.\n\n\n\n\n\n\nExample 4.15: Illustration: incomes vs. test scores vs. Likert responses\n\n\n\nThe following compares summary statistics for three small datasets: incomes (right‑skewed), test scores (symmetric), and Likert ratings (ordinal on a 1–5 scale). We compute the mean, median, IQR, standard deviation, and mode where appropriate.\nHere are the values:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIncomes\n25\n28\n30\n32\n35\n40\n45\n50\n120\n\n\nTest_scores\n70\n72\n73\n75\n76\n77\n78\n80\n82\n\n\nLikert_ratings\nStrongly disagree\nDisagree\nDisagree\nNeutral\nNeutral\nNeutral\nAgree\nAgree\nStrongly agree\n\n\n\n\n\n\n\n\nSummary statistics for different data types\n\n\nDataset\nMean\nMedian\nIQR\nSD\nMode\n\n\n\n\nIncomes\n45.00\n35\n15\n29.28\n25\n\n\nTest Scores\n75.89\n76\n5\n3.86\n70\n\n\nLikert Ratings\nNA\nNA\nNA\nNA\nNeutral\n\n\n\n\n\nInterpretation:\n\nIncomes. The mean income ($45k) is much higher than the median ($35k) because of the 120k outlier. The IQR (15k) shows that the middle half of incomes lies between $30k and $45k. For skewed data like this, report the median and IQR rather than the mean and standard deviation.\nTest scores. The mean (75.89) and median (76) are similar, and the distribution is narrow (SD = 4.03). Here, the mean and standard deviation are appropriate summaries.\nLikert ratings. The mode is 3. Reporting the distribution of responses conveys the typical sentiment without implying equal spacing.\n\n\n\n\n\n\nWorking in JMP\nJMP encourages choosing appropriate summaries by making you specify each column’s modeling type (Continuous, Nominal, Ordinal):\n\nNominal variables are summarized with counts and proportions. The Distribution platform shows frequency tables and bar charts; the mode is evident from the highest bar. Means are not computed.\nOrdinal variables can display medians and quartiles. In the Distribution platform, the Quantiles option lists \\(Q_1\\), median, and \\(Q_3\\). A Box Plot shows the spread without relying on means.\nContinuous variables get means and standard deviations by default. If the distribution is skewed, use the red triangle ▶ Transform options (e.g., Log) or the Nonparametric menu to request median and IQR. You can also save robust statistics using Save Summaries.\n\nIn JMP’s Graph Builder, dragging a categorical variable to the X axis and a continuous variable to Y will create boxplots by default, highlighting medians and quartiles rather than means.\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nA health‑care researcher records the number of emergency room visits last year for 50 individuals. The data are right‑skewed, with most people having 0–2 visits and a few having 5 or more. Which measure of center and spread would you report? Explain why.\nA survey asks respondents to rate their satisfaction on a 1–5 Likert scale (1 = Very dissatisfied, 5 = Very satisfied). For 300 respondents, the sample mean is 3.9, the median is 4, and the mode is 4. Which statistic(s) would you use to describe typical satisfaction? Why might the mean be misleading?\nFor a dataset of annual household incomes in a city, the mean is $65,000, the median is $45,000, the standard deviation is $40,000, and the IQR is $20,000.\n\n\nWhat does the large gap between the mean and the median tell you about the distribution?\nWhich measure(s) of center and spread would you report? Justify your choice.\n\n\nA manufacturer tracks the number of defective items per batch (a count variable). The mean number of defects per batch is 2.3 and the median is 1. What does this tell you about the distribution? Which summary statistic is more informative here?\nExplain why it doesn’t make sense to compute a mean for ZIP codes or product serial numbers. How would you summarize such data instead?\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nCounts of emergency room visits are discrete and skewed to the right. The median number of visits and the IQR are appropriate measures. The mean would be pulled upward by a few high counts and would not represent the typical person.\nFor Likert responses, the mode (4) best describe typical satisfaction. A mean of 3.9 suggests “almost 4,” but interpreting fractional satisfaction levels can be misleading because the scale’s steps are not necessarily evenly spaced. Reporting the distribution of responses (e.g., “60% rated 4 or 5”) provides context.\na)The mean ($65k) is much higher than the median ($45k), indicating a right‑skewed distribution with some very high incomes. The large standard deviation ($40k) and moderate IQR ($20k) reinforce that incomes vary widely, especially in the upper tail. b) Report the median and IQR as primary summaries because they better reflect the typical household and are less affected by very high incomes. You could also mention the mean and SD to provide a sense of the overall level and variability, but note their sensitivity to outliers.\nA mean of 2.3 and a median of 1 suggest that most batches have 1 or 2 defects, but some batches have many more, creating a right‑skewed distribution. The median is more informative because it reflects what happens in a typical batch. The mean is inflated by the few batches with many defects.\nZIP codes and serial numbers are identifiers, not quantities. Arithmetic on them (like averaging) has no meaning. They should be summarized by counts or modes if you need to know which codes occur most often. Other summaries (like proportions of orders by ZIP code) can answer meaningful questions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Describing Data with Numbers</span>"
    ]
  },
  {
    "objectID": "04.html#footnotes",
    "href": "04.html#footnotes",
    "title": "4  Describing Data with Numbers",
    "section": "",
    "text": "Small, H. (2020). Nightingale’s overlooked Scutari statistics. Significance, 17(6), 28–33.↩︎\nThe tails of a distribution are the parts that are for the lowest values and the highest values.↩︎\nWhy do we divide by \\(n-1\\) in the variance and standard deviation instead of \\(n\\)? We said that the variance was an average of the \\(n\\) squared deviations, so should we not divide by \\(n\\)? Basically it is because the deviations have only \\(n - 1\\) pieces of information about variability: That is, \\(n - 1\\) of the deviations determine the last one, because the deviations sum to 0. For example, suppose we haven \\(n= 2\\) observations and the first observation has deviation \\((x - \\bar x) = 5\\). Then the second observation must have deviation \\((x - \\bar x) = -5\\) because the deviations must add to 0. With \\(n = 2\\), there’s only \\(n - 1 = 1\\) nonredundant piece of information about variability. And with \\(n = 1\\), the standard deviation is undefined because with only one observation, it’s impossible to get a sense of how much the data vary.↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Describing Data with Numbers</span>"
    ]
  },
  {
    "objectID": "05.html",
    "href": "05.html",
    "title": "5  Probability Concepts",
    "section": "",
    "text": "5.1 Basic Probability Rules\nWhen people say “there’s a 70 percent chance of rain” or that a team has “even odds” of winning, they’re using probability to express how likely an outcome seems. In statistics, we give that vague notion a precise meaning. A probability is a number between 0 and 1 that measures how often we expect an event to occur in the long run.\nA probability of 0 means the event can never happen, a probability of 1 means it is certain, and values in between describe varying degrees of likelihood.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability Concepts</span>"
    ]
  },
  {
    "objectID": "05.html#sec-05_01",
    "href": "05.html#sec-05_01",
    "title": "5  Probability Concepts",
    "section": "",
    "text": "“The theory of probabilities is at bottom nothing but common sense reduced to calculus.” – Pierre‑Simon Laplace\n\n\n\n\nExperiments, outcomes and sample spaces\nProbability always starts with an experiment—a repeatable situation whose result is not predetermined.\nFlipping a coin, rolling a die or drawing a card are familiar examples. Each trial produces an outcome, and the set of all possible outcomes is called the sample space.\nWe often use \\(S\\) to denote the sample space. For instance, when you flip one fair coin, the sample space is \\(S=\\{\\text{H},\\text{T}\\}\\); when you roll a six‑sided die, \\(S=\\{1,2,3,4,5,6\\}\\).\nIt is essential to describe the sample space clearly because the probability of any event is computed relative to it. In practice, the sample space can be finite (like the faces of a die) or infinite (like all possible real‑valued measurements of temperature). Identifying \\(S\\) helps us keep track of what outcomes are possible before we assign probabilities.\n\n\nEvents and interpretations of probability\nAn event is any subset of the sample space. Events can be simple (containing one outcome) or compound (containing several outcomes).\nLet \\(A\\) be the event that a die roll is even. Then \\(A=\\{2,4,6\\}\\), which contains three outcomes from the sample space \\(S=\\{1,2,3,4,5,6\\}\\).\nThe probability of an event \\(A\\), written \\(P(A)\\), quantifies how likely \\(A\\) is to occur.\nWhen the outcomes in \\(S\\) are equally likely (as with a fair die or fair coin), the classical probability of \\(A\\) is\n\\[\nP(A) = \\frac{\\text{number of outcomes in }A}{\\text{number of outcomes in }S}\\,.\n\\]\nIn the die example above, there are three even numbers out of six, so \\(P(A)=3/6=0.5\\). This agrees with our intuition that even and odd numbers are equally likely on a fair die.\nProbability has several interpretations. In the frequentist view, \\(P(A)\\) is the long‑run relative frequency of \\(A\\)—the fraction of times \\(A\\) occurs if we repeat the experiment many times under identical conditions. For example, the probability of flipping a head on a fair coin is 0.5 because, in the long run, about half of the flips will show heads.\n\n\n\n\n\n\nExample 5.1: Rolling a six-sided die\n\n\n\nSuppose we rolled a six-sided die 20 times. Using the idea of equally likely outcomes, the probability of rolling a six is \\[\n\\begin{align}\nP(6)=&\\frac{1}{6}\\\\\n\\approx& 0.167\n\\end{align}\n\\] Let’s see how many times we get a six when rolling the die 20 times.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\n6\n1\n5\n5\n4\n3\n2\n2\n6\n1\n2\n3\n3\n1\n1\n3\n5\n3\n2\n\n\n\n\n\nWe see that we rolled a six 2 out of the 20 times. The relative frequency is thus \\[\n\\frac{\\text{number of sixes}}{\\text{number of rolls}}=\\frac{2}{20}=0.1\n\\]\nLet’s look at a bar chart for all of the rolls:\n\n\n\n\n\n\n\n\n\nLet’s now roll the die 1,000 times:\n\n\n\n\n\n\n\n\n\nThe number of times we rolled each value is a little more even. Focusing only on six, we have \\[\n\\frac{\\text{number of sixes}}{\\text{number of rolls}}=\\frac{160}{1000}=0.16\n\\] which is close to the theoretical proportion of \\(1/6\\approx 0.167\\)\nWe can keep increasing the number of times we roll the die and we will see the number of each value “even out” across all values:\n\n\n\n\n\n\n\n\n\nEach value will converge to the theoretical value of \\(1/6\\).\n\n\n\n\n\n\n\n\nExample 5.2: Flipping a coin\n\n\n\nSuppose we flip a coin many times and track the proportion of heads over time. Early on, the proportion jumps around; as the number of flips grows, it tends to settle near the true probability of 0.5.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLaw of Large Numbers\nIn 1689, the Swiss mathematician Jacob Bernoulli proved that as the number of trials increases, the proportion of occurrences of any given outcome approaches a particular number (such as 1/6 or 1/2) in the long run.\nTo show this, he assumed that the outcome of any one trial does not depend on the outcome of any other trial. Bernoulli’s result is known as the Law of Large Numbers.\nWe will interpret the probability of an outcome to represent long-run results. This is the frequentist approach to probability.\n\n\n\n\n\n\nFrequentist and Subjective probability\n\n\n\n\n\nProbability can be defined in different ways:\n\nFrequentist (long‑run relative frequency). For repeatable experiments, \\(P(A)\\) is the limit of the proportion of times \\(A\\) occurs in a large number of trials.\nSubjective. Probability reflects a personal belief based on available information; two people may assign different probabilities to the same event.\n\nNo matter how you interpret it, probability values lie between 0 and 1 and follow certain algebraic rules.\n\n\n\n\n\nComplements and the complement rule\nIf \\(A\\) is an event, the complement of \\(A\\) (denoted \\(A^c\\)) consists of all outcomes in \\(S\\) that are not in \\(A\\). In everyday terms, either \\(A\\) happens or it doesn’t, so the probabilities of \\(A\\) and its complement must sum to 1. This leads to the complement rule:\n\\[\nP(A^c) = 1 - P(A)\n\\]\nThe complement rule is especially handy for “at least one” problems. Suppose you toss a fair coin three times and want the probability of getting at least one head. Let \\(A\\) be the event “at least one head” and \\(A^c\\) be “no heads” (all tails). There are eight possible equally likely outcomes in the sample space \\[\nS=\\left\\{HHH, HHT, HTH, HTT, THH, THT, TTH, TTT\\right\\}\n\\]\nThere is only one outcome with no heads—TTT—so \\(P(A^c)=1/8\\).\nBy the complement rule, \\(P(A)=1-1/8=7/8\\).\n\n\nUnions, intersections and the addition rule\nWhen you combine events, you need to think about unions and intersections. The union of two events \\(A\\) and \\(B\\) (written \\(A\\cup B\\)) contains outcomes that are in \\(A\\) or in \\(B\\) (or in both).\nThe intersection of \\(A\\) and \\(B\\) (written \\(A\\cap B\\)) contains outcomes that are in both \\(A\\) and \\(B\\). The probability of the union is given by the general additive rule:\n\\[\nP(A\\cup B) = P(A) + P(B) - P(A\\cap B)\\,.\n\\]\n\n\n\n\n\n\nExample 5.3: Additive rule\n\n\n\nImagine a survey of 200 students about their beverage preferences:\n\n120 students like coffee.\n90 students like tea.\n60 students like both coffee and tea.\n\nDefine event \\(C\\) = “student likes coffee” and event \\(T\\) = “student likes tea.” Then\n\n\\(P(C) = 120/200 = 0.60\\)\n\\(P(T) = 90/200 = 0.45\\)\n\\(P(C\\cap T) = 60/200 = 0.30\\)\n\nThe probability that a randomly chosen student likes either coffee or tea (or both) is\n\\[\n\\begin{align*}\nP(C\\cup T) =& P(C) + P(T) - P(C\\cap T)\\\\\n           =& 0.60 + 0.45 - 0.30\\\\\n           =& 0.75\n\\end{align*}\n\\]\nSo 75% of students like at least one of the two beverages.\nNotice that this formula counts how many like coffee (\\(P(C)\\)). Then it adds how many like tea (\\(P(T)\\)). When these two are added together, the students who like both coffee and tea are added twice: once in \\(P(C)\\) and again in \\(P(T)\\). Therefore, those students must be subtracted off one time. That is why we subtract off the intersection \\(P(C\\cap T)\\).\n\n\n\n\nMutually Exclusive\nIf \\(A\\) and \\(B\\) are mutually exclusive (also called disjoint)—meaning they have no outcomes in common—then \\(P(A\\cap B)=0\\) and the rule simplifies to \\(P(A\\cup B)=P(A)+P(B)\\). A simple example involves rolling a die. Let \\(A\\) be “rolling a 2” and \\(B\\) be “rolling an odd number.” These events are disjoint because 2 is even, so \\[\nP(A\\cup B)=P(\\text{2})+P(\\text{odd})=1/6+3/6=2/3\n\\]\n\n\n\n\n\n\nExample 5.4: Hospital patients\n\n\n\nHospital records show that 12% of all patients are admitted for surgical treatment, 16% are admitted for obstetrics, and 2% receive both obstetrics and surgical treatment.\nIf a new patient is admitted to the hospital, what is the probability that the patient will be admitted for surgery, for obstetrics, or for both?\nLet \\(A=\\{\\text{A patient admitted to the hospital receives surgical treatment.}\\}\\)\nand \\(B=\\{\\text{A patient admitted to the hospital receives obstetrics treatment.}\\}\\)\nWe have \\[\nP(A) = 0.12\\qquad P(B) = 0.16\\qquad P(A\\cap B) = 0.02\n\\] Since some patients receives both surgical and obstetrics treatments, these events are not mutually exclusive. The general addition rule gives\n\\[\n\\begin{align*}\nP(A\\cup B) &= P(A) +  P(B) -P(A\\cap B) \\\\\n& = 0.12+.016-.02\\\\\n& = 0.26\n\\end{align*}\n\\]\nThus, 26% of all patients admitted to the hospital receive either surgical treatment, obstetrics treatment, or both.\n\n\n\n\nProbability from contingency tables\nGiven a contingency table where the data is displayed by two categorical variables, we can estimate the probabilities by using the frequencies in the cells.\n\n\n\n\n\n\nExample 5.5: PE blood test\n\n\n\nThe table below involves diagnosing pulmonary embolism (PE) using a D‑dimer blood test. In a group of 10,000 patients suspected of PE, a perfusion scan was used as the gold‑standard diagnostic. Of the 1000 individuals who actually had PE, the D‑dimer test was positive in 700 cases (and negative in 300); of the 9000 individuals without PE, the test was negative in 7700 cases (and positive in 1300). Thus 2000 people had a positive D‑dimer test result, but only 700 of them truly had PE.\n\n\n\n\n\n\n\n\n\nPulmonary‐embolism status\nD‑dimer positive\nD‑dimer negative\nTotal\n\n\n\n\nPulmonary embolism present\n700\n300\n1000\n\n\nPulmonary embolism absent\n1300\n7700\n9000\n\n\nTotal\n2000\n8000\n10000\n\n\n\nWe can estimate probabilities by taking the frequencies in the cells and dividing by the total number of patients (10,000). For example, the probability that a patient had a pulmonary embolism is \\[\nP(PE)=\\frac{1000}{10000}=0.1\n\\]\nThe probability that a patient pas a pulmonary embolism and is D-dimer positive is \\[\nP(PE\\cap \\text{D-dimer positive})=\\frac{700}{10000}=0.07\n\\]\nFor an union, we can apply the additive rule. For example, the probaility that a patient has a pulmonary embolism or has a D-dimer positive result is \\[\n\\begin{align*}\nP(PE\\cup \\text{D-dimer positive})=&P(PE)+P(\\text{D-dimer positive})-P(PE\\cap \\text{D-dimer positive})\\\\\n=&\\frac{1000}{10000}+\\frac{2000}{10000}-\\frac{700}{10000}\\\\\n=&\\frac{2300}{10000}\\\\\n=& 0.23\n\\end{align*}\n\\]\n\n\n\n\nWorking in JMP\nAlthough this book emphasizes concepts, it’s worth noting how you can explore probability in JMP. The Distribution Calculator (under Help → Probability Calculator) lets you compute probabilities and quantiles for many distributions without writing formulas. For custom experiments like coin tosses or die rolls, you can simulate data by adding a column of random values: use Rows → Add Rows to create the desired number of trials, add a column with a formula such as Random Integer(1,6) for die rolls, and then use Analyze → Distribution to summarise the results. This empirical approach reinforces the frequentist idea that probabilities emerge from long‑run relative frequencies.\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nProbability\nA number between 0 and 1 measuring how likely an event is to occur.\n\n\nExperiment\nA repeatable process with uncertain outcome (e.g., flipping a coin, rolling a die).\n\n\nOutcome\nA single possible result of an experiment; elements of the sample space.\n\n\nSample space\nThe set of all possible outcomes of an experiment.\n\n\nEvent\nA subset of the sample space; may include one or many outcomes.\n\n\nFrequentist interpretation\nDefines probability as the long‑run relative frequency of an event in repeated trial.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nDescribe the parts of a probability model. In your own words, explain the difference between an outcome, an event and a sample space. Use the experiment of rolling a six‑sided die as an example.\nEqually likely probability. If you roll a fair die, what is the probability of getting a number greater than 4? Explain how you arrive at your answer.\nComplement rule in practice. You toss a fair coin four times. What is the probability of getting at least one tail? Use the complement rule to find the answer.\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nParts of a probability model. A sample space is the set of all possible outcomes—for a six‑sided die, \\(S=\\{1,2,3,4,5,6\\}\\). An outcome is one specific element of \\(S\\), such as rolling a 4. An event is any subset of \\(S\\); it might contain one outcome (e.g., \\(\\{6\\}\\), “rolling a six”) or several (e.g., \\(\\{2,4,6\\}\\), “rolling an even number”). We assign probabilities to events based on the rules discussed above.\nProbability of a number greater than 4. The event \\(A\\) of rolling greater than 4 corresponds to outcomes \\(\\{5,6\\}\\). There are two favorable outcomes and six outcomes in total, so by the equally likely events \\(P(A)=2/6=1/3\\).\nAt least one tail in four tosses. Let \\(A\\) be “at least one tail.” The complement \\(A^c\\) is “no tails,” which means getting all heads (HHHH). Since the probability of all heads is \\((1/2)^4=1/16\\), the complement rule gives \\(P(A)=1-1/16=15/16\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability Concepts</span>"
    ]
  },
  {
    "objectID": "05.html#sec-05_02",
    "href": "05.html#sec-05_02",
    "title": "5  Probability Concepts",
    "section": "5.2 Conditional Probability and Independence",
    "text": "5.2 Conditional Probability and Independence\n\n“Is it probable that probability brings certainty?” – Blaise Pascal\n\nOur earlier discussion focused on single events and how to combine them using unions and complements. In many situations, we want to know the probability of an event given that another has already occurred. Thinking this way is essential when events are not isolated but are part of a process or sequence. For example, the probability that a randomly selected person is left‑handed might be different once you know the person is an artist. This leads us to the idea of conditional probability.\n\nConditional probability\nIf \\(A\\) and \\(B\\) are events with \\(P(A)&gt;0\\), the conditional probability of \\(B\\) given \\(A\\), written \\(P(B|A)\\), describes the chance that \\(B\\) occurs assuming that \\(A\\) has occurred or will occur. When we condition on \\(A\\), the sample space shrinks to just the outcomes where \\(A\\) happens. In terms of probabilities, this shrinking translates to the formula \\[\nP(B\\mid A) = \\frac{P(A\\cap B)}{P(A)}\n\\] which says that we compare how often both events occur to how often \\(A\\) occurs at all.\nYou can read \\(P(B|A)\\) as “the probability of \\(B\\) given \\(A\\).”\nRearranging the conditional probability formula yields the multiplication rule for any two events \\(A\\) and \\(B\\): \\[\n\\begin{align*}\nP(A\\cap B) =& P(A)\\,P(B\\mid A)\\\\\n=& P(B)\\,P(A\\mid B)\n\\end{align*}\n\\] This rule tells us how to find the probability that both events occur: multiply the probability of the first event by the conditional probability of the second event given the first.\n\n\n\n\n\n\nExample 5.6: Smoking and Cancer\n\n\n\nMany medical researchers have conducted experiments to examine the relationship between cigarette smoking and cancer.\nConsider an individual randomly selected from an adult male population.\nLet \\(A\\) represent the event that the individual smokes, and let \\(A^c\\) denote the complement of \\(A\\) (the event that the individual does not smoke).\nSimilarly, let \\(B\\) represent the event that the individual develops cancer, and let \\(B^c\\) be the complement of that event.\nSuppose a large sample from the population resulted in the proportions below:\n\n\n\n\n\\(B\\)\n\\(B^{c}\\)\n\n\n\n\n\\(A\\)\n.05\n.20\n\n\n\\(A^{c}\\)\n.03\n.72\n\n\n\nUse these probabilities to examine the relationship between smoking and cancer.\nFirst, find the probability that an individual smokes: \\[\n\\begin{align*}\nP(A) =& {P(A\\cap B)+P(A\\cap B^c)}\\\\\n=& {0.05+0.20}\\\\\n=& {0.25}\n\\end{align*}\n\\]\nNow find the probability that an individual doesn’t smoke: \\[\n\\begin{align*}\nP(A^c) =& {P(A^c\\cap B)+P(A^c\\cap B^c)}\\\\\n=& {0.03+0.72}\\\\\n=& {0.75}\n\\end{align*}\n\\]\nLet’s now determine the probability that an individual develops cancer given they are a smoker: \\[\n\\begin{align*}\nP(B\\mid A) =& \\frac{P(A\\cap B)}{P(A)}\\\\\n=& \\frac{0.05}{0.25}\\\\\n=& 0.2\n\\end{align*}\n\\]\nLet’s now determine the probability that an individual develops cancer given they are a smoker: \\[\n\\begin{align*}\nP(B\\mid A^c) =& \\frac{P(A^c\\cap B)}{P(A^c)}\\\\\n=& \\frac{0.03}{0.75}\\\\\n=& 0.04\n\\end{align*}\n\\]\n\n\n\n\nIndependent and dependent events\nWhen the occurrence of one event does not change the probability of another, we call the events independent.\nFormally, \\(A\\) and \\(B\\) are independent if \\[\nP(B\\mid A)=P(B)\n\\] or equivalently \\[\nP(A\\mid B)=P(A)\n\\] If either holds, then the other does too.\nFor independent events, the multiplication rule simplifies to \\[\nP(A\\cap B)=P(A)\\,P(B)\n\\] If knowing that \\(A\\) has occurred does change the probability of \\(B\\), the events are called dependent.\n\n\n\n\n\n\nExample 5.7: Flipping coins\n\n\n\nThink about tossing a fair coin twice. Let \\(H_1\\) be the event “the first toss is heads” and \\(H_2\\) be “the second toss is heads.” The probability of a head on each toss is 0.5, and the outcome of the first toss does not affect the outcome of the second. Therefore, \\(P(H_2\\mid H_1)=P(H_2)=0.5\\), and the events are independent. Using the multiplication rule, \\[\nP(H_1\\cap H_2) = P(H_1)\\,P(H_2) = 0.5\\times0.5 = 0.25\\\n\\]\n\n\n\n\n\n\n\n\nExample 5.8: Sampling without replacement\n\n\n\nIndependence can break down when we sample from a finite population without replacement.\nSuppose a box contains 10 batteries, 4 of which are defective. You draw two batteries at random without replacement.\nLet \\(D_1\\) be the event “the first battery is defective” and \\(D_2\\) be “the second battery is defective.”\nThe probability the first battery is defective is \\(P(D_1)=4/10=0.4\\).\nIf the first battery is defective, there are now 3 defectives among 9 remaining batteries, so \\(P(D_2\\mid D_1)=3/9=0.333\\). If the first battery is good, there are still 4 defectives among 9 batteries, so \\(P(D_2\\mid D_1^c)=4/9\\approx0.444\\).\nBecause \\(P(D_2)\\) depends on whether \\(D_1\\) happened, the events are dependent.\nWhen we sample with replacement, the draws are independent, since after each draw the composition of the box resets to 4 defectives out of 10.\n\n\nIn practice, when the sample size is much smaller than the population, the difference between sampling with and without replacement becomes negligible and we can treat the draws as independent.\n\n\nMutually Exclusive Events versus Independent Events\nThere is often confusion between the concepts of independent events and disjoint events.\nActually, these are quite different notions, and perhaps this is seen best by comparisons involving conditional probabilities.\nSpecifically, if \\(A\\) and \\(B\\) are mutually exclusive, then \\[\nP(A \\cap B) = 0\n\\] whereas for independent events, we have \\[\nP(A \\cap B)  = P(A) P(B)\n\\]\nIf \\(P(A)&gt;0\\) and \\(P(B)&gt;0\\), then how can \\[\nP(A) P(B) = 0 ?\n\\] This is impossible.\nIn other words, the property of being mutually exclusive involves a very strong form of dependence, because the occurrence of one event mean the other event cannot occur.\nThus, if two events are mutually exclusive, they cannot be independent.\nIf two events are independent, they cannot be mutually exclusive.\n\n\nConditional probability from contingency table\nLet’s go back to the pulmonary embolism example from last section.\nHere is the table again:\n\n\n\n\n\n\n\n\n\nPulmonary‐embolism status\nD‑dimer positive\nD‑dimer negative\nTotal\n\n\n\n\nPulmonary embolism present\n700\n300\n1000\n\n\nPulmonary embolism absent\n1300\n7700\n9000\n\n\nTotal\n2000\n8000\n10000\n\n\n\nSuppose we wanted to estimate the probability of a pulmonary embolism given the patient had a D-dimer positive result.\nWhen looking for conditional probability from a contingency table, remember that the conditioned event is all we are focusing on .\nIn this case, the conditioned event is D-dimer positive. There are a total of 2000 patients that are D-dimer positive. Of those 2000 patients, 700 had a pulmonary embolism. Thus, the conditional probability is\n\\[\nP(PE\\mid \\text{D-dimer positive})=\\frac{700}{2000}=0.35\n\\]\nWe can think of this as an application of the Law of Large Numbers. We use the counts in the table to estimate the desired probability. We just need to know what to use as the numerator and denominator based on what conditional probability you want to find.\n\nWorking in JMP\nJMP makes it easy to explore conditional probabilities and independence using contingency tables. Enter your data with one column for each categorical variable (for example, “Major” and “Year”). Under Analyze → Fit Y by X, choose one variable as the response and the other as the explanatory factor. The resulting mosaic or contingency table will show joint counts. Right‑click and choose Row Percents or Column Percents to view conditional proportions. If the row percentages are the same across all columns (and vice versa), the variables are approximately independent; if the percentages differ, there is evidence of dependence.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nConditional probability\nThe probability of one event given that another event has occurred; computed as \\(P(B\\mid A)=P(A\\cap B)/P(A)\\) when \\(P(A)&gt;0\\).\n\n\nMultiplication rule\nA rule that expresses the joint probability of two events: \\(P(A\\cap B)=P(A)\\,P(B\\mid A)=P(B)\\,P(A\\mid B)\\).\n\n\nIndependent events\nEvents \\(A\\) and \\(B\\) for which \\(P(B\\mid A)=P(B)\\) (equivalently \\(P(A\\mid B)=P(A)\\)) and \\(P(A\\cap B)=P(A)P(B)\\).\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nCalculus and statistics courses. At a certain college, 40% of students have taken calculus, 30% have taken statistics, and 15% have taken both.\n\nWhat is the probability that a student has taken statistics given they have taken calculus?\nAre taking calculus and statistics independent events? Explain your reasoning.\n\nDrawing marbles without replacement. A jar contains 5 red marbles and 3 green marbles. You draw two marbles at random without replacement.\n\nWhat is the probability that both marbles are red?\nIf you draw with replacement, what would be the probability of drawing two red marbles?\nWhich scenario involves independent events?\n\nPass rates. In a school, 60% of students pass mathematics, 50% pass physics, and 35% pass both subjects.\n\nWhat is \\(P(\\text{Physics} \\mid \\text{Math})\\)?\nAre passing mathematics and passing physics independent? Why or why not?\n\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nCalculus and statistics.\n\nThe conditional probability that a student has taken statistics given they have taken calculus is \\(P(\\text{Stats}\\mid \\text{Calc}) = P(\\text{Stats}\\cap \\text{Calc}) / P(\\text{Calc}) = 0.15/0.40 = 0.375\\).\nIf calculus and statistics were independent, we would have \\(P(\\text{Stats}\\cap \\text{Calc}) = P(\\text{Stats})\\,P(\\text{Calc}) = 0.30\\times0.40 = 0.12\\). Because the actual joint probability 0.15 is greater than 0.12, these events are not independent; students who take calculus are more likely to take statistics.\n\nDrawing marbles.\n\nWithout replacement, the probability both marbles are red is \\(P(\\text{first red})\\times P(\\text{second red}\\mid \\text{first red}) = (5/8)\\times (4/7) = 20/56 \\approx 0.357\\).\nWith replacement, the probability stays \\((5/8)\\times (5/8) = 25/64 \\approx 0.391\\) because the composition of the jar is reset after each draw.\nDrawing with replacement yields independent events. Without replacement the draws are dependent, since the first draw changes the composition for the second.\n\nPass rates.\n\nThe conditional probability that a student passes physics given they pass mathematics is \\(P(\\text{Physics}\\mid \\text{Math}) = 0.35/0.60 \\approx 0.583\\).\nFor independence we would need \\(P(\\text{Physics}\\mid \\text{Math}) = P(\\text{Physics}) = 0.50\\). Because \\(0.583\\ne 0.50\\), passing mathematics and passing physics are not independent; students who pass one subject are more likely to pass the other.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability Concepts</span>"
    ]
  },
  {
    "objectID": "05.html#sec-05_03",
    "href": "05.html#sec-05_03",
    "title": "5  Probability Concepts",
    "section": "5.3 Bayes Rule",
    "text": "5.3 Bayes Rule\n\n“Absence of evidence is not evidence of absence.” – Carl Sagan\n\nWhen you receive a lab report or a rapid test result it is tempting to take the outcome at face value: a positive means you have the disease; a negative means you do not. In reality, every diagnostic test has some chance of making mistakes. To interpret a result responsibly we need to take two things into account:\n\nThe accuracy of the test itself. How often does it correctly flag someone who is sick? How often does it incorrectly flag someone who is healthy? These properties are captured by sensitivity and specificity.\nThe prevalence of the condition. How common is the disease in the population you are testing? This is the prior probability that a random person is infected before considering the test result.\n\nBayes’ Rule is the mathematical tool that combines those two pieces of information. It takes the evidence (the test result) and updates the prior probability to produce a new, posterior probability: the chance the person actually has the disease given the result. Bayes’ Rule is simply a rearrangement of the multiplication rule for probabilities; for events \\(A\\) and \\(B\\) with \\(P(B)&gt;0\\) it says\n\\[\nP(A\\mid B) = \\frac{P(B\\mid A)\\,P(A)}{P(B)}.\n\\]\nIn diagnostic testing we let \\(A\\) be the event “person has the disease” and \\(B\\) be “test is positive.” Then:\n\n\\(P(A)\\) is the prevalence of the disease (our prior belief). This is usually known or estimated by epidemiologists.\n\\(P(B\\mid A)\\) is the sensitivity: the probability the test correctly labels a diseased person as positive. This value is estimated in a clinical setting.\n\\(P(B\\mid A^c)\\) is the false–positive rate, which is \\(1\\) minus the specificity. Also estimated in a clinical setting.\n\\(P(A\\mid B)\\) is the quantity we really want: the probability a person is truly sick given they test positive. This is called the positive predictive value (PPV) of the test.\n\nNotice that Bayes’ Rule also works for a negative result. If \\(T^c\\) denotes a negative test, the probability of being healthy given a negative result, \\(P(A^c\\mid T^c)\\), is the negative predictive value (NPV).\n\nUnderstanding Bayes’ Rule through an example\nTo see how Bayes’ Rule plays out, imagine a disease that affects 1% of the population. A new screening test has 90% sensitivity and 95% specificity. In plain language:\n\nIf you have the disease, there is a 90% chance the test is positive (and a 10% chance of a false negative).\nIf you do not have the disease, there is a 95% chance the test is negative (and a 5% chance of a false positive).\n\nSuppose you go to the clinic and your test comes back positive. What is the probability you actually have the disease? Let \\(D\\) be “has the disease” and \\(T\\) be “test is positive.” We can use the conditional probability rule:\n\\[\nP(D\\mid T)=\\frac{P(T\\cap D)}{P(T)}\n\\] Unfortunately, we do not know \\(P(T\\cap D)\\) or \\(P(T)\\). All we know is\n\n\\(P(T\\mid D)\\), which is the sensitivity\n\\(P(T^c\\mid D^c)\\) which is the specificity. Sometimes we have instead the complement of specificity \\(P(T\\mid D^c)\\).\n\\(P(D)\\) which is the prevalence.\n\nIn terms of our example, we have\n\n\\(P(T\\mid D)=0.90\\)\n\\(P(T^c\\mid D^c)=0.95\\) which implies \\(P(T \\mid D^c)=1-P(T^c\\mid D^c)=0.05\\)\n\\(P(D)=0.01\\)\n\nIn the numerator above, we can use the multiplicative rule to find \\(P(T\\cap D)\\): \\[\nP(T\\cap D)=P(T\\mid D)\\,P(D)\n\\] In the denominator, we can write \\[\nP(T) = P(T\\cap D) + P(T\\cap D^c)\n\\] Now, let’s apply the multiplicative rule to both of these intersections using the information that we do know: \\[\nP(T) = P(T\\mid D)\\,P(D) + P(T\\mid D^c)\\,P(D^c)\n\\]\nLet’s put everything together to give us a formula that has quantities we do know: \\[\nP(D\\mid T)=\\frac{P(T\\mid D)\\,P(D)}{P(T\\mid D)\\,P(D) + P(T\\mid D^c)\\,P(D^c)}\n\\]\nThis equation is known as Bayes’ Rule.\nSubstituting our known values into the equation gives us \\[\n\\begin{align*}\nP(D\\mid T)=&\\frac{P(T\\mid D)\\,P(D)}{P(T\\mid D)\\,P(D) + P(T\\mid D^c)\\,P(D^c)}\\\\\n=&\\frac{(0.90)(0.01)}{(0.90)(0.01) + (0.05)(0.99)}\\\\\n=&0.1538\n\\end{align*}\n\\]\nEven though the test is fairly accurate, a *positive** result only raises the probability of disease to about 0.1538. The reason is that the disease is so rare that most positive tests are false alarms.\n\n\n\n\n\n\nExample 5.9: Genetic Screening\n\n\n\nBayesian thinking is not confined to clinical medicine. Geneticists often screen organisms for rare traits. Imagine a plant species in which 5% of individuals carry a gene conferring resistance to a particular fungus. A DNA test has 90% sensitivity and 80% specificity. If a randomly selected plant tests positive, how likely is it to possess the resistance gene?\nLet \\(R\\) be “has the resistance gene” and \\(T\\) be “test positive.” Then \\(P(R)=0.05\\), \\(P(T\\mid R)=0.90\\) and \\(P(T\\mid R^c)=0.20\\) (since specificity is 80%). Bayes’ Rule tells us\n\\[\nP(R\\mid T) = \\frac{0.90\\times 0.05}{0.90\\times 0.05 + 0.20\\times 0.95} \\approx 0.191.\n\\]\nSo even a positive test only implies about a 19% chance of carrying the gene. The key message is the same: when a condition is rare, false positives can overwhelm true positives, and Bayes’ Rule quantifies how evidence updates our beliefs.\n\n\n\n\nWorking in JMP\nJMP does not have a dedicated “Bayes’ Rule” button, but you can apply the rule by constructing a contingency table and computing appropriate proportions. To explore the medicine example above in JMP:\n\nCreate a table with one column for the true condition (Diseased vs Healthy) and one column for the test result (Positive vs Negative). Enter the counts from the confusion matrix (for example, 90 true positives, 495 false positives, 9 405 true negatives and 10 false negatives per 10 000 screened).\nChoose Analyze → Fit Y by X and assign True condition to the Y role and Test result to the X role. The resulting contingency table shows the joint counts. Right‑click on the table and select Row Percents or Column Percents to view conditional probabilities.\nYou can then compute the PPV as the percentage in the Diseased row of the Positive column divided by the total percentage in the Positive column, and the NPV as the percentage in the Healthy row of the Negative column divided by the total percentage in the Negative column.\n\nThese steps replicate the Bayes’ Rule calculation without writing formulas. JMP’s probability calculators can also be used for more complex Bayesian analyses in later chapters.\n\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition/Meaning\n\n\n\n\nBayes’ Rule\nFormula to update probabilities in light of new evidence\n\n\nSensitivity\nProbability that a diagnostic test correctly identifies a diseased individual as positive; a high sensitivity means few false negatives.\n\n\nSpecificity\nProbability that a diagnostic test correctly identifies a healthy individual as negative; a high specificity means few false positives.\n\n\nPositive predictive value\nThe probability that a person actually has the condition given a positive test result, \\(P(D\\mid T)\\); depends on sensitivity, specificity and the disease prevalence.\n\n\nNegative predictive value\nThe probability that a person is disease‑free given a negative test result, \\(P(D^c\\mid T^c)\\); often high when the disease is rare and the test has good sensitivity.\n\n\nPrevalence\nThe proportion of individuals in the population who truly have the disease before any testing; the prior belief in Bayes’ Rule.\n\n\n\n\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nInterpreting a positive screening result. Suppose 2% of a population has a certain virus. A test for the virus has 95% sensitivity and 90% specificity.\n\nUsing Bayes’ Rule, compute the probability that a randomly selected person actually has the virus given they test positive.\nWhat is the probability that a person who tests negative is truly virus‑free?\n\nUnderstanding predictive values. A small genetic screening test for a mutation has 80% sensitivity and 97% specificity. In the population being tested, 0.5% carry the mutation. Explain why the positive predictive value of this test is quite low, despite its high specificity. How would the predictive value change if the prevalence were higher?\nDesigning a screening program. A veterinary lab develops a new blood test for heartworm disease in dogs. Preliminary studies estimate the prevalence of heartworm in the region at 3%, and the test’s sensitivity and specificity are 92% and 94%, respectively. Explain how you would use Bayes’ Rule to decide whether a positive test result warrants immediate treatment or should be confirmed with a second test. What role does the disease prevalence play in your decision?\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nInterpreting a positive screening result.\n\nLet \\(D\\) be “has the virus” and \\(T\\) be “test positive.” We are given \\(P(D)=0.02\\), \\(P(T\\mid D)=0.95\\) and \\(P(T\\mid D^c)=0.10\\) (because specificity 90% implies a 10% false‑positive rate). First compute \\(P(T)=0.95\\times0.02 + 0.10\\times0.98=0.019 + 0.098=0.117\\). Then \\[P(D\\mid T)=\\frac{0.95\\times0.02}{0.117}\\approx0.162.\\] So even with good sensitivity and decent specificity, a positive result only implies about a 16% chance of infection when prevalence is 2%.\nFor a negative result we look at the NPV. The probability of being virus‑free given a negative test is \\[P(D^c\\mid T^c)=\\frac{P(T^c\\mid D^c)\\,P(D^c)}{P(T^c)}.\\] Here \\(P(T^c\\mid D^c)=0.90\\) (the specificity) and \\(P(T^c\\mid D)=1-0.95=0.05\\). Compute \\(P(T^c)=0.05\\times0.02 + 0.90\\times0.98 = 0.019 + 0.882=0.901\\). Then \\[P(D^c\\mid T^c)=\\frac{0.90\\times0.98}{0.901}\\approx0.979.\\] Thus, a negative test is highly reassuring: about 98% of negatives are true negatives.\n\nUnderstanding predictive values. With sensitivity 80%, specificity 97% and prevalence 0.5%, we have \\(P(D)=0.005\\), \\(P(T\\mid D)=0.80\\) and \\(P(T\\mid D^c)=0.03\\). Using Bayes’ Rule, \\[P(D\\mid T)=\\frac{0.80\\times0.005}{0.80\\times0.005 + 0.03\\times0.995} \\approx 0.118.\\] So only about 12% of positive results reflect true mutation carriers. The test is quite specific, but the condition is extremely rare, so the few false positives swamp the handful of true positives. If the prevalence were higher—say 5%—the same calculation would yield \\[P(D\\mid T)=\\frac{0.80\\times0.05}{0.80\\times0.05 + 0.03\\times0.95} \\approx 0.585,\\] meaning a positive result would be far more convincing. The predictive value increases with prevalence.\nDesigning a screening program. Let \\(H\\) be “has heartworm” and \\(T\\) be “test positive.” We are given \\(P(H)=0.03\\), \\(P(T\\mid H)=0.92\\) and \\(P(T\\mid H^c)=0.06\\) (since specificity 94% means a 6% false‑positive rate). Bayes’ Rule tells us \\[P(H\\mid T)=\\frac{0.92\\times0.03}{0.92\\times0.03 + 0.06\\times0.97} \\approx 0.32.\\] A single positive test thus implies roughly a 32% chance of heartworm. Whether that warrants immediate treatment depends on the risks and costs of treating a false positive versus missing a true case. Because the prevalence is relatively low and the PPV modest, many veterinarians would confirm a positive with a second, more specific test or use a confirmatory diagnostic (like an antigen test or imaging). If the disease were more common in the region (higher prevalence), the PPV would rise and clinicians might opt to treat after one positive result. Bayes’ Rule helps quantify this trade‑off: as prevalence increases, positive tests become more trustworthy.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability Concepts</span>"
    ]
  },
  {
    "objectID": "06.html",
    "href": "06.html",
    "title": "6  Discrete Probability Distributions",
    "section": "",
    "text": "6.1 Random Variables\nWith proper methods of gathering data, the values that a variable takes should arise from some random phenomenon. Even if the process is carefully controlled, chance still plays a role: different samples, repeated experiments, or repeated measurements rarely produce exactly the same result. Statistics exists precisely because of this randomness.\nAt first glance, the outcomes of random phenomena may seem naturally numerical. But this raises an interesting question:\nAre outcomes themselves really numbers?\nIn reality, numbers are often just abstractions—labels we assign to describe and measure aspects of the world.\nConsider weight. Weight is a physical force resulting from the interaction between mass and gravity. The force itself is not inherently a number; it is a physical property. We assign numbers—such as pounds or kilograms—to represent this property so that we can compare, analyze, and communicate it.\nThe same idea appears in many settings. Numerical values do not exist independently of measurement; rather, they are tools we use to represent outcomes.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "06.html#sec-06_01",
    "href": "06.html#sec-06_01",
    "title": "6  Discrete Probability Distributions",
    "section": "",
    "text": "“There are some statistics that I’d like to share with you now, and they are numbers.” - Perd Hapley (Parks and Recreation)\n\n\n\n\n\n\n\n\nTurning outcomes into numbers\nThink about rolling a six-sided die. Physically, the outcome is not really the number 1 or 6—it is a particular face landing upward.\n\n\n\nBy convention, we assign the labels\n\\[\n\\{1,2,3,4,5,6\\}\n\\]\nto these outcomes. We have created a rule that maps each physical outcome to a number.\nMathematically, this rule is a function whose:\n\ndomain is the sample space (all possible outcomes of the experiment), and\nrange is a subset of the real numbers.\n\nThis function is called a random variable.\n\n\nA conceptual view of random variables\nAnother way to think about it is this:\nWhen we talk about chance, we need a language that converts unpredictable events into something we can analyze mathematically. A random variable provides that language. It takes the messy details of “what happened” and translates them into a numerical value.\nImportantly, the random variable is not the outcome itself; it is the rule that assigns numbers to outcomes.\nFor example, the outcome of tossing three coins might be (HHT), (TTT), or (HTH). A random variable could assign a number to each outcome, such as the number of heads observed.\nThus multiple different outcomes can produce the same numerical value.\n\n\nTypes of random variables\nRandom variables come in two broad types:\n\nA discrete random variable has a countable set of possible values. These often arise from counting: number of successes, number of arrivals, number of defects, and so on.\nA continuous random variable can take any value within an interval, such as height, temperature, or blood pressure. Continuous variables will be studied in more detail later; for now, our focus is primarily on discrete random variables.\n\n\n\nRandom variables describe processes, not data\nA key idea is that a random variable describes the process that could generate data, not the data themselves. Before an experiment is conducted, the value of the random variable is unknown and may take many possible values.\nOnce the experiment occurs, we observe a realization (or observed value) of that random variable.\n\n\nExamples\n\n\n\n\n\n\nExample 6.1: Example from Medicine\n\n\n\nSuppose a clinic screens 20 patients for a rare side effect of a new drug. Let\n\\[\nX = \\text{number of patients who experience the side effect}.\n\\]\nBefore screening begins, \\(X\\) could be any value from 0 to 20. Each possible value corresponds to many different combinations of patient outcomes. After the screening is complete, we observe one specific value (say \\(x=3\\)). This observed value is one realization of the random variable.1\nAnother clinic running the same experiment might observe a different value because the process contains randomness.\n\n\n\n\n\n\n\n\nExample 6.2: Example from Biology\n\n\n\nIn a genetics lab, 10 plants are grown from seeds, each of which may carry a mutation that confers resistance to a fungus. Let\n\\[\nY = \\text{number of resistant plants}.\n\\]\nThe possible values of \\(Y\\) are 0 through 10. Even without running the experiment repeatedly, probability allows us to model how likely each value is, based on the underlying mutation rate.\n\n\n\n\nWhy random variables matter\nIn both examples, the random variable converts a complex outcome—many individual successes or failures—into a single numerical summary. That number changes from trial to trial, but its behavior follows predictable probabilistic rules.\nThis abstraction is powerful. By representing outcomes numerically, we can:\n\ndescribe variability mathematically,\ncalculate probabilities,\nbuild models of uncertainty, and\nmake predictions about future observations.\n\nIn short, random variables are the bridge between real-world randomness and the mathematics of probability. They allow us to move from observing chance events to analyzing and understanding them quantitatively.\n\n\nWorking in JMP\nIn JMP you can work with discrete random variables by creating a column that represents the outcome of a chance process. For example:\n\nSimulate a random count. To explore the distribution of a count like \\(X\\) or \\(Y\\), use Rows → Add Rows to create a data table with a desired number of simulated trials (say 1000). Add a new column, choose Column → Formula, and use a random function such as Random Binomial(n, p) to generate counts of successes (e.g., the number of side effects among 20 patients with probability \\(p\\) of a side effect per patient). Each row in this column is one realization of your discrete random variable.\nExplore the distribution. Use Analyze → Distribution to see how often each value occurs. The histogram and summary tables show the possible values and their relative frequencies. This empirical distribution will get closer to the true probability distribution as you increase the number of simulated trials.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nrandom variable\nA function that assigns a numeric value to each outcome of a random experiment.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nDescribe in your own words the difference between a random variable and the observed data.\n\nGive an example of a discrete random variable in a medical context and list its possible values.\n\nIs “blood type” (A, B, AB, O) a discrete random variable? Explain why or why not.\n\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nThe random variable is a rule that maps every possible outcome of a random experiment to a number. The observed data are the actual values of that mapping in one experiment. Before you flip 10 coins, the random variable “number of heads” could be 0–10; after you flip, you observe a single number, such as 4.\nLet \\(X\\) be the number of patients out of 15 who respond favorably to a new therapy. The possible values of \\(X\\) are 0,1,2,…,15—one number for each possible count of responders.\nBlood type is categorical rather than numeric. To analyze it as a random variable you would typically convert it to a binary indicator (e.g., 1 if type A, 0 otherwise). The categories A, B, AB and O do not have a numerical order, so blood type on its own is not a discrete random variable in the sense used here.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "06.html#sec-06_02",
    "href": "06.html#sec-06_02",
    "title": "6  Discrete Probability Distributions",
    "section": "6.2 Probability Distributions and Their Properties",
    "text": "6.2 Probability Distributions and Their Properties\n\n“Iacta alea est. (The die is cast.)” - Julius Ceasar\n\nOnce we have defined a discrete random variable, the next step is to describe how likely each of its possible values is. A probability distribution (also called a probability mass function) assigns a probability \\(P(X=x)\\) to every value \\(x\\) in the support of the random variable \\(X\\). A valid probability distribution must satisfy two simple but fundamental conditions:\n\nNon‑negativity: For every possible value \\(x\\), the probability satisfies \\(0 \\le P(X=x) \\le 1\\). Probabilities can’t be negative or exceed one.\nSum to one: The probabilities of all possible outcomes add up to one: \\[\n\\sum_{x} P(X=x) = 1\\,.\n\\] This reflects the fact that something in the sample space must occur on each trial.\n\nA probability distribution summarizes the long‑run behavior of a random variable. It tells you how often each value would appear if you repeated the experiment many times. Let’s look at an example.\n\n\n\n\n\n\nExample 6.3: number of mutated alleles\n\n\n\nSuppose biologists examine individual plants and record the number of copies of a recessive allele they carry. The random variable \\(X\\) takes the values 0, 1, 2 or 3 with the following probabilities:\n\n\n\n\\(x\\)\n0\n1\n2\n3\n\n\n\n\n\\(P(X=x)\\)\n0.50\n0.30\n0.15\n0.05\n\n\n\nEvery probability is between 0 and 1, and the probabilities sum to \\[\n0.50+0.30+0.15+0.05=1\n\\] Thus this table is a valid probability distribution. If you grew a very large number of plants from the same genetic cross, about 50% would carry no copies of the allele, 30% would have one copy, 15% two copies, and 5% three copies.\nCommonly, the probability distribution of a discrete random variable is displayed with a bar chart. As long as the bar chart displays all possible values of the random variable, and the probability of each value, then it displays the probability distribution.\nBelow is a bar chart for the probability distribution in the table above.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 6.4: Medical testing\n\n\n\nIn medical testing you often have a random variable that counts the number of positive results in a sample. For instance, if you screen 5 animals for a virus and each has a small probability of being infected, the distribution of \\(Y\\) = “number of infected animals” might look something like this:\n\n\n\n\\(y\\)\n0\n1\n2\n3\n4\n5\n\n\n\n\n\\(P(Y=y)\\)\n0.70\n0.21\n0.07\n0.015\n0.004\n0.001\n\n\n\nAgain, each probability is between 0 and 1 and the sum is 1. The table is more detailed because there are more possible counts, but the principles are the same.\n\n\n\nValidity checks\nSometimes you will be given a proposed distribution and asked whether it is valid. To check validity:\n\nMake sure all probabilities are non‑negative and at most 1.\nAdd them up. If they sum to 1 (allowing for small rounding error) the distribution is valid; otherwise it is not.\n\nIf a distribution is not valid, you cannot use it until it is corrected or rescaled.\n\n\nWorking in JMP\nTo examine a discrete probability distribution in JMP:\n\nEnter the values and probabilities. Create a new data table with one column for the possible values and another for the corresponding probabilities. Be careful that your probabilities add to 1.\nVisualise the distribution. Use Graph → Graph Builder. Drag the value column to the X‑axis and the probability column to the Y‑axis. Choose Bar as the graph type. The resulting bar chart is your probability mass function. You can also label the bars with the probabilities by clicking the red triangle ▸ and selecting Label.\nCheck the sum. You can add a column formula using Column → New Column and formula sum(:Probability) to verify that the probabilities sum to one. JMP will report the sum in a single cell.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nprobability distribution\nA table, graph, or function assigning each possible value of a discrete random variable a probability between 0 and 1.\n\n\nprobability mass function (pmf)\nAnother name for the probability distribution of a discrete random variable.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nConsider a random variable \\(X\\) that takes values 0, 1, 2 with \\(P(X=0)=0.2\\), \\(P(X=1)=0.5\\) and \\(P(X=2)=0.4\\). Is this a valid probability distribution? Explain.\n\nA diagnostic test counts the number of positive samples among three blood samples from the same patient. Suggest a possible probability distribution for the number of positives and describe how you might visualize it.\n\nThe following table describes a proposed distribution for a random variable \\(X\\):\n\n\n\n\\(x\\)\n0\n1\n2\n3\n\n\n\n\n\\(P(X=x)\\)\n0.3\n\\(–0.1\\)\n0.6\n0.2\n\n\n\nIs this distribution valid? Why or why not?\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nThe probabilities must sum to 1. Here \\(0.2 + 0.5 + 0.4 = 1.1\\), which is greater than 1, so this is not a valid distribution. One or more probabilities needs to be adjusted or rescaled.\nThe number of positive samples (\\(Y\\)) can be 0, 1, 2 or 3. A plausible distribution might be \\(P(Y=0)=0.70\\), \\(P(Y=1)=0.20\\), \\(P(Y=2)=0.08\\) and \\(P(Y=3)=0.02\\), but the actual numbers depend on the underlying infection probability. To visualize the distribution, enter these values and probabilities into JMP and create a bar chart with categories 0–3 on the X‑axis and probabilities on the Y‑axis.\nA valid distribution cannot have negative probabilities. Because \\(P(Z=1)=-0.1\\) is negative, this table is invalid. You cannot assign negative weight to an outcome. The probabilities also sum to \\(0.3-0.1+0.6+0.2=1.0\\), but the negativity alone makes it invalid.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "06.html#sec-06_03",
    "href": "06.html#sec-06_03",
    "title": "6  Discrete Probability Distributions",
    "section": "6.3 Mean and Standard Deviation of Discrete Distributions",
    "text": "6.3 Mean and Standard Deviation of Discrete Distributions\n\n“The most important questions of life are, for the most part, really only problems of probability.” -Pierre Simon, Marquis de Laplace\n\nAs stated previously in this course, we want to examine the center, spread, and shape when we have a data distribution. In Chapter 4, we discussed doing this with sample data where we calculate statistics like \\(\\bar{x}\\), \\(s\\), and \\(s^2\\). These are sample statistics. The probability distribution tells us how likely each outcome of \\(X\\) is. Thus, we should view the probability distribution as a representation of the population.\nWe often want numeric values to describe the probability distribution in the same way we used statistics for the sample. Since these numeric values will be describing a population, these will be parameters.\nIt is common (but not always) to denote parameters with lowercase Greek letters. Below are some of the statistics we have discussed and the corresponding parameter.\n\n\n\n\nSampleStatistic\nPopulationParameter\n\n\n\n\nMean\n\\(\\bar{x}\\)\n\\(\\mu\\)\n\n\nStandard Deviation\n\\(s\\)\n\\(\\sigma\\)\n\n\nVariance\n\\(s^2\\)\n\\(\\sigma^2\\)\n\n\n\nThe mean or expected value (denoted \\(E(X)\\)) of a discrete probability distribution is a weighted average: \\[\n\\mu = E(X) = \\sum_x x \\cdot P(X=x)\n\\] You can think of the expected value as the long‑run average of \\(X\\) over many repetitions of the experiment.\n\n\n\n\n\n\nExample 6.5: Florida lottery\n\n\n\nThe Florida Lottery runs two popular daily games called Pick 3 and Pick 4.\nIn Cash 3, players pay $1 to select three numbers in order, where each number ranges from 0 to 9. If the three numbers selected (e.g., 2–8–4) match exactly the order of the three numbers drawn, the player wins $500. The probability of winning Pick 3 is 0.001.\nPlay 4 is similar to Cash 3, but players must match four numbers (each number ranging from 0 to 9). For a $1 Play 4 ticket (e.g., 3–8–3–0), the player will win $5,000 if the numbers match the order of the four numbers drawn. The probability of winning Pick 4 is 0.0001\nLet \\(X\\) be the random variable representing the amount of money you get for playing Pick 3.\nThe possible values of \\(X\\) is then\nIf you lose: \\(-\\$1\\)\nIf you win: \\(\\$500-\\$1=\\$499\\)\nWhat is the expected amount of money you get for playing this game? \\[\n\\begin{align*}\n    \\mu &= \\sum xP$X$\\\\\\\\\n    &{=-1(0.999)+499(0.001)}\\\\\n    &{=-0.999+.499}\\\\\n    &{=-0.5}\n\\end{align*}\n\\]\nLet \\(X\\) be the random variable representing the amount of money you get for playing Pick 4.\nThe possible values of \\(X\\) is then\nIf you lose: \\(-\\$1\\)\nIf you win: \\(\\$5000-\\$1=\\$4999\\) The possible values of \\(X\\) is then\nWhat is the expected amount of money you get for playing this game? \\[\n\\begin{align*}\n    \\mu &= \\sum xP$X$\\\\\\\\\n    &{=-1(0.9999)+4999(0.0001)}\\\\\n    &{=-0.9999+.4999}\\\\\n    &{=-0.5}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\nExample 6.6: Life insurance\n\n\n\nSuppose you work for an insurance company and you sell a $10,000 one-year term insurance policy at an annual premium of $290. Actuarial tables show that the probability of death during the next year for a person of your customer’s age, sex, health, etc., is .001. What is the expected gain (amount of money made by the company) for a policy of this type? \\[X=\\text{money made by the company  per policy}\\] If the customer lives: \\(\\$290\\)\nIf the customer dies:\\(\\$290-\\$10,000=-\\$9710\\)\nThe expected gain for a policy of this type: \\[\n\\begin{align*}\n    \\mu &= \\sum xP$X$\\\\\\\\\n    &{=290(0.999)+(-9710)(0.001)}\\\\\n    &{=280}\n\\end{align*}\n\\]\n\n\n\nVariance\nThe variance of \\(X\\), denoted \\(\\sigma^2\\), measures how spread out \\(X\\) is around its mean. It is defined as \\[\n\\begin{align*}\n\\sigma^2 = \\sum_x (x - \\mu)^2  P(X=x)\n\\end{align*}\n\\]\nIt weights squared deviations from the mean by the probability of each value. The standard deviation \\(\\sigma\\) is simply the square root of the variance. It has the same units as \\(X\\) and describes the typical distance between \\(X\\) and its mean.\n\n\n\n\n\n\nExample 6.7: Allele copies revisited\n\n\n\nUsing the allele copy distribution from the previous section, we can compute the mean and variance. Recall the probabilities:\n\n\n\n\\(x\\)\n0\n1\n2\n3\n\n\n\n\n\\(P(X=x)\\)\n0.50\n0.30\n0.15\n0.05\n\n\n\nThe expected value is\n\\[\n\\begin{align*}\n\\mu_X =& 0 \\cdot 0.50 + 1 \\cdot 0.30 + 2 \\cdot 0.15 + 3 \\cdot 0.05\\\\\n=& 0 + 0.30 + 0.30 + 0.15 \\\\\n=& 0.75\n\\end{align*}\n\\]\nOn average, a plant carries 0.75 copies of the allele. To compute the variance, we first calculate \\((x - \\mu)^2\\) for each \\(x\\):\n\n\n\n\\(x\\)\n\\((x - 0.75)^2\\)\n\\(P(X=x)\\)\nContribution\n\n\n\n\n0\n\\((0 - 0.75)^2 = 0.5625\\)\n0.50\n\\(0.5625 \\times 0.50 = 0.28125\\)\n\n\n1\n\\((1 - 0.75)^2 = 0.0625\\)\n0.30\n\\(0.0625 \\times 0.30 = 0.01875\\)\n\n\n2\n\\((2 - 0.75)^2 = 1.5625\\)\n0.15\n\\(1.5625 \\times 0.15 = 0.234375\\)\n\n\n3\n\\((3 - 0.75)^2 = 5.0625\\)\n0.05\n\\(5.0625 \\times 0.05 = 0.253125\\)\n\n\n\nThe variance is the sum of the contributions: \\[\n\\sigma^2 = 0.28125 + 0.01875 + 0.234375 + 0.253125 = 0.7875\n\\]\nThus the standard deviation is \\[\n\\sigma = \\sqrt{0.7875} \\approx 0.887\n\\]\nThis tells us that individual plants typically differ from the mean of 0.75 allele copies by about 0.89 copies. Remember that the mean and standard deviation summarize the distribution; they do not necessarily correspond to actual observed values (you cannot have 0.75 copies of a gene, but the average over many plants can be fractional).\n\n\n\n\nWorking in JMP\nTo compute the mean and standard deviation of a discrete distribution in JMP:\n\nEnter the distribution. Create a table with columns for the values and their probabilities.\nCalculate the expected value. Add a new column and use Column → Formula to multiply each value by its probability. Then use Tables → Summary to sum that column. The sum is \\(\\mu_X\\).\nCompute the variance and standard deviation. Add a column for \\((x - \\mu)^2 \\times P(X=x)\\) (you can store \\(\\mu_X\\) in a script variable for reuse). Summarize that column to get the variance. Take the square root for the standard deviation. Alternatively, use Distribution on the simulated values (as described in Section 6.1) to see empirical estimates of the mean and standard deviation.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nexpected value\nWeighted average of a random variable: \\(\\sum_x x \\,P(X=x)\\). Another name for population mean.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nA random variable \\(W\\) takes values 0, 1, 2, 3 with \\(P(W=0)=0.1\\), \\(P(W=1)=0.2\\), \\(P(W=2)=0.4\\) and \\(P(W=3)=0.3\\). Compute \\(E(W)\\) and \\(\\sigma_W\\).\n\nExplain in plain language what the standard deviation tells you about a random variable.\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nCompute the mean: \\(E(W) = 0(0.1) + 1(0.2) + 2(0.4) + 3(0.3) = 0 + 0.2 + 0.8 + 0.9 = 1.9\\). The variance is \\[\n\\sum (w - 1.9)^2 P(W=w) = (0-1.9)^2\\cdot 0.1 + (1-1.9)^2\\cdot 0.2 + (2-1.9)^2\\cdot 0.4 + (3-1.9)^2\\cdot 0.3.\n\\] Numerically this is \\(3.61(0.1) + 0.81(0.2) + 0.01(0.4) + 1.21(0.3) = 0.361 + 0.162 + 0.004 + 0.363 = 0.890\\). Thus \\(\\sigma = \\sqrt{0.890} \\approx 0.944\\).\nThe standard deviation measures how far the values of a random variable typically fall from their average value. A small standard deviation means the values cluster tightly around the mean; a larger standard deviation means the values are more spread out and vary more from trial to trial.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "06.html#sec-binomial",
    "href": "06.html#sec-binomial",
    "title": "6  Discrete Probability Distributions",
    "section": "6.4 The Binomial Distribution",
    "text": "6.4 The Binomial Distribution\n\n“If people do not believe that mathematics is simple, it is only because they do not realize how complicated life is.” -John Louis von Neumann\n\nMany count variables in medicine and biology arise from a simple process: you perform the same experiment \\(n\\) times, each time there are only two possible outcomes (success or failure), and the probability of success stays the same from trial to trial. The random variable that counts the number of successes in these \\(n\\) trials is said to follow a binomial distribution. We denote it by \\(X \\sim \\mathrm{Bin}(n,p)\\), where:\n\n\\(n\\) is the number of independent trials.\n\\(p\\) is the probability of success on each trial.\n\nFor the binomial model to be appropriate we must have:\n\nA fixed number \\(n\\) of trials.\nEach trial results in a success or failure.\nThe probability \\(p\\) of success is the same on every trial.\nThe trials are independent of each other.\n\n\nMotivating Example\nJohn Doe claims to possess extrasensory perception (ESP). An experiment is conducted in which a person in one room picks one of the integers 1, 2, 3, 4, 5 at random and concentrates on it for one minute. In another room, John Doe identifies the number he believes was picked. The experiment is done with three trials. After the third trial, the random numbers are compared with John Doe’s predictions. Doe got the correct result twice.\nIf John Doe does not actually have ESP and is merely guessing the number, what is the probability that he’d make a correct guess on two of the three trials?\nLet \\(X\\) = number of correct guesses in \\(n = 3\\) trials. Then \\(X = 0, 1, 2, \\text{ or } 3\\).\nLet \\(p\\) denote the probability of a correct guess for a given trial.\nIf Doe is guessing, \\(p = 0.2\\) for Doe’s prediction of one of the five possible integers. Then, \\(1 - p = 0.8\\) is the probability of an incorrect prediction on a given trial.\nDenote the outcome on a given trial by \\(S\\) or \\(F\\), representing success or failure for whether Doe’s guess was correct or not. The table below shows the eight outcomes in the sample space for this experiment. For instance, \\(FSS\\) represents a correct guess on the second and third trials. It also shows their probabilities by using the multiplication rule for independent events.\n\n\n\nThe three ways John Doe could make two correct guesses in three trials are \\(SSF\\), \\(SFS\\), and \\(FSS\\). Each of these has probability equal to \\[\n(0.2)^2(0.8) = 0.032\n\\]\nThe total probability of two correct guesses (note these are mutually exclusive outcomes) is \\[\n\\begin{align*}\nP(SSF\\cup SFS \\cup FSS) = & (0.2)^2(0.8) + (0.2)^2(0.8) + (0.2)^2(0.8)\\\\\n=& 3(0.2)^2(0.8)\\\\\n=& 3(0.032)\\\\\n=& 0.096\n\\end{align*}\n\\]\nWhen the number of trials \\(n\\) is large, it’s tedious to write out all the possible outcomes in the sample space. But there’s a formula you can use to find binomial probabilities for any \\(n\\).\n\n\nProbabilities for a Binomial Distribution\nDenote the probability of success on a trial by \\(p\\). For \\(n\\) independent trials, the probability of \\(x\\) successes equals \\[\nP(X) = \\binom{n}{x}p^x(1-p)^{n-x}, \\qquad x=0, 1, 2, \\ldots, n\n\\] where \\[\n\\binom{n}{x}=\\frac{n!}{x!(n-x)!}\n\\]\nLet’s use this formula to find the probability that Doe would get only one correct in the previous section: \\[\n\\begin{align*}\n    P(1) &= \\frac{3!}{1!(3-1)!}0.2^1(1-0.2)^{3-1}\\\\\\\\\n   & {=\\frac{6}{1}0.2(0.8)^2}\\\\\n   &{ =.3840}\n\\end{align*}\n\\]\n\n\nCheck to See If Binomial Conditions Apply\nBefore you use the binomial distribution, check that its three conditions apply. These are\n\nbinary data (success or failure),\nthe same probability of success for each trial (denoted by \\(p\\)), and\na fixed number \\(n\\) of independent trials.\n\nOne scenario where the assumptions do not apply is when sampling from a small population without replacement. When this occurs, the assumption of independence does not hold since the probability of success will change after each trial.\nGuideline: Population and Sample Sizes to Use the Binomial For sampling \\(n\\) separate subjects from a population (that is, sampling without replacement), the exact probability distribution of the number of successes is too complex to discuss here, but the binomial distribution approximates it well when \\(n\\) is less than 10% of the population size. In practice, sample sizes are usually small compared to population sizes, and this guideline is satisfied.\n\n\nMean and Standard Deviation of the Binomial Distribution\nRecall that the formula for the expected value of a discrete random variable is \\[\nE(X) = \\mu = \\sum xP(X)\n\\]\nIf we substitute in the binomial formula for \\(P(X)\\) in the formula for expected value we get[^2] \\[\n\\begin{align*}\nE(X) &= \\mu = \\sum_{x=0}^n x \\frac{n!}{x!(n-x)!}p^x(1-p)^{n-x}\\\\\n& {= np}\n\\end{align*}\n\\]\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\n\\[\n\\begin{align*}\nE[X]\n&= \\sum_{x=0}^{n} x \\, P(X=x) \\\\\n&= \\sum_{x=0}^{n} x \\binom{n}{x} p^{x}(1-p)^{\\,n-x}.\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\nE[X]\n&= \\sum_{x=1}^{n} x \\binom{n}{x} p^{x}(1-p)^{\\,n-x}\n\\qquad (\\text{the }x=0\\text{ term is }0) \\\\\n&= \\sum_{x=1}^{n} \\left[n\\binom{n-1}{x-1}\\right] p^{x}(1-p)^{\\,n-x}\n\\qquad \\left(\\text{since } x\\binom{n}{x}=n\\binom{n-1}{x-1}\\right) \\\\\n&= np \\sum_{x=1}^{n} \\binom{n-1}{x-1} p^{x-1}(1-p)^{\\,n-x}.\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\nE[X]\n&= np \\sum_{y=0}^{n-1} \\binom{n-1}{y} p^{y}(1-p)^{\\,(n-1)-y}\n\\qquad (\\text{let } y=x-1) \\\\\n&= np \\cdot 1 \\\\\n&= np\n\\end{align*}\n\\]\n\n\n\nSo if we know the random variable is binomial, the expected value is just \\(E(X) =np\\).\nThe same thing can be done for the variance. If we know the random variable is binomial, the variance and standard deviation are \\[\n{\\sigma^2=npq}\\qquad\\qquad{\\sigma=\\sqrt{npq}}\n\\]\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nWe begin from the probability mass function \\[\nP(X=x)=\\binom{n}{x}p^x(1-p)^{,n-x}, \\qquad x=0,1,\\dots,n.\n\\]\nRecall \\[\n\\mathrm{Var}(X)=E[X^2]-(E[X])^2,\n\\] and from the previous result, \\[\nE[X]=np.\n\\]\nCompute \\(E[X(X-1)]\\)\nA useful identity is \\[\nX^2 = X(X-1)+X.\n\\] So we first compute \\(E[X(X-1)]\\).\n\\[\n\\begin{align}\nE[X(X-1)]\n&= \\sum_{x=0}^{n} x(x-1)\\binom{n}{x}p^x(1-p)^{,n-x} \\\\\n&= \\sum_{x=2}^{n} x(x-1)\\binom{n}{x}p^x(1-p)^{,n-x}\n\\qquad (\\text{terms }x=0,1\\text{ vanish}) \\\\\n&= \\sum_{x=2}^{n} n(n-1)\\binom{n-2}{x-2}p^x(1-p)^{,n-x}\n\\end{align}\n\\]\nusing the combinatorial identity \\[\nx(x-1)\\binom{n}{x}=n(n-1)\\binom{n-2}{x-2}.\n\\]\nFactor out constants and two powers of \\(p\\):\n\\[\n\\begin{align}\nE[X(X-1)]\n&= n(n-1)p^2\n\\sum_{x=2}^{n}\n\\binom{n-2}{x-2}p^{x-2}(1-p)^{,n-x}.\n\\end{align}\n\\]\nLet \\(y=x-2\\). Then \\(y=0,\\dots,n-2\\):\n\\[\n\\begin{align}\nE[X(X-1)]\n&= n(n-1)p^2\n\\sum_{y=0}^{n-2}\n\\binom{n-2}{y}p^{y}(1-p)^{(n-2)-y}.\n\\end{align}\n\\]\nApply the binomial theorem:\n\\[\n\\begin{align}\nE[X(X-1)]\n&= n(n-1)p^2 (p+(1-p))^{n-2} \\\n&= n(n-1)p^2.\n\\end{align}\n\\]\nCompute \\(E[X^2]\\)\nUsing \\(X^2=X(X-1)+X\\),\n\\[\n\\begin{align}\nE[X^2]\n&= E[X(X-1)] + E[X] \\\\\n&= n(n-1)p^2 + np.\n\\end{align}\n\\]\nNow apply the variance formula:\n\\[\n\\begin{align}\n\\mathrm{Var}(X)\n&= E[X^2] - (E[X])^2 \\\\\n&= \\left[n(n-1)p^2 + np\\right] - (np)^2 \\\\\n&= n(n-1)p^2 + np - n^2p^2 \\\\\n&= np - np^2 \\\\\n&= np(1-p).\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\nExample 6.8: Vaccine side effects\n\n\n\nFor a vaccine trial: each of 10 volunteers has a 0.2 probability of experiencing a side effect. Let \\(X \\sim \\mathrm{Bin}(10, 0.2)\\) be the number of side effects. The probability of exactly \\(x\\) side effects is \\[\nP(X = x) = \\binom{10}{x} 0.2^x 0.8^{10 - x}.\n\\] For instance, the probability of exactly 3 participants experiencing side effects is \\[\n\\begin{align*}\nP(X=3) &= \\binom{10}{3} (0.2)^3 (0.8)^{7}\\\\\n&= 120 \\times 0.008 \\times 0.2097152\\\\\n&\\approx 0.2013\n\\end{align*}\n\\] The expected number of side effects is \\[\nE(X)=10\\times 0.2=2\n\\] and the standard deviation is \\[\n\\sigma = \\sqrt{10 \\times 0.2 \\times 0.8}\\approx 1.265\n\\]\nA bar chart of the distribution shows that most of the probability mass is centered around 2:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 6.9: Gene carriers\n\n\n\nSuppose a particular allele is present in 10% of a plant population. If you examine 12 randomly selected plants, the number \\(Y\\) of plants carrying the allele follows \\(\\mathrm{Bin}(n=12, p=0.10)\\).\nThe probability of exactly \\(x\\) carriers is \\[\n\\binom{12}{x} 0.1^x 0.9^{12-x}\n\\]\nThe expected number of carriers is \\(12 \\times 0.10 = 1.2\\) and the standard deviation is \\(\\sqrt{12 \\times 0.10 \\times 0.90} \\approx 1.039\\).\nThe probability of at least one carrier is \\[\n1 - P(Y=0) = 1 - (0.9)^{12} \\approx 0.717\n\\] meaning there is a 71.7% chance of finding at least one resistant plant in 12.\n\n\n\n\n\n\n\n\nExample 6.10: Counting positive results\n\n\n\nBiologists often perform assays where each sample has a probability \\(p\\) of yielding a positive result. If you test \\(n\\) independent samples, the number of positives follows a binomial distribution. For example, if you test 6 fruit flies for a particular gene with presence rate 0.3, the distribution of \\(X \\sim \\mathrm{Bin}(6, 0.3)\\) is:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(x\\)\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(P(X=x)\\)\n\\((0.7)^6\\)\n\\(6(0.3)(0.7)^5\\)\n\\(15(0.3^2)(0.7^4)\\)\n…\n…\n…\n\\((0.3)^6\\)\n\n\n\nThe probabilities can be computed by hand or with software.\n\n\n\n\nWorking in JMP\nJMP has built‑in tools for the binomial distribution:\n\nProbability Calculator. Choose Help → Probability Calculator, select Binomial as the distribution, and specify \\(n\\) and \\(p\\). You can then compute probabilities for specific values (e.g., \\(P(X=3)\\)), cumulative probabilities (e.g., \\(P(X \\le 3)\\)), and tail probabilities (e.g., \\(P(X \\ge 5)\\)). The calculator displays both numerical results and a dynamic bar chart.\nSimulate binomial counts. Add a new column and set its formula to Random Binomial(n, p). Generate many rows to see the empirical distribution, and analyse it with Distribution. This is a good way to build intuition for binomial variability.\nExpected value and standard deviation. JMP’s probability calculator reports the mean and standard deviation for the chosen \\(n\\) and \\(p\\). You can also compute them manually using formula columns (n*p and sqrt(n*p*(1-p))).\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nbinomial distribution\nThe distribution of the number of successes in \\(n\\) independent trials with success probability \\(p\\).\n\n\nbinomial pmf\n\\(P(X=x) = \\binom{n}{x} p^x (1-p)^{n-x}\\) for \\(x=0,\\dots,n\\).\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nAn antibiotic cures a bacterial infection in 75 % of cases. In a study of 8 independent patients, let \\(X\\) be the number of patients cured. Compute:\n\n\\(P(X=6)\\).\n\n\\(P(X\\le 4)\\).\n\n\\(E\\)X$$ and \\(\\sigma\\)\n\nA gene occurs in 15% of the population. You sample 10 individuals at random. What is the probability that exactly 2 individuals carry the gene? What is the probability that at least one individual carries the gene?\n\nA researcher flips a biased coin (probability of heads is 0.6) 5 times. Does the number of heads follow a binomial distribution? Why or why not?\n\nDescribe a real‑world scenario where the binomial model would not be appropriate even though the outcome is a count of successes. Explain which assumption is violated.\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nHere \\(X \\sim \\mathrm{Bin}(n=8,p=0.75)\\). a) \\(P(X=6) = \\binom{8}{6} 0.75^6 0.25^2 = 28 \\times 0.1779785 \\times 0.0625 \\approx 0.311\\). b) \\(P(X\\le 4) = \\sum_{x=0}^4 \\binom{8}{x} 0.75^x 0.25^{8-x} \\approx 0.0081\\) (you can compute this with a calculator or software). c) The mean is \\(E\\)X\\(=8\\times 0.75=6\\) and the standard deviation is \\(\\sigma_X=\\sqrt{8\\times 0.75 \\times 0.25}=\\sqrt{1.5}\\approx 1.225\\).\n\nLet \\(Y \\sim \\mathrm{Bin}(10,0.15)\\). Then \\(P(Y=2) = \\binom{10}{2} (0.15)^2 (0.85)^8 \\approx 45 \\times 0.0225 \\times 0.27249 \\approx 0.276\\). The probability of at least one carrier is \\(1 - P(Y=0) = 1 - 0.85^{10} \\approx 1 - 0.1969 = 0.8031\\).\n\nYes. Each flip is a trial with two outcomes (heads or tails), the probability of heads is the same (0.6) on each flip, and the flips are assumed independent. Therefore the number of heads in 5 flips follows \\(\\mathrm{Bin}(5,0.6)\\).\n\nSuppose we sample 10 patients from a small village where tuberculosis is contagious and individuals tend to be exposed through one another. Let \\(X\\) be the number of infected patients. Even if each individual infection has some probability \\(p\\), the infections are not independent: once one person is infected, the chance that their neighbor is infected rises. This dependence violates the independent‑trials assumption of the binomial model, so \\(X\\) would not follow a binomial distribution.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "06.html#footnotes",
    "href": "06.html#footnotes",
    "title": "6  Discrete Probability Distributions",
    "section": "",
    "text": "Note that we use a uppercase letter to denote the random variable itself. We use a lowercase letter when denoting a realization of that random variable.↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "07.html",
    "href": "07.html",
    "title": "7  Continuous Probability Distributions",
    "section": "",
    "text": "7.1 Continuous Probability Distributions and Their Properties\nSo far, we has focused on discrete outcomes: counts of patients, number of mutated alleles and so on. In those settings we could list the possible values, assign a probability to each one, and check that the probabilities summed to one. Many measurements in medicine and biology, however, can take any value within a range rather than a handful of distinct values. A person’s height could be 170.23 cm or 170.231 cm; the concentration of a hormone in blood plasma might be 2.7 or 2.701 ng/mL. When a random quantity can assume infinitely many values on an interval we call it a continuous random variable.\nBecause there are infinitely many possible values, we cannot find the probability that a continuous random variable takes any exact value. Instead of assigning probabilities to single points, we assign probabilities to intervals: the chance that a drug’s plasma concentration is between 2.5 and 3.5 ng/mL, for example.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "07.html#sec-07_01",
    "href": "07.html#sec-07_01",
    "title": "7  Continuous Probability Distributions",
    "section": "",
    "text": "“Statistics is the grammar of science.” – Karl Pearson\n\n\n\n\nProbability Distribution for Continuous Random Variables\nRecall in Chapter 3, we discussed histograms in which the width of the bars is some interval of values and the height is either the frequency or relative frequency of the observations that fall in that interval. We could examine the relative frequency of the data that fall between any two value by adding the relative frequencies of the bars in that interval. For example, suppose we are looking for the relative frequency of the data shaded in the histogram below.\n\nThus, the area of these bars is the relative frequency in the interval of interest. In Chapter 5, we stated that we are using the relative frequency interpretation of probability. Therefore, the area shaded in the histogram will estimate the probability of the random variable being in that interval.\nNow, let’s think of all of the possible data in the population. In this case, we can shrink the width of the bars to however small we wish. As we let the bar widths shrink to zero, then we end up with a smooth curve like below.\n\nThe smooth curve (probability distribution of a continuous random variable) is denoted by the symbol \\(f(x)\\) and is often called the probability density function (pdf). We can still view the area of the shaded region as the probability.\nBecause the area under the pdf represents probability, then by definition we have \\[\nP(X=x)=0\n\\] In other words, we assign a probability of zero at a point. This happens since there is no area under the curve at a point.\nWe do find the probability of a continuous random variable in an interval: \\[\nP(a&lt; X&lt; b)\n\\] How do we do this? We find the area under the curve between \\(a\\) and \\(b\\). We find the area under the curve by taking the integral \\[\nP(a &lt; X &lt; b) = \\int_a^b f(x) \\; dx.\n\\]\nThe cumulative distribution function (cdf) of \\(X\\), denoted \\(F(x)\\), gives the probability that \\(X\\) is less than or equal to \\(x\\); it is the area under the density to the left of \\(x\\).\nIn summary:\nThe probability distribution of a continuous random variable \\(X\\)\n\nis represented by a smooth curve\nthe curve is called the probability density function (pdf)\nthe probability \\(P(a&lt;X&lt;b)=P(a\\le X\\le b)\\) is the area under the curve between \\(a\\) and \\(b\\)\nthe cumulative distribution function (cdf) give the area to the left of some value: \\(F(x)=P(X\\le x)\\)\n\n\n\n\n\n\n\nExample 7.1: Drug Metabolism\n\n\n\n\n\nAfter an oral dose, the amount of a drug in the bloodstream rises and then falls over time. If we pick a random patient and record their peak plasma concentration, that value could be any number within a physiological range.\nThe probability that the peak is exactly 3.000 µg/mL is zero; but we can talk meaningfully about the probability it lies between 2.8 and 3.2 µg/mL, which is the area under the density between those points.\n\n\n\n\n\n\n\n\n\nExample 7.2: Plant heights\n\n\n\n\n\nThe height of a genetically identical group of plants grown under controlled conditions will vary due to micro‑environmental factors. Those heights are modeled as a continuous random variable. We might ask, for example, how likely it is for a plant to be taller than 15 cm; again, we look at the area under the density to the right of 15.\n\n\n\nWhen interpreting a probability density function for a continuous random variable, it is crucial to remember what the graph is—and what it is not—telling you.\nA pdf does not give the probability of a specific value. Instead, it describes how probability is distributed across an interval. The vertical axis represents probability density, not probability itself. As a result, a taller portion of the curve indicates a region where probability is more densely concentrated—but the probability of any single exact value is still zero.\nA helpful analogy is to think of the pdf like a topographic map. A taller peak indicates where probability is concentrated, but the “amount” of probability depends on how wide that region is. A very tall but extremely narrow spike may contribute little total probability because its area is small.\n\n\nMean, Variance, and Standard Deviation of a Continuous Random Variable\nRecall that for a discrete random variable, probabilities are assigned to individual points. Because there are only countably many possible values, we can compute means and variances using sums:\n\\[\n\\mu = \\sum_x xP(x),\n\\qquad\n\\sigma^2 = \\sum_x (x-\\mu)^2 P(x).\n\\]\nEach term in the sum represents the contribution of one possible value of \\(X\\), weighted by how likely that value is.\n\nWhy sums no longer work for continuous variables\nFor a continuous random variable, the situation is fundamentally different. The variable can take infinitely many values in any interval, and the probability of any single exact value is \\[\nP(X=a)=0.\n\\]\nBecause probability is spread continuously across the number line, there is no meaningful way to “add up” probabilities at individual points. Instead, probability accumulates over intervals, and the natural mathematical tool for accumulation over infinitely many points is integration.\nYou can think of the discrete formulas as weighted sums and the continuous formulas as their limiting counterparts—weighted integrals.\n\n\nExpected value for a continuous random variable\nIf \\(X\\) has probability density function \\(f(x)\\), the expected value (mean) is\n\\[\n\\mu = \\int_{-\\infty}^{\\infty} x f(x)dx.\n\\]\nThis integral plays the same conceptual role as the discrete sum:\n\n\\(x\\) is the value,\n\\(f(x)\\) is the “weight” (probability density),\nthe integral aggregates contributions across all possible values.\n\nGraphically, this computes a kind of balance point of the density curve, just as the discrete mean is the balance point of weighted masses.\nFor the mean to exist, the integral must converge (be finite).\n\n\nVariance for a continuous random variable\nThe variance measures the average squared distance from the mean. For a continuous random variable,\n\\[\n\\sigma^2 = \\int_{-\\infty}^{\\infty} (x-\\mu)^2 f(x)dx.\n\\]\nThis mirrors the discrete formula but replaces the sum with an integral.\n\n\nDiscrete vs. continuous: the big picture\nIt is helpful to see the parallel structure:\n\\[\n\\begin{aligned}\n\\textbf{Discrete:} \\quad & \\sum_x (\\text{value}) \\times (\\text{probability}) \\\\\n\\textbf{Continuous:} \\quad & \\int (\\text{value}) \\times (\\text{density})dx\n\\end{aligned}\n\\]\nThe formulas look similar because they represent the same conceptual operation—a weighted average—but adapted to two different types of random variables.\nIntegration is the natural extension of summation when probability is distributed continuously across infinitely many possible values.\n\n\n\n\n\n\nExample 7.3: Mean and Variance of a Continuous Random Variable\n\n\n\n\n\nSuppose \\(X\\) has pdf\n\\[\nf(x)=\n2x, \\qquad 0 \\le x \\le 1\n\\]\nWe will find the mean \\(,\\mu = E[X],\\) and the variance \\(,\\sigma^2 = \\mathrm{Var}(X),\\).\n\nFind the mean\nFor a continuous random variable,\n\\[\n\\mu = E[X] = \\int_{-\\infty}^{\\infty} x f(x)dx.\n\\]\nSince \\(f(x)=2x\\) only on \\([0,1]\\), this becomes\n\\[\n\\mu = \\int_0^1 x(2x)dx\\\\\n= \\int_0^1 2x^2dx.\n\\]\nNow integrate:\n\\[\n\\begin{align*}\n\\mu &= 2\\int_0^1 x^2,dx\\\\\n&= 2\\left[\\frac{x^3}{3}\\right]_0^1\\\\\n&= 2\\left(\\frac{1}{3}-0\\right)\\\\\n&= \\frac{2}{3}\n\\end{align*}\n\\]\nSo the mean is\n\\[\n\\mu = \\frac{2}{3}.\n\\]\n\n\nFind the variance\nUse\n\\[\n\\sigma^2 = \\int_{-\\infty}^{\\infty} (x-\\mu)^2 f(x)dx.\n\\]\nSubstitute \\(\\mu=\\frac{2}{3}\\) and \\(f(x)=2x\\) on \\([0,1]\\):\n\\[\n\\sigma^2 = \\int_0^1 \\left(x-\\frac{2}{3}\\right)^2(2x)dx.\n\\]\nExpand the square first:\n\\[\n\\left(x-\\frac{2}{3}\\right)^2 = x^2 - \\frac{4}{3}x + \\frac{4}{9}.\n\\]\nNow multiply by \\(2x\\):\n\\[\n\\begin{align*}\n\\left(x-\\frac{2}{3}\\right)^2(2x)\n&= 2x\\left(x^2 - \\frac{4}{3}x + \\frac{4}{9}\\right)\\\\\n&= 2x^3 - \\frac{8}{3}x^2 + \\frac{8}{9}x\n\\end{align*}\n\\]\nSo\n\\[\n\\sigma^2 = \\int_0^1 \\left(2x^3 - \\frac{8}{3}x^2 + \\frac{8}{9}x\\right)dx.\n\\]\nIntegrate term by term:\n\\[\n\\begin{align*}\n\\sigma^2\n&= \\left[\\frac{2x^4}{4} - \\frac{8}{3}\\cdot\\frac{x^3}{3} + \\frac{8}{9}\\cdot\\frac{x^2}{2}\\right]_0^1\\\\\n&= \\left[\\frac{x^4}{2} - \\frac{8x^3}{9} + \\frac{4x^2}{9}\\right]_0^1\n\\end{align*}\n\\]\nEvaluate at \\(x=1\\) and \\(x=0\\):\n\\[\n\\begin{align*}\n\\sigma^2\n&= \\left(\\frac{1}{2} - \\frac{8}{9} + \\frac{4}{9}\\right) - 0\\\\\n&= \\frac{1}{2} - \\frac{4}{9}\n\\end{align*}\n\\]\nPut over a common denominator:\n\\[\n\\begin{align*}\n\\frac{1}{2} - \\frac{4}{9}\n&= \\frac{9}{18} - \\frac{8}{18}\\\\\n&= \\frac{1}{18}\n\\end{align*}\n\\]\nSo the variance is\n\\[\n\\sigma^2 = \\frac{1}{18}.\n\\]\n\n\n\n\n\n\n\nWorking in JMP\nJMP can help you explore continuous distributions experimentally. Here is a general workflow using an exponential example, but you can adapt it to other distributions:\n\nSimulate continuous data. Create a new data table and use Rows → Add Rows to add, say, 1 000 rows. Add a new column and choose Column → Formula. In the formula editor search for Random Exponential(rate) and specify a rate (e.g., 1/10). Each cell will then contain a simulated lifetime.\nVisualize the distribution. Use Analyze → Distribution and select your simulated column. JMP produces a histogram and summary statistics. You can overlay a smooth density by clicking the red triangle ▸ next to the variable name and choosing Continuous Fit → Exponential.\nCompute probabilities. JMP’s distribution calculator (found under Add‑ins → Calculators → Distribution Calculator in JMP Pro 17) lets you choose a distribution, enter parameter values, and compute the probability that a continuous random variable lies between two values. For the exponential example, choose Exponential, set the rate, and enter the lower and upper bounds to find \\(P(a ≤ X ≤ b)\\).\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\ncontinuous random variable\nA random variable that can take any value in an interval; probabilities are assigned to ranges of values rather than individual points.\n\n\nprobability density function (PDF)\nA non‑negative function \\(f(x)\\) such that \\(P(a ≤ X ≤ b)\\) equals the area under \\(f(x)\\) between \\(a\\) and \\(b\\) and the total area under the curve of \\(f(x)\\) is one.\n\n\ncumulative distribution function (CDF)\nThe function \\(F(x)=P(X ≤ x)\\) giving the area under the PDF to the left of \\(x\\). It increases from 0 to 1 as \\(x\\) goes from \\(-∞\\) to \\(∞\\).\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nExplain in your own words why the probability that a continuous random variable equals exactly 5 is zero. How, then, do we assign probabilities for continuous variables?\nSketch or describe the shape of a PDF that would model serum cholesterol levels in a population. Why can’t a PDF ever dip below the horizontal axis?\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nA continuous random variable can take infinitely many values within any interval. Because the PDF spreads probability continuously across these values, the probability of landing on any single point is zero. We obtain meaningful probabilities by integrating the density over an interval to find the area under the curve between the limits.\nSerum cholesterol tends to cluster around an average value with fewer extremely low or high values. A plausible PDF would be unimodal and right‑skewed: low near 0, rising to a peak near the typical cholesterol level, and gradually decreasing. The density must always stay at or above zero because probabilities cannot be negative.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "07.html#sec-07_02",
    "href": "07.html#sec-07_02",
    "title": "7  Continuous Probability Distributions",
    "section": "7.2 The Uniform Distribution",
    "text": "7.2 The Uniform Distribution\n\n“Don’t mistake possibilities for probabilities. Anything is possible. It’s the probabilities that matter” – Ray Dalio\n\nThe simplest continuous distribution is the uniform distribution. Imagine selecting a time uniformly at random within a two‑hour window; any minute in that window is just as likely as any other. More formally, a continuous random variable \\(X\\) has a Uniform\\((c,d)\\) distribution if its PDF is constant on the interval \\((c,d)\\) and zero elsewhere.\nSince all values in the interval \\((c,d)\\) are equally likely, the pdf of a uniform random variable appears as a horizontal line:\n\n\n\n\n\n\n\n\n\nNote that the area under the curve must equal 1 (since the area corresponds to probability). Therefore, the area between \\(c\\) and \\(d\\) \\[\n\\begin{align*}\n    \\text{area of rectangle} = \\text{base}\\times \\text{height} &\\Longrightarrow{ 1 = (d-c) \\times \\text{height}}\\\\\\\\\n    &\\Longrightarrow \\text{height} = \\frac{1}{d-c}\n\\end{align*}\n\\]\nSo, the pdf of a uniform random variable is \\[\nf(x) = \\frac{1}{d-c}\n\\]\n\n\n\n\n\n\n\n\n\nThe expected value is \\[\n\\begin{align*}\nE(X)=\\int_{c}^d xf(x)dx &= \\int_c^dx\\left(\\frac{1}{d-c}\\right)dx\\\\\n& {= \\left(\\frac{1}{d-c}\\right)\\int_c^dxdx}\\\\\n& {= \\left(\\frac{1}{d-c}\\right)\\left(\\frac{1}{2}\\right)x^2\\Big\\vert^d_c} \\\\\n& {= \\left(\\frac{1}{d-c}\\right)\\left(\\frac{1}{2}\\right)\\left(d^2-c^2\\right)} \\\\\n& {= \\left(\\frac{1}{d-c}\\right)\\left(\\frac{1}{2}\\right)\\left(d-c\\right)\\left(d+c\\right)} \\\\\n& {= \\frac{c+d}{2}}\n\\end{align*}\n\\]\nWe will not show the steps here but we could find the variance in a similar fashion to get \\[\n\\sigma^2 = \\frac{\\left(d-c\\right)^2}{12}\n\\] The standard deviation is then \\[\n\\sigma = \\frac{\\left(d-c\\right)}{\\sqrt{12}}\n\\]\nFor a uniform random variable \\(X\\), what is the probability \\(P(a&lt;X&lt;b)\\)? \\[\n\\begin{align*}\n    P(a&lt;X&lt;b)=\\int_{a}^b f(x)dx & = \\int_{a}^b \\frac{1}{d-c} dx\\\\\n   & {= \\left(\\frac{1}{d-c}\\right)x\\Big\\vert_a^b}\\\\\n    &{= \\left(\\frac{b-a}{d-c}\\right)}\\\\\n\\end{align*}\n\\]\nSummary of Uniform RVs:\n\nthe pdf is \\(f(x)=\\frac{1}{d-c}\\qquad c\\le X\\le d\\)\nthe mean is \\(\\mu = \\frac{c+d}{2}\\) and the standard deviation is \\(\\sigma=\\frac{d-c}{\\sqrt{12}}\\)\n\\(P(a&lt;X&lt;b)=\\frac{b-a}{d-c}\\)\n\n\n\n\n\n\n\nExample 7.4: Patient Arrival Time\n\n\n\n\n\nSuppose a clinic accepts blood samples from 8 am to 10 am and the phlebotomist expects donors to arrive at random. Let \\(T\\) be the arrival time after 8 am (in hours).\nIf arrivals are equally likely at any moment, \\(T \\sim \\text{Uniform}(0,2)\\). The probability that a randomly arriving donor comes between 8:30 and 9:00 am (i.e., \\(0.5 ≤ T ≤ 1\\)) is \\[\n\\begin{align*}\nP(0.5 ≤ T ≤ 1)&=\\frac{1-0.5}{2-0}\\\\\n&=0.25\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\nExample 7.5: Randomized drug administration\n\n\n\n\n\nn a study, participants are randomly assigned to take a dose of medication at any time between noon and 3 pm. The time of ingestion is Uniform\\((0,3)\\) hours after noon. If we want the probability that a dose is taken in the first half‑hour, we compute \\[\n\\begin{align*}\nP(0 ≤ T ≤ 0.5)=&\\frac{0.5-0}{3-0}\\\\\n=&0.167\n\\end{align*}\n\\]\n\n\n\n\nWorking in JMP\nUniform simulations are straightforward in JMP:\n\nGenerate uniform random values. In a new data table, choose Rows → Add Rows to add your desired number of observations. Use Column → Formula, find the function Random Uniform, and specify the lower and upper bounds \\(c\\) and \\(d\\).\nVisualize and compute probabilities. Use Analyze → Distribution to produce a histogram. Since the density is flat, the histogram should approximate a rectangle when you use many bins. To compute \\(P(a ≤ X ≤ b)\\) without simulation, use the distribution calculator: select Uniform from the list, enter \\(c\\) and \\(d\\), and set the lower and upper limits. JMP will report the probability \\((b-a)/(d-c)\\).\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nuniform distribution\nA continuous distribution on \\((c,d)\\) whose PDF is constant at height \\(1/(d-c)\\). All intervals of equal length within \\((c,d)\\) have equal probability.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nSuppose \\(X\\sim\\text{Uniform}(0,10)\\). What is \\(P(3 ≤ X ≤ 7)\\)? Explain your reasoning.\nA researcher measures the pH of soil samples collected uniformly at random along a transect from 0 to 100 m. What is the probability that a randomly selected soil sample comes from between 20 m and 35 m? Express your answer numerically.\nIf \\(Y\\sim\\text{Uniform}(c,d)\\) and you know that \\(P(Y ≤ 5) = 0.5\\), what relationship does this imply between \\(c\\), \\(d\\) and 5?\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nThe interval from 3 to 7 has length 4. Since the distribution is Uniform\\((0,10)\\), the probability of any subinterval equals its length divided by the total length: \\(4/10=0.4\\).\nThe transect is 100 m long. The segment from 20 to 35 m is 15 m long, so \\(P(20 ≤ X ≤ 35) = 15/100 = 0.15\\).\nFor a uniform distribution, \\(P(Y ≤ y) = (y-c)/(d-c)\\) for \\(c ≤ y ≤ d\\). Setting \\(P(Y≤5)=0.5\\) implies \\((5 - c)/(d - c) = 0.5\\). Equivalently, \\(5\\) is the midpoint of the interval and \\(5 = (c + d)/2\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "07.html#sec-07_03",
    "href": "07.html#sec-07_03",
    "title": "7  Continuous Probability Distributions",
    "section": "7.3 The Normal Distribution",
    "text": "7.3 The Normal Distribution\n\n“the normal distribution is seldom, if ever, observed in nature.” – Louis Guttman\n\nThe normal distribution (also called the Gaussian distribution) is the most celebrated continuous distribution in statistics. It appears throughout science, engineering, and the social sciences and forms the theoretical backbone of many statistical procedures, including confidence intervals, hypothesis tests, and regression methods.\n\nBasic shape and properties\nA random variable with distribution \\(N(\\mu,\\sigma)\\) has a density curve that is:\n\nsymmetric about \\(\\mu\\),\nbell-shaped (unimodal with a single peak),\nand continuous over the entire real line.\n\nBecause of the symmetry,\n\\[\n\\text{mean} = \\text{median} = \\text{mode} = \\mu.\n\\]\nThe curve extends infinitely in both directions, getting closer and closer to the horizontal axis but never actually touching it. Thus, in theory, any real number is possible, although values far from the mean have very small probability density.\n\n\nThe parameters \\(\\mu\\) and \\(\\sigma\\)\nThe normal distribution is completely determined by two parameters:\n\n\\(\\mu\\) (mu): the location parameter, which sets the center of the distribution.\n\\(\\sigma\\) (sigma): the scale parameter, which determines the spread.\n\nMore specifically:\n\n\\(\\mu\\) is the location of the peak and the balance point of the distribution.\n\\(\\sigma\\) is the standard deviation and controls the width of the bell.\n\nChanging \\(\\mu\\) shifts the entire curve left or right without altering its shape. Changing \\(\\sigma\\) stretches or compresses the curve:\n\nLarger \\(\\sigma\\) → wider, flatter bell\nSmaller \\(\\sigma\\) → narrower, taller bell\n\n\n\n\n\n\n\n\n\n\n\n\nThe Normal Probability Density Function\nThe probability density function (pdf) of a normal random variable is\n\\[\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\n\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right), \\qquad -\\infty &lt; x &lt; \\infty.\n\\]\nWhile this formula looks complicated, its key role is to produce the familiar bell-shaped curve and to ensure that the total area under the curve equals 1.\nYou do not need to memorize this formula to use the normal model. More important are its qualitative properties: it is unimodal, symmetric, and tails off smoothly; the total area under the curve is one.\n\n\nWhy the normal distribution matters\nThe normal distribution occupies a special place in statistics not because every dataset is normal—many are not—but because the normal model emerges naturally in a wide range of settings and underpins much of statistical theory and practice.\nSeveral key reasons explain its importance:\n\nMany natural phenomena are approximately normal. Measurements such as adult heights, measurement errors in instruments, and certain biological traits often display the familiar symmetric, bell-shaped pattern. This occurs because the observed value is frequently the result of many small, independent influences acting together. When no single factor dominates, the combined effect tends to produce a roughly normal distribution.\nSample means are often approximately normal (Central Limit Theorem). Perhaps the most powerful reason for the normal distribution’s prominence is the Central Limit Theorem (CLT). Roughly speaking, the CLT states that the distribution of the sample mean becomes approximately normal as the sample size grows, regardless of the shape of the original population (provided certain mild conditions hold). This remarkable result allows statisticians to use normal-based methods even when the underlying data are skewed or irregular. We will discuss the Central Limit Theorem more thouroughly in Chapter 8.\nMany statistical methods rely on normal approximations. Classical inference procedures—such as z-tests, many confidence intervals, regression inference, and numerous quality-control methods—are derived under normal assumptions or justified by the CLT. The mathematics of the normal distribution is especially tractable, which makes it a convenient and powerful modeling tool.\n\nBeyond these points, the normal distribution also serves as a benchmark model. Analysts often begin by comparing their data to a normal curve to assess symmetry, detect skewness, or identify heavy tails. Even when the normal model is not ultimately appropriate, it provides a useful reference point for understanding the data’s structure.\nImportantly, real-world data are rarely perfectly normal. They may be skewed, have heavier tails, or exhibit multiple peaks. Nevertheless, the normal model often provides a useful first approximation, especially for averages and measurement error, and it forms the conceptual and mathematical foundation for many more advanced statistical techniques.\nIn short, the normal distribution matters not because it fits everything perfectly, but because it appears frequently, behaves predictably, and enables a vast toolkit of statistical methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking in JMP\nTo explore normal distributions in JMP:\n\nSimulate normal data. Create a new column with Column → Formula and use Random Normal(µ, σ) to generate values. For example, Random Normal(175,7) will simulate heights in centimetres with mean 175 and standard deviation 7.\nVisualise the distribution. Use Analyze → Distribution to generate a histogram and overlay a fitted normal curve. Click the red triangle ▸ next to the variable and choose Continuous Fit → Normal. JMP displays parameter estimates and a density overlay.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nnormal distribution\nA continuous, symmetric bell‑shaped distribution defined by its mean \\(\\mu\\) and standard deviation \\(\\sigma\\); mean = median = mode.\n\n\nstandard normal distribution\nThe special case Normal\\((0,1)\\); its values are often called z‑scores, and any normal distribution can be standardized via \\(z = (x - \\mu)/\\sigma\\).\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nWhat does it mean that the mean, median and mode of a normal distribution are equal? How is this reflected in the shape of the curve?\nFor a \\(N(150,20)\\) distribution (representing, say, birth weights in grams), approximately what percentage of babies weigh between 110 g and 190 g?\nExplain why extreme values (more than 3 standard deviations from the mean) are considered unusual under the normal model.\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nA normal curve is perfectly symmetric about its mean; the highest point occurs at \\(\\mu\\) and the curve declines equally on both sides. Because of this symmetry, the most typical value (the mode), the point dividing the distribution in half (the median) and the arithmetic average (the mean) coincide.\nTwo standard deviations on either side of the mean cover about 95% of the data. The interval from \\(\\mu-2\\sigma = 150 - 40 = 110\\) to \\(\\mu+2\\sigma = 190\\) therefore captures roughly 95% of birth weights.\nUnder the normal model, only about 0.3% of observations lie beyond three standard deviations from the mean by the empirical rule. Thus values outside that range are rare and often signal measurement error or a departure from normality.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "07.html#sec-07_04",
    "href": "07.html#sec-07_04",
    "title": "7  Continuous Probability Distributions",
    "section": "7.4 Finding Probability for a Normal Distribution",
    "text": "7.4 Finding Probability for a Normal Distribution\n\n“I once worked with a guy for three years and never learned his name. Best friend I ever had. We still never talk sometimes.” -Ron Swanson\n\nOnce we determine the normal model for a variable, we can compute probabilities by standardizing the variable to a z‑score. Given \\(X\\sim N(\\mu,\\sigma)\\), the z‑score corresponding to a value \\(x\\) is\n\\[\nz = \\frac{x - \\mu}{\\sigma}.\n\\]\nThis transformation rescales and recenters \\(X\\) so that \\(Z \\sim N(0,1)\\). We then look up the area under the standard normal curve up to \\(z\\) (or beyond) using tables, software or JMP. Because the normal distribution is continuous, we always compute probabilities for intervals, not exact points.\n\nStep‑by‑step procedure\n\nState the distribution. Identify \\(\\mu\\) and \\(\\sigma\\) for your normal variable.\nDraw a sketch. Label the mean and the point(s) of interest on a bell curve. Shading the region corresponding to the probability helps visualize whether you need the area to the left, right or between two points.\nCompute z‑scores. For each boundary \\(x\\) compute \\(z=(x-\\mu)/\\sigma\\).\nUse a table or software. For the standard normal distribution, tables give \\(P(Z ≤ z)\\) for many \\(z\\) values. For probabilities of the form \\(P(X ≥ x)\\) or \\(P(a ≤ X ≤ b)\\), convert to z‑scores and use the fact that \\(P(Z &gt; z) = 1 - P(Z ≤ z)\\) and \\(P(a ≤ X ≤ b) = P(z_a ≤ Z ≤ z_b)\\).\nInterpret in context. State your answer in terms of the original problem.\n\n\n\n\n\n\n\nExample 7.6: antihypertensive drug\n\n\n\n\n\nSuppose the reduction in systolic blood pressure (SBP) after taking a new antihypertensive follows a \\(N(10,4)\\) distribution. What is the probability that a randomly treated patient experiences a reduction of at least 15 mm Hg?\nLet \\(X\\) be the reduction in SBP. We want \\(P(X ≥ 15)\\).\n\nCompute the z‑score.\n\n\\[\nz=\\frac{15-10}{4}=1.25\n\\]\n\nFind the area to the right. Using the Normal Probability Calculator in JMP 18 Student Edition (Student → Applets → Distribution Calculator):\n\n\nFrom this calculator we have: \\[\nP(Z ≥ 1.25)=0.1056\n\\]\nUsing the Normal Calculator also allows us to find the probability without taking a z-score first. We just need to change the values of Mean and Std. Dev. in the applet.\n\n\nInterpretation. About 10.6% of patients have a reduction of at least 15 mm Hg.\n\n\n\n\n\n\nWorking in JMP\nJMP’s distribution calculator makes these computations straightforward:\n\nOpen the distribution calculator. In JMP Pro 17 go to Add‑ins → Calculators → Distribution Calculator. Choose Normal from the list of distributions.\nEnter parameters. Enter the mean and standard deviation (e.g., 10 and 4) and specify whether you want the area Left, Right or Between two values. For a “greater than” probability like \\(P(X ≥ 15)\\), choose Right and enter 15. JMP will display the area to the right.\nVisual check. The calculator shows a graph of the normal curve with the relevant region shaded. Use this to verify that you selected the correct tail or interval.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nz‑score\nThe number of standard deviations a value \\(x\\) is from the mean: \\(z=(x-\\mu)/\\sigma\\). Converting to z‑scores allows probabilities from any normal distribution to be found using the standard normal distribution.\n\n\n\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nCholesterol reductions after a dietary intervention follow a \\(N(20,5)\\) distribution. What is the probability that a randomly selected participant’s reduction is less than 12 mg/dL? Show the z‑score and compute the probability.\nSerum calcium levels in a normal population have mean 9.5 mg/dL and standard deviation 0.4 mg/dL. What proportion of individuals have calcium levels between 9.1 and 9.9 mg/dL? Sketch the problem and find the probability.\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nCompute \\(z=(12-20)/5=-1.6\\). From a standard normal table, \\(P(Z ≤ -1.6)\\approx0.0548\\). Therefore \\(P(X ≤ 12)=0.0548\\).\nFirst convert the endpoints to z‑scores: \\(z_1=(9.1-9.5)/0.4=-1.0\\) and \\(z_2=(9.9-9.5)/0.4=1.0\\). Using the calculator, we have \\(P(-1\\le Z \\le 1)=0.6827\\)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "07.html#sec-07_05",
    "href": "07.html#sec-07_05",
    "title": "7  Continuous Probability Distributions",
    "section": "7.5 Finding a Quantile for a Normal Distribution",
    "text": "7.5 Finding a Quantile for a Normal Distribution\n\n“Two things are infinite: the universe and human stupidity; and I’m not sure about the universe.” – Albert Einstein\n\nIn many applications we know a desired probability and wish to find the corresponding value of \\(x\\) such that \\(P(X ≤ x)=p\\). This value is called a quantile or percentile of the distribution. Mathematically, the quantile function \\(F^{-1}(p)\\) is the inverse of the cumulative distribution function. For a normal distribution, the quantile function returns the \\(x\\) value whose cumulative probability is \\(p\\).\n\nRelationship between the CDF and quantiles\nThe cumulative distribution function \\(F(x)\\) gives the probability that a random variable \\(X\\) is less than or equal to \\(x\\). The quantile function does the reverse: it takes a probability \\(p\\) and returns the threshold \\(x\\) such that \\(P(X ≤ x)=p\\). For example, the 0.5 quantile is the median. Because the normal CDF does not have a simple algebraic inverse, quantiles are typically obtained from tables or software.\nAt times, there are special quantiles that will show up in statistical methods that we will discuss later. We denote these as \\[\np = P(Z&gt; z_p)\n\\] In other words, \\(z_p\\) is the value of the standard normal distribution that will have \\(p\\) area to the right. For example, \\(z_{0.05}\\) is the value that has 0.05 area to the right.\n\n\n\n\n\n\n\n\n\nHere the value is \\[\nz_{0.05}=1.645\n\\]\n\n\nProcedure for finding quantiles\n\nSpecify the probability \\(p\\). Decide whether you want a lower tail (left‑side) quantile (e.g., the 5th percentile) or a two‑sided bound, or an upper tail area.\nFind the corresponding z‑score. For the standard normal distribution, tables and software can give you the quantile.\nTransform back to \\(x\\). If \\(X\\sim N(\\mu,\\sigma)\\), then \\(x=\\mu + z\\sigma\\) gives the desired quantile. Some software can find the quantile right from \\(x\\). In which case, there is no need to find the value for \\(z\\)-score first.\n\n\n\nExample:\n\n\n\n\n\n\nExample 7.7: therapeutic drug monitoring\n\n\n\n\n\nSuppose therapeutic blood levels of a drug after dosing follow a \\(N(50,10)\\) distribution. Physicians want to define the upper control limit beyond which a concentration is considered dangerously high. If they choose the 97.5th percentile as the cut‑off, what value should they use?\n\nFind the 97.5th percentile for \\(z\\). Using the special quantile location, this would be \\(z_{0.025}\\). Using the Normal Probability Calculator in JMP 18 Student Edition (Student → Applets → Distribution Calculator): \nTransform back. \\[\n\\begin{align*}\nx=&\\mu + z\\sigma\\\\\n=& 50 + 1.96\\times10\\\\\n=& 69.6\n\\end{align*}\n\\]\nInterpretation. Only 2.5% of patients are expected to have concentrations above 69.6 ng/mL. Concentrations exceeding this threshold may warrant intervention.\n\n\n\n\n\n\nWorking in JMP\nTo find quantiles in JMP:\n\nUse the distribution calculator. Open the distribution calculator and select Normal. Switch to the Percentile (or Inverse) mode. Enter the probability \\(p\\) (e.g., 0.975) and the distribution parameters \\(\\mu\\) and \\(\\sigma\\). JMP returns the corresponding \\(x\\) value.\nCheck with the CDF. You can verify your result by switching back to the probability mode and entering the value \\(x\\) you found. The calculator should return \\(p\\).\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nquantile (percentile)\nFor a continuous distribution with CDF \\(F\\), the value \\(x=F^{-1}(p)\\) such that \\(P(X ≤ x)=p\\).\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nBirth weights in a population follow a \\(N(3.5,0.4)\\) distribution. What is the weight corresponding to the 90th percentile? Show your calculation.\nAn assay has measurement errors that are \\(N(0,2)\\) distributed. What cut‑off defines the central 80% of the error distribution (i.e., the range from the 10th to the 90th percentile)?\nExplain the relationship between the CDF and the quantile function in your own words. Why do we need tables or software to find quantiles for the normal distribution?\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nFirst find the z‑score \\(z_{0.10}\\approx1.2816\\). Then \\(x=3.5 + 1.2816\\times0.4 = 4.0126\\) kg. Therefore, about 10% of babies weigh more than approximately 4.0 kg.\nThe central 80% corresponds to the interval between the 10th and 90th percentiles. From a standard normal table \\(z_{0.90}\\approx-1.2816\\) and \\(z_{0.10}\\approx1.2816\\). Multiply by \\(\\sigma=2\\) and add the mean 0: the interval is from \\(-1.2816×2≈-2.5632\\) to \\(1.2816×2≈2.5632\\).\nThe CDF gives the probability that a random variable is less than or equal to \\(x\\). The quantile function reverses this: given a probability \\(p\\), it returns the \\(x\\) for which the CDF equals \\(p\\). The normal CDF has no simple algebraic inverse, so we rely on tables or software to compute quantiles.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "08.html",
    "href": "08.html",
    "title": "8  Sampling Distributions",
    "section": "",
    "text": "8.1 Data, Probability and Sampling Distributions\nThe leap from simply describing data to making decisions about unknown populations depends on understanding sampling distributions. Up to this point, we have focused on two main tasks:\nStatistical inference—confidence intervals, hypothesis tests, and many modeling tools—requires one more layer of thinking. Instead of asking only “What does this sample look like?”, we must ask:\nThat question leads us directly to the idea of a sampling distribution.\nBefore developing that concept, it is essential to clearly distinguish three different kinds of distributions that appear throughout statistics. Although they are related, they answer very different questions.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "08.html#sec-08_01",
    "href": "08.html#sec-08_01",
    "title": "8  Sampling Distributions",
    "section": "",
    "text": "“While nothing is more uncertain than a single life, nothing is more certain than the average duration of a thousand lives.” – Elizur Wright\n\n\n\ndescribing what we observe in a single dataset, and\nmodeling randomness using probability distributions.\n\n\n\nHow would this statistic behave if we repeated the sampling process over and over?\n\n\n\n\nThree distributions you will encounter\n\nThe data distribution\nThe data distribution is the distribution of the raw observations in a single sample. It is what you actually see when you collect data.\n\nIt is empirical (based on observed values).\nIt changes from sample to sample.\nIt is typically displayed with a histogram, dotplot, or boxplot.\n\nExample. If you measure the blood pressures of 50 patients and plot a histogram of those 50 values, you are looking at the data distribution.\nThis distribution answers the descriptive question:\n\nWhat does this particular sample look like?\n\n\n\nThe probability distribution\nA probability distribution is a theoretical model for how individual values behave in the population.\n\nIt describes the long-run behavior of a random variable.\nIt is defined mathematically (for example, normal, binomial, or uniform).\nIt represents our assumptions about the population.\n\nYou can think of it as the data-generating mechanism for individual observations.\nExample. We might assume adult systolic blood pressure in a population follows approximately a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nThis distribution answers the modeling question:\n\nHow do individual observations vary in the population?\n\n\n\nThe sampling distribution\nThe sampling distribution is the distribution of a statistic (such as a sample mean, sample median, or sample proportion) computed from all possible samples of a fixed size \\(n\\).\nKey features:\n\nIt is also theoretical.\nIt concerns statistics, not raw data.\nIt describes the variability caused by the sampling process itself.\n\nFormally: The sampling distribution of a statistic is the probability distribution of all possible values of that statistic when all possible samples of size \\(n\\) are drawn from the population.\nThis distribution answers the inferential question:\n\nHow much would our statistic vary if we repeated the study many times?\n\nThis is the critical bridge to statistical inference.\n\n\nWhy the distinction matters\nThese three distributions live at different levels:\n\nData distribution → variability among individual observations in one sample\nProbability distribution → theoretical model for individual observations\nSampling distribution → variability of a statistic across repeated samples\n\nStudents often confuse the last two. A helpful rule of thumb is:\nProbability distributions describe individuals.\nSampling distributions describe statistics.\n\n\n\n\n\n\nExample 8.1: Salamander lengths\n\n\n\n\n\nImagine an ecologist studying the lengths of salamanders in a particular swamp.\nPopulation (conceptual starting point). Suppose the true salamander lengths in the swamp follow a right-skewed probability distribution: most animals are small, but a few grow unusually long. This underlying model is the probability distribution of salamander length.\nSingle field study. The ecologist randomly captures, measures, and releases 20 salamanders. The histogram of those 20 measured lengths is the data distribution. If she repeated the fieldwork tomorrow, she would likely get a slightly different histogram.\nRepeated sampling thought experiment. Now imagine she repeats the same 20-salamander study hundreds or thousands of times, each time computing the sample mean length. If we plotted a histogram of all those sample means, that histogram would be the sampling distribution of the sample mean.\nNotice the shift in focus:\n\nThe data distribution describes individual salamanders.\nThe sampling distribution describes the sample mean across studies.\n\n\n\n\n\n\n\nStatistics versus parameters\nA clear understanding of the difference between parameters and statistics is essential for statistical inference. Much of what we do in statistics revolves around using information from a sample to learn about a larger population we cannot fully observe.\nA parameter is a numerical summary that describes an entire population. It is a fixed but usually unknown quantity. Common population parameters include:\n\nthe population mean, denoted \\(\\mu\\),\nthe population proportion, denoted \\(p\\),\nthe population standard deviation, denoted \\(\\sigma\\).\n\nBecause populations are often very large—or even conceptual—it is usually impractical or impossible to measure every individual. As a result, the true parameter value typically remains unknown.\nIn contrast, a statistic is a numerical summary computed from a sample. Statistics are observable and calculable from data we actually collect. Common examples include:\n\nthe sample mean, denoted \\(\\bar{x}\\),\nthe sample standard deviation, denoted \\(s\\).\n\nUnlike parameters, statistics vary from sample to sample because different random samples produce different values.\nThe central goal of statistical inference is to use statistics to estimate parameters. We hope that a well-chosen statistic will be close to the corresponding population parameter, but because of sampling variability, it will not match exactly every time.\nThis is where the sampling distribution plays a crucial role. The sampling distribution describes how a statistic behaves across repeated random samples of the same size. By studying this distribution, we can answer important questions such as:\n\nHow much does the statistic typically vary?\nIs the statistic usually close to the parameter?\nHow uncertain is our estimate?\n\nFor example, suppose the true mean blood pressure of all adults in Waco is \\(\\mu\\). We cannot measure everyone, so we take a random sample of 50 adults and compute the sample mean \\(\\bar{x}\\). If we repeatedly took new samples of 50 adults and recomputed \\(\\bar{x}\\) each time, the resulting values would form the sampling distribution of the sample mean.\nThe spread of this sampling distribution tells us how precisely \\(\\bar{x}\\) estimates \\(\\mu\\). A narrow sampling distribution indicates that the statistic tends to be close to the parameter (high precision), while a wide sampling distribution indicates more sampling variability (lower precision).\nA useful way to think about the relationship is:\n\nThe parameter is the fixed target.\nThe statistic is our estimate from one sample.\nThe sampling distribution describes how the estimate would fluctuate if we repeated the sampling process many times.\n\n\n\n\n\n\n\nExample 8.2: Sampling distribution from a small population\n\n\n\n\n\nSuppose we had a small population of 20 US adults. Furthermore, suppose we want to estimate the proportion of this population that are left eye dominate. Below are the values for this population1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL\nL\nR\nR\nL\nL\nL\nL\nR\nL\nR\nL\nL\nR\nR\nR\nR\nR\nR\nR\n\n\n\n\n\nOur goal is to estimate the proportion of this population who are left eye dominate. Clearly, we could easily determine this proportion by looking at this small population. By doing so, we see that the proportion that are left eye dominate is \\[\n\\begin{align*}\np &= \\frac{9}{20}\\\\\n&=0.45\n\\end{align*}\n\\] Suppose we don’t know this proportion and the only thing we can do is randomly sample of size of 5 from this population. Below is one such sample.\n\n\n\n\n\nL\nR\nR\nL\nR\n\n\n\n\n\nFrom this sample, we estimate the proportion to be \\[\n\\begin{align*}\n\\hat{p} &= \\frac{2}{5}\\\\\n&= 0.4\n\\end{align*}\n\\]\nThis is just one such sample. There are many more random samples of size 5 from this population of 20 that I could have gotten. In fact, there are \\[\n\\binom{20}{5} = 15{,}504\n\\] possible samples of size 5 from this population. Suppose we did all of these samples and each time calculated \\(\\hat{p}\\). Below is a histogram of all of these \\(\\hat{p}\\)’s.\n\n\n\n\n\n\n\n\n\nThe histogram above shows the sampling distribution of \\(\\hat{p}\\) for a sample of size 5 from a population of size 20. For most practical applications, the population is much bigger than 20. It is not uncommon to have population sizes in the tens of thousands or even millions. Even if we kept the population size relatively low, such as 1000, the number of possible samples of size 5 become massive. \\[\n\\begin{align*}\n\\binom{1000}{5} = 8{,}250{,}291{,}250{,}200\n\\end{align*}\n\\]\nEven if we wanted to examine all of the possible samples (if the population was known) of size 5, it would be unfeasible even with a computer.\nFortunately, we have some results to help us determine what these distributions look like without having to examine all of the possible samples.\n\n\n\n\n\nWorking in JMP\nJMP can simulate sampling distributions without requiring you to know R. To explore the sampling distribution of a mean:\n\nUse Help → Sample Data Library to load a dataset (for example, “Body Measurements”). Then choose Analyze → Distribution and assign your variable to Y to visualize the data distribution.\nTo simulate a sampling distribution, go to Graph Builder and use Bootstrap from the red triangle menu. Specify your statistic (mean, median or proportion) and the number of bootstrap samples. JMP will draw many samples with replacement from your data and display the distribution of the chosen statistic. This bootstrap distribution approximates the sampling distribution we would get by taking many independent samples from the population.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nData distribution\nThe distribution of the observed values in a single sample.\n\n\nProbability distribution\nA theoretical model describing how a variable behaves in the population.\n\n\nSampling distribution\nThe probability distribution of a statistic computed from all possible samples of a fixed size.\n\n\n\n\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nExplain, in your own words, the difference between a data distribution and a sampling distribution. Why is the latter crucial for inference?\nWhat is the meaning of the phrase “the sampling distribution is a theoretical idea—we do not actually build it”?\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nData vs. sampling distribution. A data distribution reflects the raw measurements from one sample. A sampling distribution reflects how a summary statistic (such as a mean or proportion) would vary if we repeatedly took new samples of the same size. The sampling distribution is crucial because it tells us how much our statistic is expected to fluctuate around the true parameter, and thus forms the basis for standard errors and confidence intervals.\nWhy theoretical? The number of possible samples of size \\(n\\) from a population is enormous, so we cannot literally take all of them. The sampling distribution therefore exists as a theoretical construct.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "08.html#sec-08_02",
    "href": "08.html#sec-08_02",
    "title": "8  Sampling Distributions",
    "section": "8.2 Sampling Distribution of the Sample Proportion",
    "text": "8.2 Sampling Distribution of the Sample Proportion\n\n“Math is the logic of certainty; statistics is the logic of uncertainty.” - Joe Blizstein\n\nMany real-world studies involve categorical outcomes rather than numerical measurements. For example, in business, we could have\n\nWhether a customer makes a purchase (yes/no),\nWhether a loan applicant defaults (default/no default),\nWhether a marketing email is opened (opened/not opened),\nWhether a customer renews a subscription (renew/does not renew).\n\nIn each of these settings, the outcome for an individual unit falls into one of two categories. Such variables are often called binary variables.\n\nThe sample proportion\nWhen outcomes are binary, a natural summary statistic is the sample proportion, denoted \\(\\hat{p}\\).\nSuppose a company sends a promotional email to \\(n\\) customers and records how many open it. If \\(X\\) customers open the email, the sample proportion is\n\\[\n\\hat{p} = \\frac{X}{n}.\n\\]\nThis quantity estimates the population proportion \\(p\\), which represents the true (but unknown) probability that a randomly selected customer would open the email.\nBecause \\(\\hat{p}\\) is computed from a sample, it will vary from sample to sample. If the company ran the same email campaign many times with different randomly selected customers, the observed open rate would not be identical each time.\n\n\nThe sampling distribution of \\(\\hat{p}\\)\nThe sampling distribution of \\(\\hat{p}\\) describes how the sample proportion behaves across repeated random samples of the same size \\(n\\).\nBefore collecting data, \\(\\hat{p}\\) is a random variable. Its value depends on which customers happen to be included in the sample. Some samples may yield a high open rate; others may yield a lower one.\nIf we repeatedly:\n\nSelect a random sample of \\(n\\) customers,\nCompute \\(\\hat{p}\\) for each sample,\nPlot all those values,\n\nthe resulting distribution would be the sampling distribution of the sample proportion.\n\n\nProperties of the sampling distribution\nWhen the outcomes in the population are independent and the population proportion is \\(p\\), the sampling distribution of \\(\\hat{p}\\) has three key properties:\n\nCenter. The mean (or expected value) of \\(\\hat{p}\\) is the true population proportion. In other words, \\(E(\\hat{p}) = p\\).\n\nSpread. The standard deviation of \\(\\hat{p}\\) is \\[\n\\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}}\n\\]\nThis formula arises because the variance of a binomial random variable with parameters \\(n\\) and \\(p\\) is \\(np(1-p)\\), and dividing by \\(n^2\\) converts the count to a proportion. The standard deviation shrinks as the sample size increases, meaning larger samples give more precise estimates.\nShape. Under mild conditions (in particular, when the expected numbers of successes and failures both exceed about 15), the sampling distribution of \\(\\hat{p}\\) is approximately normal. Thus, for large enough \\(n\\) we can use a Normal model to approximate probabilities involving \\(\\hat{p}\\).\n\n\n\n\n\n\n\nExample 8.3: Prevalence of blood type O\n\n\n\n\n\nConsider a large population of blood donors in which the true proportion with type O blood is 45%. Suppose we randomly sample \\(n=50\\) donors and record whether each has type O blood. The sample proportion of type O donors is \\(\\hat{p} = x/n\\), where \\(x\\) is the number of type O donors. Because \\(x\\) follows a binomial distribution with parameters \\((n,p) = (50,0.45)\\), we know that \\[\nE(\\hat{p}) = 0.45\n\\] and \\[\n\\sigma_{\\hat{p}} = \\sqrt{\\frac{0.45\\times 0.55}{50}} \\approx 0.070\n\\]\nTo see the sampling distribution in action, we can simulate many samples and plot their proportions. Below is a histogram of \\(\\hat{p}\\) for 10,000 samples.\n\n\n\n\n\n\n\n\n\nThe histogram of the simulated proportions (blue bars) lines up closely with the red Normal curve predicted by the theory. Most sample proportions fall within roughly two standard errors (about ±0.14) of the true proportion 0.45.\n\n\n\n\n\nConditions for the normal approximation\nThe Normal approximation to the sampling distribution of the sample proportion \\(\\hat{p}\\) is extremely useful, but it does not always work well. A common rule of thumb is that both\n\\[\nnp \\ge 15 \\quad \\text{and} \\quad n(1-p) \\ge 15.\n\\]\nHere:\n\n\\(np\\) is the expected number of successes in the sample,\n\\(n(1-p)\\) is the expected number of failures.\n\nThe sampling distribution of \\(\\hat{p}\\) is based on the binomial model. When the sample size is small or when the true proportion \\(p\\) is very close to 0 or 1, the binomial distribution is skewed, not symmetric. In such cases, the bell-shaped Normal curve is a poor approximation.\nThe conditions \\(np \\ge 15\\) and \\(n(1-p) \\ge 15\\) ensure that:\n\nthe distribution of the number of successes is reasonably symmetric,\nand there are enough observations in both categories,\n\nWhen these conditions hold, the sampling distribution of \\(\\hat{p}\\) is well-approximated by a Normal distribution.\nIf either expected count is too small, several problems arise:\n\nThe sampling distribution becomes skewed, often strongly.\nThe Normal approximation may give inaccurate probabilities.\n\nFor example, if a company studies a rare event (say \\(p=0.01\\)) with only \\(n=50\\) customers, then\n\\[\nnp = 0.5,\n\\]\nwhich is far below 15. The distribution of \\(\\hat{p}\\) in this case is highly right-skewed, and the Normal model performs poorly.\n\nWhat to do when the condition is not met\nWhen either \\(np &lt; 15\\) or \\(n(1-p) &lt; 15\\), it is safer to use methods that do not rely on the Normal approximation, such as:\n\nExact binomial calculations, which use the true discrete distribution, or\nBootstrap methods (Section 8.4), which approximate the sampling distribution through resampling.\n\nThese approaches better capture skewness and discreteness when sample sizes are small or when proportions are extreme.\nThe Normal approximation for \\(\\hat{p}\\) works well only when there are enough expected successes and failures. Always check the conditions before applying Normal-based inference. When in doubt, especially with small samples or rare events, prefer exact or bootstrap methods to avoid misleading conclusions.\n\n\n\nWorking in JMP\nTo explore sampling distributions for proportions in JMP:\n\nUse Analyze → Distribution on a binary variable (coded 1 for success and 0 for failure) to see the data distribution.\nUse Graph Builder with the Bootstrap option to resample your data with replacement. Specify the statistic as proportion of successes and the number of bootstrap samples. JMP will display the bootstrap distribution, which closely approximates the theoretical sampling distribution when the sample is random and unbiased.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nPopulation proportion \\(p\\)\nThe true fraction of individuals in the population with a certain characteristic.\n\n\nSample proportion \\(\\hat{p}\\)\nThe fraction of sampled individuals with the characteristic; an estimator of \\(p\\).\n\n\nExpected value of \\(\\hat{p}\\)\nThe expected value of the sampling distribution of \\(\\hat{p}\\), equal to \\(p\\).\n\n\nStandard deviation of \\(\\hat{p}\\)\nThe standard deviation of the sampling distribution of \\(\\hat{p}\\), equal to \\(\\sqrt{p(1-p)/n}\\).\n\n\nNormal approximation\nFor large \\(n\\) with \\(np\\ge15\\) and \\(n(1-p)\\ge15\\), the sampling distribution of \\(\\hat{p}\\) is approximately normal.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nExplain why the sampling distribution of \\(\\hat{p}\\) has mean equal to the population proportion. What would it mean if the mean of \\(\\hat{p}\\) were systematically above or below \\(p\\)?\nSuppose the true prevalence of a rare mutation is 1%. If you sample \\(n=100\\) individuals, will the sampling distribution of \\(\\hat{p}\\) be well approximated by a Normal distribution? Why or why not? How might you proceed instead?\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nUnbiasedness. If we took infinitely many random samples and averaged the sample proportions, we would recover the true population proportion. That is precisely what the expected value of \\(\\hat{p}\\) tells us: \\(E(\\hat{p})=p\\). If the mean of \\(\\hat{p}\\) were consistently above \\(p\\), our estimator would be biased, systematically overestimating the true proportion.\nRare mutation. When \\(p\\) is very small (0.01) and \\(n=100\\), the expected number of successes is \\(np=1\\) and the expected number of failures is 99. Because \\(np&lt;15\\), the sampling distribution of \\(\\hat{p}\\) is highly skewed and the normal approximation is poor. A better approach is to use the exact binomial distribution to compute probabilities or to use a bootstrap to approximate the sampling distribution (see Section 8.4).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "08.html#sec-08_03",
    "href": "08.html#sec-08_03",
    "title": "8  Sampling Distributions",
    "section": "8.3 Sampling Distribution of the Sample Mean",
    "text": "8.3 Sampling Distribution of the Sample Mean\n\n“The Scientist must set in order. Science is built up with facts, as a house is with stones. But a collection of facts is no more a science than a heap of stones is a house.” - Henri Poincare\n\nThe sample mean \\(\\bar{x}\\) is the workhorse of quantitative inference. Whenever we measure a quantitative trait—such as revenue per customer, delivery time, exam score, or daily sales—we often reduce the data to a single summary: the average. This simple statistic captures the central tendency of the sample and serves as our primary estimate of the unknown population mean \\(\\mu\\).\nBut for inference, computing \\(\\bar{x}\\) once is not enough. To draw conclusions about the population, we must understand how \\(\\bar{x}\\) behaves across repeated samples. In other words, we must study the sampling distribution of the sample mean.\nBecause samples are random, the value of \\(\\bar{x}\\) will vary from sample to sample. If we repeatedly:\n\nDraw a random sample of size \\(n\\),\nCompute the sample mean each time,\nPlot all those means,\n\nthe resulting distribution is the sampling distribution of \\(\\bar{x}\\).\nThis distribution tells us:\n\nhow close \\(\\bar{x}\\) tends to be to \\(\\mu\\),\nhow much sampling variability to expect,\nand how precise our estimate is likely to be.\n\nThese ideas are the foundation of confidence intervals, hypothesis tests, and many statistical models.\n\nProperties of the sampling distribution\nUnder mild and widely satisfied conditions, the sampling distribution of \\(\\bar{x}\\) has three powerful properties. The first two concern the mean and standard deviation of \\(\\bar{x}\\).\n\n1. The mean of \\(\\bar{x}\\) equals the population mean\nIf the population has mean \\(\\mu\\), then\n\\[\nE(\\bar{x}) = \\mu.\n\\]\n\n\n2. The variability of \\(\\bar{x}\\) shrinks with sample size\nThe standard deviation of the sampling distribution of \\(\\bar{x}\\) is\n\\[\n\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}},\n\\]\nwhere \\(\\sigma\\) is the population standard deviation.\nThis quantity is called the standard error of the mean. It shows a crucial fact:\n\nLarger samples produce more precise estimates.\n\nBecause of the \\(\\sqrt{n}\\) in the denominator, the variability of \\(\\bar{x}\\) decreases as sample size increases—but with diminishing returns. Doubling the sample size does not cut the error in half; it reduces it by a factor of \\(\\sqrt{2}\\).\n\n\n\nThe Central Limit Theorem\nThe third property involves the shape of the sampling distribution of \\(\\bar{x}\\). The Central Limit Theorem (CLT) is one of the pillars of statistics. It states that when we randomly sample from any population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the sampling distribution of \\(\\bar{x}\\) becomes approximately normal as the sample size \\(n\\) grows. In symbols, \\[\n\\bar{X} \\overset{\\cdot}{\\sim} N\\bigl(\\mu,\\, \\sigma/\\sqrt{n}\\bigr)\n\\] for sufficiently large \\(n\\). Note that the dot (\\(\\cdot\\)) above \\(\\sim\\) means “approximately distributed as”.\nThe beauty of the CLT is that it does not require the underlying population to be normal; even strongly skewed or irregular distributions yield approximately normal sample means when \\(n\\) is large enough.\n\n\n\n\n\n\nExample 8.4: Simulated Enzyme Activities\n\n\n\n\n\nEnzyme activity measurements often follow a skewed distribution because they cannot be negative but can have long right tails. Suppose the true activity in a population of cells follows an exponential distribution. We take repeated random samples of different sizes and compute the sample mean for each. The following plot shows 10,000 simulated sample means for \\(n=5\\), \\(n=20\\) and \\(n=50\\) to illustrate how the distribution of \\(\\bar{x}\\) evolves:\n\n\n\n\n\n\n\n\n\nThe three panels show that for very small samples (\\(n=5\\)) the sampling distribution of \\(\\bar{x}\\) still retains some skewness. For moderate samples (\\(n=20\\)) the distribution looks more bell‑shaped, and by \\(n=50\\) it is nearly indistinguishable from the Normal curve (red line). This behavior is exactly what the CLT predicts.\n\n\n\n\n\nPractical considerations\nThe CLT justifies using normal‑based methods for many statistics, but it is not a panacea. Large samples are not always attainable. Sometimes cost, difficulty or the preciousness of biological material limits the sample size. In such cases the sampling distribution of \\(\\bar{x}\\) may be far from normal, especially for very skewed or heavy‑tailed populations. Diagnostic plots and simulation can help you gauge whether normal approximations are reasonable.\nIf the data is not available, then the rule-of-thumb of \\(n\\ge 30\\) is adequate in most situations to determine if the sample size is large enough.\n\n\nWorking in JMP\nTo explore the sampling distribution of the mean in JMP:\n\nUse Analyze → Distribution to visualise your quantitative data and estimate the population standard deviation.\nChoose Analyze → Resampling and select Bootstrap. Specify the statistic as the mean and set the number of resamples. JMP will generate a bootstrap sampling distribution of \\(\\bar{x}\\), plot it and report the standard error. You can compare the bootstrap distribution to a Normal distribution with mean equal to the observed \\(\\bar{x}\\) and standard deviation equal to the bootstrap standard error.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nStandard deviation of \\(\\bar{x}\\)\nThe standard deviation of the sampling distribution of \\(\\bar{x}\\), equal to \\(\\sigma/\\sqrt{n}\\).\n\n\nCentral Limit Theorem (CLT)\nStates that for large \\(n\\), the sampling distribution of the sample mean is approximately normal with mean \\(\\mu\\) and standard deviation \\(\\sigma/\\sqrt{n}\\).\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nA laboratory measures the enzyme activity of 10 randomly selected yeast cultures. The population distribution is known to be highly skewed with mean 50 units and standard deviation 20 units. Without doing any calculations, would you expect the sample mean to follow a Normal distribution? Explain your reasoning.\nA nutritionist samples 64 adults and measures their daily vitamin D intake. The population mean intake is 600 IU with standard deviation 200 IU. What is the mean and standard deviation of the sampling distribution of \\(\\bar{x}\\)? If the intake distribution is skewed, is the Normal approximation still reasonable? Why or why not?\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nSmall, skewed samples. With a sample size of 10 and a highly skewed population, the sampling distribution of \\(\\bar{x}\\) will retain noticeable skewness. The Central Limit Theorem requires larger \\(n\\) before the distribution of the sample mean becomes approximately normal, so caution is warranted when applying Normal approximations.\nVitamin D intake. The sampling distribution has mean \\(\\mu = 600\\) and standard deviation \\(200/\\sqrt{64} = 25\\). Because \\(n=64\\) is reasonably large, the CLT suggests that the sample mean will be approximately normal even if the individual intakes are skewed. Therefore the Normal approximation should be adequate.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "08.html#sec-08_04",
    "href": "08.html#sec-08_04",
    "title": "8  Sampling Distributions",
    "section": "8.4 Bootstrap Sampling Distribution",
    "text": "8.4 Bootstrap Sampling Distribution\n\n“It is the mark of a truly intelligent person to be moved by statistics.” -George Bernard Shaw\n\nSometimes we cannot rely on tidy formulas or the Central Limit Theorem (CLT) to determine the sampling distribution of a statistic. The CLT works beautifully for sample means under broad conditions, but many practical situations fall outside its comfort zone. For example:\n\nThe statistic of interest may have a complicated distribution (such as the median, a trimmed mean, a percentile, or a regression coefficient).\nThe sample size may be too small for Normal approximations to be reliable.\nThe population distribution may be strongly skewed or heavy-tailed.\nThe standard error formula may be unknown or difficult to derive.\n\nIn these settings, classical theoretical methods either become inaccurate or require mathematics that is impractical to work out by hand. Bootstrapping provides a powerful, data-driven alternative.\nBootstrapping is a resampling method that approximates the sampling distribution of a statistic by using the data we already have. Instead of imagining all possible samples from the population (which we cannot see), we treat the observed sample as a stand-in for the population and repeatedly resample from it.\nThe name comes from the phrase “pulling yourself up by your bootstraps”—we use the sample to learn about its own variability.\nThe key insight is:\n\nIf the sample is representative of the population, then resampling from the sample mimics sampling from the population.\n\n\nHow bootstrapping works\nSuppose we have a sample of size \\(n\\) and a statistic of interest (say the median).\nA bootstrap procedure typically follows these steps:\n\nStart with the original sample of size \\(n\\).\nResample with replacement from the sample to create a bootstrap sample of size \\(n\\).\nCompute the statistic (e.g., the median) for this bootstrap sample.\nRepeat steps 2–3 many times (often thousands).\nExamine the distribution of the bootstrap statistics.\n\nThe resulting distribution is called the bootstrap distribution, and it serves as an approximation to the true sampling distribution of the statistic.\n\n\n\n\n\n\nExample 8.5: Median Tumor Size\n\n\n\n\n\nImagine a study measuring the diameters (in millimeters) of 25 tumors detected in a mammogram screening. The sample is small and the data are skewed; we want to estimate the sampling distribution of the median tumor size. A bootstrap approach provides the following:\n\n\n\n\n\n\n\n\n\nThe histogram shows the bootstrap distribution of the median tumor size. From this distribution we can compute a bootstrap standard deviation (the standard deviation of the bootstrap medians) and make inferences.\n\n\n\n\n\nWhy bootstrapping is useful\nBootstrapping is especially valuable when:\n\nthe sampling distribution is mathematically intractable,\nthe statistic is not well handled by the CLT,\nthe sample size is modest,\nor we want a flexible, computer-based solution.\n\nFrom the bootstrap distribution we can estimate:\n\nthe standard error of the statistic,\nconfidence intervals,\nbias in the estimator,\nand the overall shape of the sampling distribution.\n\nIn modern statistics, bootstrapping is widely used because it replaces difficult mathematics with computation.\n\n\nWhen to be cautious\nBootstrapping is powerful but not magic. It works best when:\n\nthe original sample is representative of the population,\nobservations are independent,\nand the sample size is not extremely small.\n\nIf the sample is biased or too tiny to reflect the population’s structure, the bootstrap will faithfully reproduce those problems.\n\n\nThe big picture\nClassical inference relies on theoretical sampling distributions derived from probability models. Bootstrapping takes a different approach:\n\nTheory-based inference: derive the sampling distribution mathematically.\nBootstrap inference: approximate the sampling distribution computationally.\n\nWhen formulas are unavailable or unreliable, bootstrapping provides a practical and remarkably effective way to understand the variability of complex statistics directly from the data.\n\n\nWorking in JMP\nJMP has built‑in bootstrap tools that make resampling easy:\n\nAfter running an analysis (for example Analyze → Fit Y by X for comparing two groups), click the red triangle menu (▸) and select Bootstrap. Choose the statistic you wish to bootstrap and the number of resamples. JMP will create a bootstrap distribution, display it and report standard errors and confidence intervals.\nFor custom statistics, use Tables → Bootstrap Data to generate bootstrap samples from your dataset. You can then analyse each bootstrap sample using your preferred platform and collect the statistic of interest.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nBootstrapping\nA resampling method that uses the observed data to approximate a sampling distribution by repeatedly sampling with replacement.\n\n\nBootstrap sample\nA sample of size \\(n\\) drawn with replacement from the original sample.\n\n\nBootstrap statistic\nThe value of the statistic computed on a bootstrap sample.\n\n\nBootstrap distribution\nThe distribution of many bootstrap statistics; an empirical approximation to the sampling distribution.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nWhy do we sample with replacement when constructing a bootstrap sample? What would go wrong if we sampled without replacement?\nCompare and contrast the bootstrap distribution with the theoretical sampling distribution. Under what circumstances do they coincide, and when might they differ?\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nReplacement is essential. Sampling with replacement allows each observation to appear multiple times—or not at all—in a bootstrap sample. This mimics the variability of drawing new samples from the population. Sampling without replacement would simply rearrange the data and fail to capture the variability inherent in new samples.\nBootstrap vs. theoretical. The bootstrap distribution approximates the theoretical sampling distribution when the sample is random and representative, and when the number of bootstrap resamples is large. For statistics with simple known sampling distributions (like means and proportions), the bootstrap will agree closely with theory. For statistics whose sampling distributions are complicated or unknown, the bootstrap provides a practical alternative but may differ from the true sampling distribution, especially when the sample size is very small or the sample is biased.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "08.html#footnotes",
    "href": "08.html#footnotes",
    "title": "8  Sampling Distributions",
    "section": "",
    "text": "Data was obtained from Introductory Statistical Methods classes. Here, we are treating these values as a population.↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  }
]