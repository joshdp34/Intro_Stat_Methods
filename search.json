[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 2381 Introductory Statistical Methods",
    "section": "",
    "text": "Introduction\nThese are the lecture notes for STA 2381 - Introductory Statistical Methods.\nPrerequisites: None\n\nCourse Description:\nParametric statistical methods. Topics range from descriptive statistics through regression and one-way analysis of variance. Applications are typically from biology and medicine. Computer data analysis is required.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01.html",
    "href": "01.html",
    "title": "1  Introduction to Statistics",
    "section": "",
    "text": "1.1 What is Statistics?\nGuiding question: How do we use data to make better decisions?\nWhen people say “trust the data,” it can sound like magic. It isn’t. Statistics is the discipline that helps us collect, organize, and interpret data so that our decisions are more principled and less guess-y. In this course, we’ll treat statistics as a practical toolkit you’ll use across science, business, health, and everyday life.\nThe basic premise is simple: if we describe the data clearly and account for uncertainty honestly, we can make better choices. That might mean deciding which treatment is more effective, which ad design leads to more clicks, or whether a new policy seems to be working.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "01.html#what-is-statistics",
    "href": "01.html#what-is-statistics",
    "title": "1  Introduction to Statistics",
    "section": "",
    "text": "“The non-scientist in the street probably has a clearer notion of physics, chemistry and biology than of statistics, regarding statisticians as numerical philatelists, mere collector of numbers.” - Stephen Senn, Dicing with Death: Chance, Risk and Health\n\n\n\n\n\nWhat is “statistics,” exactly?\nStatistics is the science of collecting, organizing, analyzing, and interpreting data to make decisions or draw conclusions. It’s not just about numbers—it’s about what those numbers tell us.\nIf Statistics concerns data, then we should define “data.” First, note that “data” is a plural word. “Datum” is singular, although it is common to hear someone refer to “data” in the singular. A “datum” is a piece of information or fact. So “data” is a collection of facts or information we collect. That could mean\n\nthe amount of profit a company makes,\nthe growth of plants under some conditions,\nor how many people voted in an election.\n\nA helpful way to organize the subject of Statistics is to distinguish two complementary activities:\n\nDescriptive statistics help us summarize and visualize what we observed—think graphs, tables, and numerical summaries. The goal is clarity.\nInferential statistics help us generalize from a sample to a broader group (or process) and quantify our uncertainty about that generalization. The goal is justified conclusions.\n\nEven in a short conversation about data, you’ll hear a few recurring ideas:\n\nA population is the full set of people, items, or occasions we care about (all Baylor first-years this fall, all batteries produced this week).\nA sample is the subset we actually observe.\nA parameter is a (usually unknown) number that describes a population (the true average battery life, for example).\nA statistic is a number we compute from a sample (the sample’s average battery life) that we use to learn about the parameter.\n\nWe’ll study these terms in more detail soon; for now, hold on to the big idea: we summarize what we see (description) and we reason beyond what we see (inference).\n\n\nWhy decisions need both description and inference\nSuppose a clinic tests a new flu-prevention program among 200 volunteer patients. A month later, 18% of the “usual care” group got the flu, compared to 12% of the “new program” group. Descriptively, the new program looks better. Inferentially, we ask: could this gap be due to chance? If we ran the study again with different patients, might the difference shrink or flip? Statistics gives us a way to quantify that uncertainty and decide what to do next.\nA similar story plays out in business A/B tests, manufacturing quality checks, and sports analytics. The descriptive picture tells us what happened in the data; inference tells us how strongly that evidence supports a decision.\n\n\nVariability, bias, and honest uncertainty\nTwo forces shape every data story:\n\nVariability is the natural fluctuation we see from case to case or study to study. Even fair coins produce streaks.\nBias is systematic deviation—our design, measurement, or selection method pushes results in a consistent direction.\n\nGood statistical practice aims to reduce bias and acknowledge variability. We’ll use design principles to minimize bias and inferential tools to express uncertainty honestly.\n\n\nHow we’ll work in this course (and in JMP)\nOur general workflow:\n\nStart with a clear question and name the observational units (what a single case is) and variables (what we record about each case).\nDecide how the data were or will be collected (survey, experiment, database pull).\nUse descriptive statistics and graphics to get oriented.\nBuild an inferential argument when you need to generalize or compare.\nCommunicate a conclusion in context—what it means, what it doesn’t, and what to do next.\n\nJMP Pro 17 note. In JMP, we’ll lean on Graph Builder, Distribution, and Fit Y by X for description; and on Analyze platforms (e.g., Fit Y by X, Fit Model) for inference. You’ll learn to read the output and connect it to the logic above.\n\n\nA first look at long-run regularity (illustration)\nTo make “variability vs. long-run behavior” concrete, here’s a quick simulation of coin flips. Early on, the proportion of heads jumps around. As the number of flips grows, the proportion tends to settle near 0.5. We’ll rely on this idea—randomness in the short run, stability in the long run—throughout the course.\n\n\n\n\n\n\n\n\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nStatistics\nThe discipline of learning from data to describe patterns and make decisions under uncertainty.\n\n\nData\nRecorded information about cases (people/items/occasions) used as evidence for questions of interest.\n\n\nDescriptive statistics\nMethods for summarizing and visualizing what was observed (tables, graphs, numerical summaries).\n\n\nInferential statistics\nMethods for generalizing from a sample to a population and quantifying uncertainty.\n\n\nPopulation\nThe full group or process we want to understand.\n\n\nSample\nThe subset we actually observe and analyze.\n\n\nParameter\nA (usually unknown) numerical characteristic of a population.\n\n\nStatistic\nA numerical summary computed from a sample, used to learn about a parameter.\n\n\nVariability\nNatural fluctuation in data from case to case or study to study.\n\n\nBias\nSystematic deviation caused by design, measurement, or selection issues.\n\n\nObservational unit\nThe “one thing” a single row in the data represents (a person, part, game, etc.).\n\n\n\n\n\nCheck your understanding\n\nIn your own words, how is describing data different from inferring from data? Give a short example for each.\nIdentify the population, sample, parameter, and statistic in this scenario: A battery company tests 60 batteries from today’s production line and finds an average life of 7.8 hours. The company wants to know the true average life of all batteries produced today.\nA streaming service tests two home-page designs on 5,000 visitors each. Version B produces a 0.6 percentage-point higher click-through rate than Version A. What questions would you ask before recommending the company switch to Version B?\nExplain the difference between variability and bias using a dartboard analogy.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nDescription vs. inference. Description summarizes what happened in the observed data (e.g., “The median wait time yesterday was 11 minutes.”). Inference uses the sample to say something about a broader group or process, with uncertainty (e.g., “We estimate the typical wait time for all days like yesterday is 11 minutes, with margin of error ±2 minutes.”).\nBattery scenario. Population: all batteries produced today. Sample: the 60 tested batteries. Parameter: the true mean life of all batteries produced today. Statistic: the sample mean of 7.8 hours.\nA/B test questions. How were visitors assigned to versions (randomly)? Were there differences in traffic sources or device types? Is the effect stable over time? What margin of error or confidence interval accompanies the difference? What outcome do we ultimately care about (sign-ups, retention), and does the change affect it?\nDartboard analogy. Variability: darts land around the bullseye but are spread out randomly. Bias: darts consistently land off to the lower-left—systematically shifted rather than centered.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "01.html#populations-and-samples",
    "href": "01.html#populations-and-samples",
    "title": "1  Introduction to Statistics",
    "section": "1.2 Populations and Samples",
    "text": "1.2 Populations and Samples\n\n“To understand God’s thoughts, we must study statistics, for these are the measures of his purpose.” - Florence Nightingale\n\nGuiding question: What’s the difference between a population and a sample?\nWhen you start any statistical investigation, the first two things to name are the population and the sample. The population is the full group or process you want to understand; the sample is the smaller set of cases you actually observe. Most of the time we can’t measure everyone or everything, so we sample wisely and then use statistics to bridge from the sample back to the population.\n\nDefining the population (clearly!)\nA population can be concrete (“all tires produced by Line A this week”) or conceptual (“all patients like these under similar conditions”). Two refinements are helpful:\n\nThe target population is the group you truly care about.\nThe accessible population is the group you can practically reach, often represented by a sampling frame—a list or mechanism from which you select the sample.\n\nClarity matters. If your target is “all Baylor first-year students this fall,” using a sampling frame of “students who attend welcome week” might miss commuters or students who arrived late. That gap can create bias if the missed students differ in a systematic way.\n\n\nWhat counts as a sample?\nA sample is the subset of the population you measure. We’ll study “how to sample” in Chapter 2; for now, focus on what a good sample does: it represents the population well enough that statistics computed from the sample are informative about the population.\nA special case is a census, where you attempt to measure every unit in the population. Censuses are rare in practice due to cost, time, and logistics—sampling is our workhorse.\n\n\nParameters and statistics (the bridge between the two)\nA parameter is a number that describes the population (usually unknown). A statistic is a number computed from a sample. We use statistics as estimates of parameters.\nBecause samples vary, statistics vary too. That natural fluctuation is sampling error. We’ll learn how to quantify it with margins of error and confidence intervals in later chapters.\n\n\nObservational units vs. sampling units\nTwo related terms often get mixed:\n\nThe observational unit is “one row of data” (one person, one battery, one game).\nThe sampling unit is “the thing you sample” (could be the same as the observational unit, but in cluster designs it might be a school or a household).\n\nBeing explicit about units guards against design mistakes and double-counting.\n\n\nWhy sampling works (and when it doesn’t)\nSampling works when your sample is representative of the population and your measurement is trustworthy. It struggles when parts of the population are systematically excluded (coverage problems), when participation differs by outcome (nonresponse), or when the way you select units is related to the outcome (selection effects). We’ll study these threats—and how to reduce them—next chapter.\n\n\nA quick illustration: samples differ, the goal doesn’t\nBelow is a simple simulation that shows how sample means wiggle from sample to sample even when we know the “true” population. As sample size grows, the wiggle shrinks.\n\n\n\n\n\n\n\n\n\nEven though each sample tells a slightly different story, they all tend to cluster around the true mean of 10. That’s the intuition behind using a statistic to learn about a parameter.\n\n\nConnecting to JMP Pro 17\nIn JMP you won’t “declare” the population, but you should always write it down in words before you analyze. Practically:\n\nUse Tables → Subset or randomization tools to create a sample when you have a large file standing in for a population.\nUse Analyze → Distribution to summarize your sample (means, proportions).\nKeep notes in the Data Table (Table panel → Notes) to record what your population and sampling frame are supposed to be.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nPopulation\nThe full group or process you want to understand.\n\n\nTarget population\nThe group you truly care about answering a question for.\n\n\nAccessible population\nThe portion of the target population you can practically reach.\n\n\nSampling frame\nThe list or mechanism from which the sample is drawn.\n\n\nSample\nThe subset of units you actually observe and measure.\n\n\nCensus\nAn attempt to measure every unit in the population.\n\n\nParameter\nA numerical characteristic of a population (e.g., \\(\\mu, p\\)).\n\n\nStatistic\nA numerical summary from a sample (e.g., \\(\\bar{x}, \\hat{p}\\)) used to estimate a parameter.\n\n\nSampling error\nNatural variation in a statistic from sample to sample.\n\n\nBias\nSystematic deviation caused by design, coverage, or selection issues.\n\n\nObservational unit\nThe “one case” a single row of data represents.\n\n\nSampling unit\nThe entity selected during sampling (may differ from the observational unit).\n\n\n\n\n\nCheck your understanding\n\nA battery plant tests 80 batteries from today’s production line and finds an average life of 7.8 hours. Identify the population, sample, parameter, and statistic.\nYour target is “all Baylor first-year students this fall.” You gather data from an email list of students who signed up for an early-interest program. What is the sampling frame? Name a potential source of bias.\nIn a household survey, you sample addresses, then interview every adult living there. What are the sampling units? What are the observational units?\nA marketing team uses comments on their Instagram post to judge product satisfaction among all customers. Explain why this may not represent the population.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nPopulation: all batteries produced today. Sample: the 80 tested batteries. Parameter: the true mean life of all batteries produced today. Statistic: the sample mean of 7.8 hours.\nSampling frame: students on the early-interest email list. Potential bias: students who didn’t sign up (e.g., commuters, late enrollees) may be under-represented, creating coverage/selection bias.\nSampling units: addresses (households) selected. Observational units: adults within each sampled household.\nInstagram commenters are a self-selected subset; satisfied or dissatisfied users may be more likely to comment, and customers not on Instagram are excluded—both threaten representativeness.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "01.html#variables-and-types-of-data",
    "href": "01.html#variables-and-types-of-data",
    "title": "1  Introduction to Statistics",
    "section": "1.3 Variables and Types of Data",
    "text": "1.3 Variables and Types of Data\n\n“Not everything that can be counted counts, and not everything that counts can be counted.” - Albert Einstein\n\nGuiding question: What kinds of data exist, and how do we recognize them?\nEvery data table is built from two ingredients: cases (the rows) and variables (the columns). A variable is any characteristic we record on each case. Choosing good analyses—and even drawing the right graph—depends on knowing what type of variable you’re working with.\n\nWhat is a variable?\nA variable can take different values across cases. “Resting heart rate,” “major,” “state of residence,” and “did the patient improve?” are all variables. Variables live in context: we should always be able to say what one row represents (the observational unit) and what each column means and how it was measured (the unit of measurement when relevant).\n\n\nThe two big families\nMost variables fall into two broad families. The names vary by textbook, but the ideas are stable.\n\nCategorical variables\nA categorical variable places each case into a group or label. Arithmetic on the labels doesn’t make sense.\n\nNominal: categories with no natural order (e.g., blood type, home state, device brand).\nOrdinal: categories with a meaningful order but uneven or unknown spacing (e.g., Likert ratings from “Strongly disagree” to “Strongly agree”; race finish places: 1st, 2nd, 3rd).\nBinary: exactly two categories (e.g., success/failure, disease/no disease).\n\n\n\n\n\n\n\nLikert Scale\n\n\n\n\n\nA Likert Scale is a common psychometric scale used in questionnaires to measure attitudes, opinions, or behaviors. It presents a statement and asks respondents to indicate their level of agreement or frequency on a symmetric, typically 5- or 7-point scale. The options range from one extreme (e.g., “Strongly Disagree”) to the opposite extreme (e.g., “Strongly Agree”), often including a neutral or middle point.\nExample: “How satisfied are you with our service?”\n\nStrongly Disagree\nDisagree\nNeutral\nAgree\nStrongly Agree\n\n\n\n\n\n\nQuantitative variables\nA quantitative variable records a number where arithmetic is meaningful.\n\nDiscrete: counts that jump in whole steps (e.g., number of ER visits, defects per unit).\n\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nIn math, the possible values of discrete data form what is known as countable set. This means the values form a collection (set) of values that can be put into a one-to-one correspondence with the natural numbers. In other words, a set is countable if you can list its elements in a sequence, even if the list is infinitely long.\nThere are two type of countable sets:\n\nFinite Countable Set\n\nThe set has a limited number of elements.\nExample: \\(\\{2, 4, 6, 8\\}\\) — there are exactly 4 elements.\n\nCountably Infinite Set\n\nThe set has infinitely many elements, but you can still list them in an ordered sequence.\nExample: The set of natural numbers \\(\\{1, 2, 3, 4, ...\\}\\)\nEven the set of all integers \\(\\{..., -2, -1, 0, 1, 2, ...\\}\\) is countable — you can reorder them as \\(0, 1, -1, 2, -2, 3, -3, ..\\) and still list them one by one.\n\n\n\n\n\n\nContinuous: measurements that, in principle, vary on a smooth scale (e.g., blood pressure, time to failure, height).\n\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nThe possible values of continuous data form an uncountable set. A set is uncountable if there’s no way to list all its elements in a sequence, even infinitely.\n\nExample: Real numbers between 0 and 1 — there are infinitely more of these than there are natural numbers.\nIn set notation, an example of an uncountable set is \\(\\{x: x\\ge 3\\}\\).\n\n\n\n\nTo decide if a variable is discrete or continuous, first think of an interval of possible values of the variable1. Can you count how many values are in that interval? If so, then it is discrete, if not, then it is continuous. In practice, we can simplify this further: counts are discrete, measurements are usually continuous.\n\n\n\nMeasurement scales you’ll hear about\nYou’ll occasionally see measurement scales—nominal, ordinal, interval, and ratio—used to describe variables.\n\nInterval: numeric scales where differences make sense but zero is arbitrary (°F, °C). Ratios aren’t meaningful (“40°F isn’t twice as hot as 20°F”).\nRatio: numeric scales with a true zero where ratios do make sense (Kelvin, length, mass, time). “10 minutes is twice 5 minutes” does make sense.\n\nYou don’t need to memorize the taxonomy; the practical takeaway is to be cautious about ratios on interval scales and to think about what operations make sense for each variable.\n\n\n“Identifier” and date/time variables\nSome columns are crucial for tracking rows but aren’t meant for analysis:\n\nIdentifier (ID) variables (e.g., student ID, order number, ZIP code) are labels, not quantities—even when they’re made of digits.\nDate/time variables carry timestamps that can be treated as either categorical (e.g., day of week) or quantitative (e.g., time since start). Be explicit about which role you intend.\n\n\n\nWhy types matter (graphs and analyses)\n\nCategorical → bar chart or pie chart; proportions/percents; chi-square-type tools later.\nQuantitative → dotplot, histogram, boxplot; means/medians/standard deviations; t-tools and regression later.\nOrdinal often behaves like categorical for display, but its order lets you use medians or ordered models.\nDiscrete vs. continuous matters for how the plot should look (bars separated vs. bins on a continuum) and for certain modeling choices.\n\n\n\nWorking in JMP Pro 17\nJMP keeps two concepts straight for each column:\n\nData Type (what the values are): Numeric, Character, or Date/Time.\nModeling Type (how you plan to analyze them): Continuous, Nominal, or Ordinal.\n\nDouble-click a column header (or right-click → Column Info) to set these. Typical mappings:\n\nCharacter + Nominal → categorical labels (e.g., “TX”, “CA”).\nNumeric + Continuous → quantitative measures (e.g., weight, time).\nNumeric + Nominal → numeric codes that are actually categories (e.g., 0/1 flags, ZIP codes, jersey numbers).\nNumeric + Ordinal → ordered categories encoded as 1–5 (Likert items).\n\nIf a graph or analysis looks odd in JMP, check these settings first—they control which menu options and displays you’ll see in Graph Builder and Analyze.\n\n\nA quick illustration (discrete vs. continuous)\nThe displays below look different because one variable is discrete (counts) and the other is continuous (measurements).\n\n\n\n\n\n\n\n\n\nIf you tried to “average” home states, you’d get nonsense; if you treated blood pressure as a category, you’d throw away useful information. Getting types right keeps your analysis honest and effective.\n\n\nCommon pitfalls (and quick fixes)\n\nDigits don’t guarantee “quantitative.” ZIP codes, part numbers, and jersey numbers are identifiers; set them to Character + Nominal in JMP.\nCollapsing a continuous variable into categories (“high/medium/low”) can simplify communication but often loses power. Keep the original too.\nTreating an ordinal scale as if equal steps are guaranteed can be misleading. Consider medians or nonparametric tools—or justify the approximation.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nCategorical variable\nPlaces a case into a group/label; arithmetic on labels is not meaningful.\n\n\nNominal\nCategorical with no natural order.\n\n\nOrdinal\nCategorical with a natural order but uneven/unknown spacing.\n\n\nBinary\nA categorical variable with exactly two categories.\n\n\nQuantitative variable\nNumeric values where arithmetic is meaningful.\n\n\nDiscrete\nQuantitative counts that change in whole steps.\n\n\nContinuous\nQuantitative measurements on a (nearly) continuous scale.\n\n\nIdentifier (ID)\nA label used to distinguish cases; not for numerical analysis.\n\n\nDate/time\nA timestamped variable that can be treated as categorical or quantitative depending on the question.\n\n\nMeasurement scale\nDescribes how numbers relate to the thing measured (nominal, ordinal, interval, ratio).\n\n\nInterval\nNumeric scale with arbitrary zero; differences meaningful, ratios not.\n\n\nRatio\nNumeric scale with a true zero; differences and ratios meaningful.\n\n\nUnit of measurement\nThe physical unit used to record a quantitative variable (e.g., mmHg, seconds).\n\n\n\n\n\nCheck your understanding\n\nClassify each variable as categorical nominal, categorical ordinal, binary, quantitative discrete, or quantitative continuous:\n\nNumber of missed classes this semester\nPain rating on a 0–10 scale\nWhether a chip passes final inspection\nZIP code\nBody temperature (°F)\n\nA researcher converts systolic blood pressure into “Low” (&lt;110), “Normal” (110–139), and “High” (≥140). Name one advantage and one drawback of this recoding.\nGive an example where a date/time variable should be treated as (a) categorical and (b) quantitative.\nFor each of the following, say whether ratios are meaningful and explain briefly:\n\nTemperatures in °C\nHours of weekly exercise\nIQ scores\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n(a)Quantitative discrete (counts). (b) Categorical ordinal (ordered scale with uneven spacing)—often treated as numeric for convenience, but it’s ordinal by design. (c) Binary (pass/fail). (d) Identifier (categorical nominal label; not a quantitative variable). (e) Quantitative continuous.\nAdvantage: simplifies communication and enables comparisons across broad groups. Drawback: throws away information; analyses lose power and can depend on arbitrary cutpoints.\n(a)Categorical: day of week when a call was received (Mon–Sun). (b) Quantitative: minutes since admission, time to recovery, or days from treatment to follow-up.\n(a)°C is an interval scale: ratios aren’t meaningful (20°C isn’t “twice as hot” as 10°C). (b) Hours of exercise is a ratio scale with a true zero: ratios are meaningful (4 hours is twice 2 hours). (c) IQ is typically treated as an interval scale: differences are interpretable; ratios are not.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "01.html#the-role-of-statistics-in-research",
    "href": "01.html#the-role-of-statistics-in-research",
    "title": "1  Introduction to Statistics",
    "section": "1.4 The Role of Statistics in Research",
    "text": "1.4 The Role of Statistics in Research\nGuiding question: How does the statistical process turn questions into answers?\nResearch isn’t just “run a test and see what comes out.” It’s a disciplined loop that starts with a clear research question, moves through careful study design and data collection, and ends with reasoned conclusions—always with honesty about uncertainty. Statistics is the glue in this loop: it helps you plan how to learn, summarizes what you saw, and quantifies what you can responsibly claim.\n\nFrom question to estimand to design\nEvery project should name three things up front:\n\nThe research question: a plain-language statement of what you want to learn (“Does the new tutoring program improve exam scores for Calculus I students?”).\nThe target parameter: the specific quantity in the population you want to know (e.g., the difference in mean exam scores between students who receive tutoring and those who don’t).\nThe study design: how you’ll collect data that speak to the target parameter (sampling plan, measures, and whether you’ll assign treatments).\n\nTwo broad design families appear everywhere:\n\nAn observational study records what already happens without assigning treatments. It’s great for describing associations but vulnerable to confounding (other factors moving with your variable of interest).\nAn experiment assigns a treatment and compares outcomes. With good randomization and control, experiments support stronger causal claims.\n\nYour choice should match the target parameter and the practical constraints.\n\n\nThe statistical process: a workable blueprint\nHere’s a simple, reusable workflow:\n\nSpecify the problem clearly. Name the population, observational units, and variables.\nDesign the study. Choose an observational or experimental approach, plan the sample, and anticipate sources of bias.\nCollect data with quality in mind. Strive for reliable measurement and good documentation.\nExplore first. Use exploratory data analysis (EDA)—graphs and summaries—to understand patterns, errors, and context before modeling.\nModel and infer. Pick a tool that matches the question and data type (comparison of means, relationships, predictions).\nQuantify uncertainty. Report estimates with confidence intervals and, when appropriate, p-values (confidence intervals and p-values will be covered in Chapters 9 and 10, respectively). Distinguish statistical significance from practical significance.\nDecide and communicate. Put results back in context: what do they mean, what they don’t, and what action (if any) follows.\nMake it reproducible. Ensure someone else could re-create your steps and numbers from the same data.\n\nYou won’t always march in a perfect line—often you loop back after EDA to refine the question or design. That’s good science.\n\n\nWorking in JMP Pro 17\nJMP is built for this process:\n\nDocument the plan and steps. Use File → New → Journal to create a running research notebook. Paste screenshots, notes, and output as you go.\nSave scripts to reproduce output. In most reports, click the red triangle ► Save Script → To Data Table (or To Journal). This stores a runnable recipe with the data.\nExplore first. Start with Graph → Graph Builder and Analyze → Distribution to profile variables and check data quality.\nFit models with assumptions in view. Use Analyze → Fit Y by X for two-variable comparisons and Analyze → Fit Model for multiple predictors. Residual and diagnostic tools are right there in the platform menus.\nShare and rerun. Bundle data, scripts, and notes with Projects or send a Journal so collaborators can reproduce your analysis.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nResearch question\nThe plain-language question your study seeks to answer.\n\n\nTarget parameter\nThe specific population quantity you aim to learn (e.g., mean difference, proportion).\n\n\nStudy design\nThe plan for collecting data (sampling, measurement, assignment) to answer the question.\n\n\nObservational study\nA design that observes existing groups without assigning treatments.\n\n\nExperiment\nA design that assigns treatments (often at random) and compares outcomes.\n\n\nTreatment\nThe condition or intervention applied in an experiment.\n\n\nConfounding\nA third factor related to both treatment and outcome that distorts comparisons.\n\n\nExploratory data analysis (EDA)\nEarly summaries/graphs used to understand data and spot issues.\n\n\nReproducibility\nThe ability for someone else to re-create your results from the same data and documented steps.\n\n\n\n\n\nCheck your understanding\n\nA hospital asks: “Does a new scheduling system reduce ER wait times compared to the current system?”\n\nState a suitable target parameter.\nName a design choice (observational vs. experimental) and one reason for your choice.\n\nA university compares GPA between students who use tutoring and those who don’t, with no assignment or randomization. Name a potential confounder and how an experiment would address it.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n(a)Target parameter: the mean difference in ER wait time (new minus current) for all eligible ER visits. (b) Design: a randomized experiment (e.g., randomize days or shifts to “new” vs. “current”), so groups differ only by scheduling system on average, improving causal interpretation.\nPotential confounder: prior academic preparation (e.g., incoming math placement). Students who seek tutoring might differ systematically. An experiment could randomly assign eligible students to tutoring or control (or randomize access), balancing confounders by design.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "01.html#the-statistical-process",
    "href": "01.html#the-statistical-process",
    "title": "1  Introduction to Statistics",
    "section": "1.5 The Statistical Process",
    "text": "1.5 The Statistical Process\n\n“An approximate answer to the right problem is worth a good deal more than an exact answer to an approximate problem.” -John Tukey\n\nGuiding question: Why does context matter as much as calculation?\nTwo analysts can compute the same number and reach very different conclusions—because numbers live inside stories. The context of your data—who and what were measured, when and where, and under what conditions—determines which comparisons are fair, which models fit, and what your results mean. In short: good statistics is not just calculation; it’s thoughtful interpretation anchored in how the data were produced.\n\nWhat “context” actually includes\nWhen we say “use context,” we mean four concrete ingredients:\n\nData context. What are the observational units? What do the variables represent, and what are their units of measurement?\nDesign context. How were cases selected or assigned? Is this an observational study or an experiment? Where could bias or confounding sneak in?\nMeasurement context. What is the precise operational definition of each variable? Are instruments calibrated? Is the measurement reliable and valid for the construct?\nAnalysis context. What assumptions do your methods make, and are they plausible here? What domain constraints or conventions matter?\n\nTogether, these pieces describe the data-generating process (DGP)—the mechanism by which the numbers in your table came to be. We make better choices when we reason about the DGP, not just the dataset.\n\n\nSame numbers, different stories: why stratification matters\nComparisons can flip when you ignore a meaningful background factor—a classic “wait, what?” known as Simpson’s paradox. Below is a toy illustration with a medical “success” outcome, two treatments (A, B), and a background factor “severity” with two levels (Easy/Hard). Within each severity group, A does slightly better. But because B is given more often to Easy cases, B looks better overall.\n\n\n\nSuccess rates by severity (within-group)\n\n\nseverity\ntreatment\nsuccesses\ntotal\nrate\n\n\n\n\nEasy\nA\n93\n100\n0.93\n\n\nEasy\nB\n828\n900\n0.92\n\n\nHard\nA\n657\n900\n0.73\n\n\nHard\nB\n72\n100\n0.72\n\n\n\n\n\n\nOverall success rates (ignoring severity)\n\n\ntreatment\nsuccesses\ntotal\nrate\n\n\n\n\nA\n750\n1000\n0.75\n\n\nB\n900\n1000\n0.90\n\n\n\n\n\nIgnoring context (severity) leads to a different recommendation than comparing like with like. The fix is simple in principle: compare within relevant strata or adjust for them in a model.\n\n\nA practical context checklist (before you compute)\nSlow down—ask and answer these, in writing:\n\nWho/what is a row? Name the observational unit.\nHow were data obtained? Sampling plan? Assignment? Time window? Inclusion/exclusion rules?\nWhat exactly was measured? Give operational definitions and units.\nWhich comparisons are fair? Identify potential confounders and plan to stratify or adjust.\nWhat assumptions will you need? Independence? Linearity? Constant variance? Distributional shape? (We will discuss these assumptions in later chapters)\nWhat will your claim be? Descriptive statement, association, or causal effect? Match your language to your design.\n\n\n\nWorking in JMP Pro 17: making context visible\nJMP gives you several context-friendly tools:\n\nDocument units and roles. Right-click a column → Column Info to set Units, Modeling Type (Nominal/Ordinal/Continuous), and notes. This prevents accidental “numeric” analysis of IDs.\nCompare like with like. Use Graph → Graph Builder with Group X/Y or Wrap to make side-by-side comparisons within levels of a context variable (e.g., severity, grade level).\nStratify or filter. Turn on Local Data Filter (from the report’s red triangle) to subset or facet analyses without touching the source table.\nAdjust in a model. In Analyze → Fit Model, include the context variable(s) as effects and consider an interaction to check for effect modification (i.e., whether the effect changes with context).\nUse “By” wisely. Many platforms allow a By variable to run the same analysis separately within each level—great for quick stratified summaries.\n\n\n\nLanguage that respects context\nBe precise. In observational data, prefer “associated with,” not “caused by.” Reserve causal language for designs (or assumptions) that justify it. Match conclusions to the population actually studied to protect external validity (generalizability), and avoid threats to internal validity (biases that distort the effect).\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nContext\nThe who/what/when/where/how surrounding the data that shapes fair comparisons and interpretation.\n\n\nData-generating process (DGP)\nThe mechanism or process by which the observed data were produced.\n\n\nStratification\nComparing groups within levels of a context variable to reduce confounding.\n\n\nSimpson’s paradox\nA reversal of an association when data are aggregated versus when compared within relevant strata.\n\n\n\n\n\nCheck your understanding\n\nA hospital reports that Treatment B has a higher overall recovery rate than Treatment A. After stratifying by initial severity, A has a higher recovery rate in each severity group. Name the phenomenon.\nA university analyzes GPA vs. tutoring usage from administrative records (no random assignment). What language should they use to describe the relationship? Name one likely confounder.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n(a)Simpson’s paradox.\nUse association language: “Tutoring use is associated with higher GPA,” not causal. Likely confounders include prior preparation (placement scores), motivation, or course load.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "01.html#footnotes",
    "href": "01.html#footnotes",
    "title": "1  Introduction to Statistics",
    "section": "",
    "text": "In real world applications, you are usually limited by how you measure. For instance, you may be measuring the length of insects and you measure to the nearest millimeter. This limitation should not play a role in determining continuous RVs. So in theory, insects could measure between 10 and 20 millimeters. You only measure in millimeters but an insect could be 10.1, 10.114, 10.675, 10.000004, etc. Since there are infinite number of values in 10 to 20 that insects could measure in theory, we say the length is continuous.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "02.html",
    "href": "02.html",
    "title": "2  Collecting Data",
    "section": "",
    "text": "2.1 Sampling Methods\nGuiding question: What makes a sample “representative”?\nA sample is just a subset of your population, but not every subset is equally useful. A representative sample matches the important characteristics of the population so well that if you analysed the sample’s data, you would reach the same conclusions you would have by measuring everyone. In other words, it’s a smaller group whose answers reflect those of the larger group. Designing studies that produce representative samples is the heart of statistics: it lets us reduce costs and time without sacrificing credibility.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "02.html#sec-sampling_methods",
    "href": "02.html#sec-sampling_methods",
    "title": "2  Collecting Data",
    "section": "",
    "text": "“And I knew exactly what to do. But in a much more real sense, I had no idea what to do.” - Michael Scott\n\n\n\n\nProbability vs. non‑probability sampling\nThere are two big approaches to sampling:\n\nIn probability sampling, you use random mechanisms so that every member of the population has a known chance of being selected. This ensures that any differences between the sample and the population are due to random sampling error, not researcher choice, and makes it possible to quantify uncertainty in estimates.\nIn non‑probability sampling, you choose participants based on convenience, volunteer responses, judgement or quotas. These methods are cheaper and often necessary for exploratory or qualitative work, but they make it hard to know whether the sample really reflects the population. Lack of a representative sample reduces the validity of conclusions and can introduce sampling bias.\n\nWhenever you want to generalize, probability sampling is the gold standard.\n\n\nSimple random sampling (SRS)\nIn a simple random sample (SRS), each individual in the population has an equal chance of being selected. You start with a complete sampling frame (a list of all units) and use a random number generator to pick your sample. In practice, SRS can be done with or without replacement; without replacement avoids picking the same unit twice. SRS is conceptually simple, but it requires a complete list and may be impractical when populations are spread out or access is difficult.\nExample: Suppose you want to survey 100 employees of a social media marketing company out of 1,000. You assign each employee a number from 1 to 1,000 and use a random number generator to select 100 numbers. Those employees become your sample. Because each employee had the same chance of selection, the sample is likely to be representative if the sample size is large enough.\n\n\nSystematic sampling\nSystematic sampling makes SRS easier by selecting every \\(k\\)-th unit from an ordered list. For example, pick a random starting point between 1 and 10, then choose every 10th person. If the list order has no hidden pattern, this approach approximates SRS while being cheaper to implement. However, if the ordering itself is related to the variable of interest (e.g., employees grouped by seniority), systematic sampling can inadvertently oversample or undersample certain groups.\nExample: All employees of a company are listed in alphabetical order. You randomly select a starting point among the first ten names—say, the 6th person—and then select every 10th person on the list (6, 16, 26, 36, …) until you have 100 employees. This produces a sample that is easy to implement but may be risky if the list order coincides with job role or seniority.\n\n\nStratified sampling\nIn stratified sampling, you first divide the population into subgroups (strata) based on an important characteristic (e.g., gender, age, region). You then sample randomly within each stratum in proportion to its size. Stratification ensures that each subgroup is properly represented and often reduces sampling error, especially when differences between strata are substantial. The trade‑off is increased complexity and cost, since you must track and sample separately within each stratum.\nExample: A company has 800 junior employees and 200 senior employees. To ensure the sample reflects the seniority balance, you sort the population into two strata and randomly select 80 junior employees and 20 senior employees. Your 100‑person sample preserves the 80/20 split of the population.\n\n\nCluster sampling\nCluster sampling also divides the population into groups, but here each cluster is (ideally) a mini‑population containing a mix of units. Instead of sampling individuals from every cluster, you randomly select a few clusters and include every individual within them or take a sample from them. This method is useful for large, geographically dispersed populations because it reduces travel or administrative costs. The downside is that if clusters differ substantially from one another, estimates may have higher variance than those from an SRS of the same size.\nExample: A company operates offices in 10 cities across the country, each with roughly the same number of employees. To conduct an employee survey, you randomly select 3 offices and survey everyone at those offices. This cuts down on travel but only works well if the offices are similar.\n\n\nNon‑probability methods\nSometimes you cannot (or choose not to) use random selection. Non‑probability methods are practical for exploratory research or hard‑to‑reach groups, but they cannot guarantee a representative sample.\n\nConvenience sampling\nA convenience sample includes whoever is easiest to reach. It is inexpensive but cannot produce generalizable results.\nExample: You want to learn about student support services, so after each of your classes you ask fellow students to complete a quick survey. Because you only surveyed students in your own classes, the sample is not representative of all students.\n\n\nVoluntary response sampling\nA voluntary response sample consists of people who opt in on their own. These participants are often those with strong opinions, leading to self‑selection bias.\nExample: You email a survey to the entire student body asking for opinions about a new campus policy. Only those who feel strongly (either for or against) are likely to respond, so the results cannot be trusted to reflect the average student’s view.\n\n\nPurposive sampling\nIn purposive sampling, you choose participants because they have specific characteristics that are particularly informative.\nExample: You want to understand the experiences of disabled students using university services, so you purposefully select students with varied support needs to gather a range of perspectives. This approach is useful for qualitative insight but not for estimating population-level quantities.\n\n\nSnowball sampling\nSnowball sampling recruits participants via referrals from initial subjects. It’s used when populations are difficult to identify or contact.\nExample: To study experiences of homelessness, you interview one person who agrees to participate; she introduces you to other people she knows who are also homeless. The sample grows like a snowball, but you have little control over how representative it is.\n\n\n\nWorking in JMP Pro 17\nAlthough you won’t implement complex sampling plans in JMP, it’s helpful to know how to create random subsets for practice or model validation. To select a simple random sample from a data table, use Rows → Row Selection → Random Sample to mark a proportion or number of rows at random, then Tables → Subset to create a new table of just those rows. For stratified sampling, add a column indicating the stratum (e.g., gender) and use Row Selection → Stratify or a script to select random rows within each level. To take a cluster sample from a large data set, you can create a column identifying clusters (e.g., schools) and use Tables → Subset to extract all rows from randomly chosen clusters. While JMP has tools for convenience (e.g., sorting and taking the first \\(n\\) rows), remember that non‑probability sampling cannot support generalizable inference.\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nRepresentative sample\nA sample that accurately reflects key characteristics of the population.\n\n\nProbability sampling\nSampling technique using random selection so each unit has a known chance of inclusion.\n\n\nNon‑probability sampling\nSampling techniques based on convenience or judgement without randomisation.\n\n\nSimple random sampling\nEvery unit has an equal chance of selection; implemented via random number generators.\n\n\nSystematic sampling\nSelecting every \\(k\\)-th unit from an ordered list after a random start.\n\n\nStratified sampling\nDividing the population into subgroups and randomly sampling within each subgroup.\n\n\nCluster sampling\nRandomly selecting entire groups (clusters) and studying all units within them.\n\n\nConvenience sampling\nIncluding the most accessible units; prone to sampling and selection bias.\n\n\nVoluntary response\nSampling based on participants who choose to respond, often those with strong opinions.\n\n\nPurposive sampling\nSelecting cases based on researcher judgement of what is most informative.\n\n\nSnowball sampling\nRecruiting participants via referrals from initial subjects, often for hidden populations.\n\n\n\n\n\nCheck your understanding\n\nYou want to estimate the average GPA of all first‑year students at your university.\n\nName two probability sampling methods you could use.\nBriefly explain why a convenience sample of your friends might mislead you.\n\nA researcher selects every 5th name from a sorted list of patients to survey. What sampling method is this? Under what circumstance might this method introduce bias?\nCompare stratified sampling and cluster sampling. Give an example of a scenario where each would be appropriate.\nExplain why voluntary response samples often yield extreme views and cannot be trusted for generalizing to a population.\nIn JMP, how could you create a simple random sample of 150 observations from a data table with 2,000 rows?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n(a)Simple random sampling (assign each first‑year student a number and randomly select using a random number generator); stratified sampling (divide students by major or residence hall and sample proportionally within each group). (b) Your friends are likely from similar classes or social circles, so they may have similar study habits; they might not reflect the broader student body.\nThis is systematic sampling. It works well if the list has no pattern related to the outcome. If patients are sorted by appointment time, every 5th patient might always be a morning appointment, which could bias results if morning and afternoon patients differ.\nStratified sampling divides the population into meaningful groups and samples within each (e.g., sampling men and women separately when studying height). It ensures each subgroup is represented. Cluster sampling selects whole groups (e.g., choosing three hospitals at random and surveying all nurses within them) to save cost when the population is geographically spread out.\nPeople with strong positive or negative feelings are more likely to volunteer, while those who are neutral remain silent. This self‑selection skews the sample, so the responses do not reflect the average opinion in the population.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "02.html#experimental-design",
    "href": "02.html#experimental-design",
    "title": "2  Collecting Data",
    "section": "2.2 Experimental Design",
    "text": "2.2 Experimental Design\n\n“All life is an experiment. The more experiments you make the better.” -Ralph Waldo Emerson\n\nGuiding question: How do we design surveys and experiments?\nStatistics gives us two complementary ways to gather evidence: surveys and experiments. In a survey we choose cases from a population, ask questions, and summarize what we learn. In an experiment we assign treatments to units and observe responses. The logic of inference depends on both good sampling and good design. A common confusion is that random sampling (how you pick cases) and random assignment (how you distribute them to treatments) are the same thing. They’re not: random sampling increases external validity (generalizability), while random assignment increases internal validity (causal credibility).\n\nPrinciples of good experimental design\nExperiments aim to isolate the effect of a treatment by controlling other sources of variation. Four key principles help us get there:\n\nRandomization. Assigning units to conditions by chance ensures that, on average, groups are comparable on both observed and unobserved characteristics. In a completely randomized design, every unit is assigned independently—like students randomly given different cell phone use limits in a sleep study. In a randomized block design, units are first grouped on a relevant characteristic (e.g., soil plots by rainfall zone) and then randomly assigned within each block. Randomization also applies to order of runs: in a manufacturing experiment, running treatments in random order prevents the effect of ambient conditions (e.g., temperature) from confounding with other variables.\nFor example, in a drug study with 20 mice and two test kits (A and B), you might randomly select 10 mice to receive kit A and the remaining 10 to receive kit B. Randomization transforms systematic variation (such as age or weight differences) into random variation, preventing confounding of the treatment effect with extraneous factors.\nControl and placebo. Experiments compare a treatment group to a control group. The control may receive “usual care” or a placebo to account for expectations. Placebo-controlled designs are common in medicine because participants may improve simply because they believe they’re being treated. By keeping all other conditions identical and varying only the active ingredient, the researcher can attribute differences in outcomes to the treatment itself rather than to the act of giving the treatment.\nReplication. Repeating the same treatment on multiple experimental units lets us estimate natural variability and distinguish real effects from random noise. For example, measuring battery life on several cells under the same charging condition gives a better estimate than using just one battery.\nBlocking. If you know a nuisance factor (like soil type or patient age) affects the response, block on it. In a block design, you divide units into homogeneous groups and then randomize treatments within each block. In biological experiments, mice are naturally grouped by litter, and chemical samples often come in batches; these natural groupings can serve as blocks. By assigning treatments randomly within each litter or batch, you control for variation due to the block (e.g., genetic or batch differences) and gain more precise estimates of the treatment effect.\n\nA good design often combines these ideas: you might randomize fertilizer treatments within rainfall blocks and measure each plot’s yield multiple times. Designs also come in two broad structures: between‑subjects experiments, where each unit receives exactly one condition, and within‑subjects (or repeated‑measures) experiments, where each unit experiences all conditions in random order. Within‑subjects designs reduce variability but require care to avoid order effects—randomizing or counterbalancing the order of conditions is essential.\n\n\nDesigning unbiased survey questions\nSurveys need two kinds of care: how you select respondents and how you ask questions. For representative sampling methods, see Section 2.1. Here we focus on question design. Well‑written questions are the foundation of trustworthy research. Biased or confusing wording can distort data and mislead decision‑makers. Common pitfalls include:\n\nLeading questions: wording that nudges respondents toward a particular answer.\nBiased: “Don’t you agree that our new app is much easier to use?”\nUnbiased: “How would you rate the ease of use of our new app?”\nLoaded questions: questions that assume something controversial is true (e.g., “When did you stop wasting time on your phone?”)\nDouble‑barreled questions: asking about two things but allowing only one answer. The respondent may have different answers for each part, so the result is uninterpretable. For example, “Do you intend to leave work and return to full‑time study this year?”—someone may be planning to leave work but not to study, or vice versa. Another example: “How would you rate our products and level of service?” Because it blends product quality with service quality, a single rating cannot reveal which aspect the respondent is judging.\nAmbiguous wording: vague or multi‑interpretation phrases that lead different respondents to answer different questions. For example, a survey item that asks “How do we compare to our competitors?” leaves respondents guessing whether you mean price, quality, or customer service.\n\nTo craft unbiased survey questions:\n\nUse neutral language. Avoid emotionally charged words and let respondents choose their own answer.\nBe specific and clear. Define terms and make sure every respondent interprets the question the same way.\nAsk one thing at a time. Split double‑barreled questions into separate items.\nBalance response options. Provide symmetrical, evenly spaced choices (ie. 5-point Likert Scale).\nPilot test your survey. Try it on a small group to catch confusing wording or unexpected interpretations.\n\nThese principles help respondents understand your survey and give honest answers.\n\n\nPutting it all together\nIn practice, you often combine surveys and experiments. For example, a company might randomly assign new customers to receive one of two onboarding emails (experiment), then survey them about satisfaction. You would:\n\nRandomly assign customers to email A or B.\nEnsure both groups are comparable (randomization).\nCollect follow‑up satisfaction surveys using neutral, clear questions.\nReplicate across multiple cohorts and possibly block by customer type (e.g., new vs. returning).\n\n\n\nWorking in JMP Pro 17\n\nRandomize and block. When running experiments in JMP, create a column for treatment assignments using Col → Formula → Random to simulate random assignment. Use Tables → Sort to randomize run order, or define blocking factors as separate columns and use Fit Model to include them in your analysis.\nControl and replicate. For experiments with control and treatment conditions, use Analyze → Fit Y by X for simple comparisons or Analyze → Fit Model for multifactor designs. Replicate treatments by duplicating rows; JMP will treat these as separate experimental units.\nDesign surveys. Use Rows → Row Selection → Random Sample to draw pilot samples. Store survey questions in a Data Table with labels and notes. To pilot test question wording, subset your sample and collect feedback, then refine the wording before launching the full survey.\n\n\n\nRecap\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nRandom assignment\nAssigning sampled units to treatment conditions by chance to create comparable groups.\n\n\nTreatment group / control group\nGroups receiving the experimental intervention and baseline comparison, respectively.\n\n\nPlacebo\nAn inert treatment used to mimic the experience of the intervention to control for expectations.\n\n\nReplication\nRepeating the same treatment on multiple experimental units to estimate variability.\n\n\nBlocking\nGrouping similar units and randomizing within each group to control a nuisance factor.\n\n\nBetween‑subjects design\nEach unit experiences only one condition; comparisons are across subjects.\n\n\nWithin‑subjects design\nEach unit experiences all conditions in random order.\n\n\nLeading question\nA survey question that suggests a particular answer.\n\n\nLoaded question\nA survey question containing an assumption or implication.\n\n\nDouble‑barreled question\nA single question that asks about two things.\n\n\nAmbiguous wording\nVague terms that can be interpreted differently by different respondents.\n\n\n\n\n\nCheck your understanding\n\nExplain the difference between random sampling and random assignment. Why are both important, and in what contexts do they apply?\nName the four principles of good experimental design and give a brief example of each.\nConsider this survey question: “How satisfied are you with the cost and quality of your textbooks?” Identify the problem and rewrite the question.\nIn a study of exam performance, 60 students volunteer for tutoring and 60 do not. The volunteer group has a higher average GPA than the non‑volunteer group. Explain why this study may not show that tutoring causes better performance. How could you redesign it?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nRandom sampling determines who gets into the study. Every member of the population has a known chance of selection, improving generalizability. Random assignment determines which condition participants experience, creating comparable groups and allowing causal conclusions. Surveys rely on random sampling; experiments rely on random assignment.\nRandomization: assign units by chance (e.g., randomize phone use levels to study sleep). Control/placebo: include a baseline or placebo condition to isolate the treatment effect. Replication: repeat treatments on multiple units, like testing several batteries under the same condition. Blocking: group units by a nuisance factor (e.g., soil type) and randomize within blocks.\nThe question is double‑barreled—it asks about cost and quality. Rewrite as two separate questions (e.g., “How satisfied are you with the cost of your textbooks?” and “How satisfied are you with the quality of your textbooks?”).\nVolunteers may differ systematically from non‑volunteers (e.g., motivation or prior GPA). Random assignment is missing. To infer causality, randomly assign students to tutoring or control groups and compare outcomes, possibly blocking on prior GPA.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "02.html#observational-studies-vs.-experiments",
    "href": "02.html#observational-studies-vs.-experiments",
    "title": "2  Collecting Data",
    "section": "2.3 Observational Studies vs. Experiments",
    "text": "2.3 Observational Studies vs. Experiments\n\n“You can observe a lot by just watching.” - Yogi Berra\n\nGuiding question: Why are observational studies sometimes problematic for conclusions?\nAt first glance, collecting data looks the same whether you’re watching what happens or deliberately changing something. But how you gather the data matters tremendously for what you can conclude.\n\nObservational studies: watching without intervening\nIn an observational study, researchers record what happens without actively assigning exposures or treatments. Common types include:\n\nCohort studies, where a group of people linked by a characteristic (e.g., birth year) is followed over time. Researchers compare outcomes between those exposed to some factor and those not exposed.\nA classic example of a cohort study is the Framingham Heart Study (FHS), which has been described in detail in the International Journal of Epidemiology. In 1948 the National Heart Institute recruited a community‑based cohort of 5,209 adults aged 30–59 years from Framingham, Massachusetts, to investigate causes of cardiovascular disease (CVD). Two of every three families in the town were randomly sampled and invited; 4,494 (about 69%) agreed to participate, and an additional 715 volunteers joined. This initial prospective cohort has been followed every two to four years with detailed medical histories, physical examinations, electrocardiograms and laboratory tests. By following participants longitudinally for decades, the FHS identified major risk factors for CVD—such as high blood pressure, cholesterol, and smoking—helping to shape modern cardiovascular prevention guidelines.\nCase–control studies, where people with a condition (“cases”) are compared to similar people without it (“controls”) to look for differences in past exposures.\nFor example, a German study1 compared 118 patients with a rare form of eye cancer called uveal melanoma to 475 healthy patients who did not have this eye cancer. The patients’ cell phone use was measured using a questionnaire. On average, the eye cancer patients used cell phones more often. The cases were those who had developed uveal melanoma and the controls were those who did not uveal melanoma. The cell phone use was compared between the two groups.\n\nBecause participants choose their own behaviors, observational data reflect the real world and are often the only ethical way to study harmful exposures. For example, you can’t ethically assign people to smoke or not smoke, so the long‑term effects of smoking are studied by tracking smokers and non‑smokers over time.\nObservational studies are usually quicker and cheaper than experiments and have high ecological validity (they mirror everyday life). But they have a critical limitation: you can’t be sure whether differences in outcomes are caused by the exposure or by other factors that differ between groups. For example, a highly publicized 1985 study from Johns Hopkins University linked coffee consumption to an increased risk of heart disease, especially for heavy drinkers. The study’s findings, published in the American Journal of Epidemiology, were later challenged by other research that pointed out the failure to adequately control for the effect of cigarette smoking. Once smoking was controlled for, the link between coffee consumption and increased risk of heart diseas was no longer significant.\n\n\nExperiments: deliberately changing something\nIn an experiment, researchers assign treatments or interventions to units and observe the effects. Randomization—assigning units by chance—ensures that, on average, the groups are comparable on both observed and unobserved characteristics. The classic experimental design is the completely randomized design: participants are randomly allocated to receive a new drug, a placebo, or no treatment, and outcomes are compared. Experiments are considered the gold standard for establishing causality because randomization eliminates systematic differences between groups.\nExperiments also offer a controlled environment, making it easier to isolate the effect of a single factor. However, they can be expensive, time-consuming, or unethical to conduct.\nFor example, suppose we were interested in the association between eye cancer and smart phone use. Suppse we conduct an experiment, such as the following:\n\nPick half the students from your school at random and tell them to use a smart phone each day for the next 50 years.\nTell the other half of the student body not to ever use smart phones.\nFifty years from now, analyze whether cancer was more common for those who used smart phones.\n\nThere are obvious difficulties with such an experiment:\n\nIt’s not ethical for a study to expose over a long period of time some of the subjects to something (such as smart phone radiation) that we suspect may be harmful.\nIt’s difficult in practice to make sure that the subjects behave as told. How can you monitor them to ensure that they adhere to their treatment assignment over the 50-year experimental period?\nWho wants to wait 50 years to get an answer?\n\nThus, an observational study would be preferred over an experiment.\n\n\nWhy observational studies can mislead\nObservational data are susceptible to confounding—a situation where a third factor influences both the exposure and the outcome, creating a spurious association. For example, an observational cohort might find that people who meditate have lower rates of heart disease. But meditators may also exercise more and eat healthier diets, making it unclear whether meditation or lifestyle explains the difference. Similarly, people who choose to take daily vitamins might generally have healthier habits, so observed vitamin benefits may reflect those habits rather than the vitamins themselves.\nRandomization is the only method that can eliminate potential confounders by balancing both measured and unmeasured factors across treatment groups. In observational research, statistical methods like stratification, regression adjustment and propensity score matching can reduce bias, but they depend on untestable assumptions: all confounders must be measured correctly and modeled properly. Many important confounders may be unknown or infeasible to measure. Even meticulously controlled observational studies cannot remove all confounding. As a result, observational evidence alone “cannot support conclusions of causation”.\nBelow is a simple simulation illustrating confounding. Shoe size and reading ability appear positively related, but both are driven by age. When age is not controlled, a misleading association emerges.\n\n\n\n\n\n\n\n\n\nThe scatterplot shows a strong correlation between shoe size and reading, even though neither directly affects the other. The common cause is age. Observational studies must always consider whether a hidden variable like age could be responsible for an observed association.\n\n\nWorking with observational data in JMP Pro 17\nIn JMP you can explore observational data using Graph → Graph Builder or Analyze → Fit Y by X to visualize associations. Use Analyze → Fit Model for regression or ANOVA, including potential confounders as predictors. JMP will happily fit a model and produce p-values, but you must interpret the results cautiously. Remember:\n\nIncluding covariates can adjust for measured confounders, but unmeasured confounders remain.\nAssociations observed in scatterplots or regression analyses do not imply causation.\nUse notes and metadata fields to document the study design—observational or experimental—and any potential sources of bias.\n\n\n\nRecap\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nObservational study\nA study in which researchers record exposures and outcomes without assigning treatments or interventions.\n\n\nCohort study\nObservational design where a group is followed over time to compare outcomes between exposed and unexposed members.\n\n\nCase–control study\nObservational design where people with a condition (“cases”) are compared to similar people without the condition (“controls”) to look for differences in past exposures.\n\n\nExperiment\nA study where researchers introduce an intervention and randomly assign subjects to treatment or control groups.\n\n\nConfounding\nA situation where a third factor influences both the exposure and the outcome, potentially creating a spurious association.\n\n\n\n\n\nCheck your understanding\n\nA nutrition researcher recruits people who already take daily multivitamins and compares their health outcomes to people who do not.\n\nIs this an observational study or an experiment?\nName at least two potential confounding variables.\n\nIn a randomized trial, half the participants are assigned to eat a Mediterranean diet and half to continue their usual diet. After a year, the first group shows lower cholesterol. Explain why randomization strengthens the causal interpretation.\nA cohort study finds that people who bike to work have lower rates of depression than those who drive. Suggest two reasons why this association may not reflect a causal effect of biking.\nDescribe a research question that would be unethical or impractical to answer via experiment but could be studied observationally. Explain why.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\na)This is an observational study because participants choose whether or not to take multivitamins. b) Potential confounders include diet quality, exercise habits, socioeconomic status, access to healthcare, smoking status and other health behaviors.\nRandomization assigns diets by chance, so, on average, both known and unknown factors (age, lifestyle, genetics) are balanced across the groups. Therefore, differences in cholesterol are likely due to the diet rather than pre‑existing differences.\nPeople who bike may have higher baseline fitness and better mental health; they might live in neighborhoods with better infrastructure or community support; they may also have lifestyles that promote well‑being (e.g., more time outdoors). Any of these confounders could explain the observed association.\nStudying the long‑term effects of smoking is unethical to do experimentally, because you can’t randomly assign people to smoke. Instead, researchers observe smokers and non‑smokers and compare outcomes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "02.html#sources-of-bias",
    "href": "02.html#sources-of-bias",
    "title": "2  Collecting Data",
    "section": "2.4 Sources of Bias",
    "text": "2.4 Sources of Bias\n\n“Normally if given a choice between doing something and nothing, I’d choose to do nothing. But I would do something if it helps someone do nothing. I’d work all night if it meant nothing got done.” - Ron Swanson\n\nGuiding question: How can bias creep into data collection?\nWhen we talk about bias in statistics, we mean a systematic error built into the way we select or measure our data. Bias is different from random sampling error. Sampling error comes from the natural variability you get when you observe only part of the population and tends to shrink as sample sizes grow. Bias, by contrast, does not go away with bigger samples; a flawed design simply produces more confident wrong answers. That makes it important to understand the different ways bias sneaks into our studies.\n\nHow bias differs from sampling error\nWhenever we select a sample, the numbers we compute (like the mean or proportion) will vary from one sample to the next. This variability is called sampling error. If we repeated our survey many times with different random samples, the average of those sample statistics would be the true population value, and the spread among them would reflect sampling error. Increasing the sample size reduces sampling error, but it does not correct for systematic flaws in how the sample was chosen. When the method of collecting or measuring data systematically favors some outcomes over others, we call it bias. A biased sample can be huge and still be wrong because its error is baked into the design.\nTo illustrate the difference, imagine a population with a true average income of $8 (in arbitrary units), made up of 70% low earners (income of 5) and 30% high earners (income of 15). Below we simulate two ways of sampling from this population: a fair simple random sample and a biased sample that over‑selects high earners (80% high, 20% low). As the sample size grows, the random sample mean settles near the true average, while the biased sample mean stays high. This shows that increasing the sample size reduces random error but does not fix bias.\n\n\n\n\n\n\n\n\n\n\n\nCommon sources of bias\nBias can enter at many points in the data‑collection process. Here are some of the most common culprits:\n\nCoverage (undercoverage) bias. A coverage bias occurs when some members of the population are not included in the sampling frame. The Literary Digest’s famous 1936 presidential poll relied on telephone directories and car registration lists, thereby missing less affluent voters who tended to support Franklin Roosevelt. Because those voters were excluded, the sample favored wealthier respondents and overpredicted Alfred Landon’s support.\n\n\n\n\n\nNonresponse bias. A nonresponse bias arises when selected individuals choose not to participate and the responders differ systematically from nonresponders. In the same 1936 survey only 25 % of those sampled returned the mail‑in ballot. Landon supporters were more likely to return the survey, so the results overestimated his popularity.\nVoluntary response bias. When people opt into a survey on their own—like call‑in radio polls about controversial topics—the sample disproportionately includes individuals with strong opinions. This voluntary response bias can produce extreme results because moderate voices remain silent.\nConvenience sampling bias. A convenience sample chooses whoever is easiest to reach. If you stand outside a gym to survey “all adults in the city,” your sample will overrepresent health‑conscious people. Convenience sampling often leads to coverage problems.\nResponse (measurement) bias. Even if we select the right people, the way we ask questions or record data can introduce response bias. Response bias occurs when the measurement process influences the answer: leading questions or unbalanced answer choices can nudge respondents toward particular responses. Social desirability bias occurs when people underreport socially undesirable behaviors or overreport virtuous ones.\nSurvivorship bias. When we only observe “survivors” and ignore those that dropped out or failed, we can mistake success for the rule.\n\n\n\nThe most famous example of this is the WWII bomber problem. During WWII, analysts tallied bullet holes on returning Allied bombers and saw clusters on wings and fuselage, with relatively few in engines and cockpit. The intuitive fix was to add armor where the holes were densest. Statistician Abraham Wald pointed out the trap: these data come only from planes that survived. Holes on the survivors mark places a plane can be hit and still make it home. The missing planes—those that didn’t return—are precisely the ones likely hit in the “clean” areas (e.g., engines). So Wald recommended reinforcing the areas with the fewest holes on the survivors, not the most. That’s survivorship bias: drawing conclusions from only the observed “winners” and ignoring the unseen “failures.”\nRecall bias. In retrospective studies, participants may not remember past events accurately. People who have developed an illness might recall exposures differently than healthy controls, leading to systematic differences.\nInterviewer bias. The interviewer’s tone, appearance or expectations can subtly influence responses. Neutral wording and training can reduce this effect.\nHealthy‑user bias and attrition bias. People who choose to participate in certain programs or who remain in a study for its duration often differ from those who do not, leading to biased estimates.\n\nEach of these biases stems from the way participants are chosen or how data are measured; they cannot be “averaged out” by larger samples.\n\n\nMitigating bias\nTo minimize bias:\n\nUse probability sampling whenever you want to generalize to a population. Random sampling helps guard against undercoverage and voluntary response bias.\nEnsure your sampling frame matches your target population. Consider oversampling underrepresented groups and weighting responses to reflect their true proportion.\nFollow up with nonresponders and offer multiple modes of participation to reduce nonresponse bias.\nDesign neutral, balanced questions and offer anonymity to reduce measurement and social desirability bias.\nDocument who was invited and who actually participated so you can assess potential biases.\nIn observational studies, adjust for measured differences between participants and nonparticipants using weighting or modeling; but remember that unmeasured biases may remain.\n\n\n\nWorking in JMP Pro 17\nJMP cannot magically remove bias, but it can help you detect and address it:\n\nExplore representativeness. Use Distribution and Graph Builder to compare your sample’s demographics to known population benchmarks. If certain groups are underrepresented, consider weighting or stratified analysis.\nIdentify nonresponse patterns. Create an indicator column for nonresponders and use Fit Y by X or Fit Model to see whether response differs by variables like age or region.\nDesign balanced surveys. Although JMP does not build surveys, you can use scripts or formulas to randomize question order, generate balanced answer scales, and check for item nonresponse.\nApply weights. If you oversample some groups, add a weight column and specify it in analyses (under the red triangle menu) so that estimates reflect the target population.\n\n\n\nRecap\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nCoverage bias\nSystematic error that arises when part of the population is missing from the sampling frame.\n\n\nNonresponse bias\nBias introduced when individuals who do not respond differ meaningfully from those who do respond.\n\n\nVoluntary response bias\nBias caused by allowing people to opt into a survey; respondents with strong opinions dominate the sample.\n\n\nResponse bias\nBias that arises from flaws in the measurement process, such as leading questions or social desirability.\n\n\nSampling error\nNatural variability in statistics from sample to sample; decreases with larger samples.\n\n\nBias\nSystematic error due to design or measurement; does not diminish with larger samples.\n\n\nSurvivorship bias\nFocusing only on observed “survivors” and ignoring those that failed, leading to overly optimistic conclusions.\n\n\n\n\n\nCheck your understanding\n\nA tech company sends an email survey to customers using its premium service. Over half of the recipients do not respond. The company concludes that 85 % of its customers are satisfied.\n\nIdentify two potential sources of bias.\n\nSuggest one way to mitigate each bias.\n\nA political action group hosts an online poll on its website asking visitors whether they support a proposed tax increase. Seventy‑five percent say “no.” What type of bias is most likely, and why does this poll not reflect general public opinion?\nSuppose you draw a simple random sample of 1,000 Baylor students from a roster and send them a questionnaire. Only 200 students respond. How could you use follow‑ups or weighting to reduce bias? Explain your reasoning.\nExplain the difference between sampling error and bias in your own words. Why can a huge sample still give a wrong answer?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n(a)Coverage bias and nonresponse bias. The company sampled only premium users (ignoring basic or free users) and most of the sampled customers did not respond, so respondents may differ from nonrespondents. (b) To reduce coverage bias, draw a sample from all customers or weight responses to reflect the full user base. To reduce nonresponse bias, send reminders, offer incentives or provide alternative modes (e.g., phone, mail).\nThis is voluntary response bias: only visitors who care enough to vote participate, and they may have strong opinions. A poll embedded on a partisan website cannot be generalized because participants are self‑selected and not representative of the broader population.\nThe low response rate introduces nonresponse bias. You could send follow‑up reminders, offer incentives, or contact nonrespondents by phone to increase participation. If demographic data are available for all sampled students, you can apply weights so that the 200 responders reflect the distribution of the 1,000 sampled students (and thus the target population).\nSampling error is the random fluctuation you see from one sample to the next; it decreases with larger samples. Bias is a systematic error built into the design or measurement; it doesn’t shrink with bigger samples. A huge convenience or volunteer sample can still give a wrong answer if it systematically excludes part of the population or asks questions in a biased way.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "02.html#footnotes",
    "href": "02.html#footnotes",
    "title": "2  Collecting Data",
    "section": "",
    "text": "Stang, A., Anastassiou, G., Ahrens, W., Bromen, K., Bornfeld, N., & Jöckel, K. H. (2001). The possible role of radio frequency radiation in the development of uveal melanoma. Epidemiology, 7-12.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "03.html",
    "href": "03.html",
    "title": "3  Describing Data with Tables and Graphs",
    "section": "",
    "text": "3.1 Organizing Categorical Data\nGuiding question: Which graphs work best for categorical data?\nWhen you collect data that fall into groups—like preferred streaming service, political affiliation, or type of pet—the first step is to count how many observations fall into each category. Those counts form the backbone of both tables and graphs for categorical data. In this section we’ll learn how to build simple frequency tables, translate them into proportions or percentages, and organize two categorical variables together in a two‑way table. Along the way we’ll see when different visual summaries make sense and preview bar and pie charts (covered in detail in Section 3.2).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Describing Data with Tables and Graphs</span>"
    ]
  },
  {
    "objectID": "03.html#sec-03_01",
    "href": "03.html#sec-03_01",
    "title": "3  Describing Data with Tables and Graphs",
    "section": "",
    "text": "“The greatest value of a picture is when it forces us to notice what we never expected to see.” – John Tukey\n\n\n\n\nFrequency and relative frequency tables\nA frequency table lists the possible values of a categorical variable and the number of times each value occurs. It’s one of the simplest ways to summarize data, but it packs a punch. Such tables usually have two columns: one for the categories and one for their counts. Creating one involves listing the distinct categories and tallying how many observations fall into each category.\nSometimes absolute counts aren’t enough—especially when comparing samples of different sizes. A relative frequency expresses each category’s count as a proportion of the total. When we add a column of proportions (or percentages) to a frequency table we get a relative frequency table. To compute the relative frequencies, divide each count by the total number of observations. Relative frequencies always add up to 1 (or 100% when expressed as percentages), which makes them handy for comparing distributions across different sample sizes.\n\nAn example: common symptoms in a clinic\nImagine you survey 30 patients at a local clinic about the primary symptom that brought them in. You record four categories: “Headache,” “Back pain,” “Fatigue,” and “Nausea.” We can organize the responses in a simple table of counts and proportions. Below we simulate such a survey and display the results.\n\n\n\n\n\nresponse\nfrequency\nrelative_frequency\n\n\n\n\nBack pain\n9\n0.3000000\n\n\nFatigue\n9\n0.3000000\n\n\nHeadache\n8\n0.2666667\n\n\nNausea\n4\n0.1333333\n\n\n\n\n\n\n\n\n\n\n\n\nThe table lists the four categories in alphabetical order with their counts and relative frequencies. For instance, if 8 of the 30 patients reported “Headache,” the relative frequency of “Headache” is \\(8/30 \\approx 0.27\\). The accompanying bar chart gives a visual sense of the same information: each bar’s height corresponds to a category’s frequency, and the bars are separated to emphasize that the categories have no inherent order. In practice you might reorder the bars to make the graph easier to read—perhaps putting the largest category first.\n\n\n\n\n\n\nPareto charts\n\n\n\n\n\nSometimes you want to highlight the few categories that account for most of the observations. A Pareto chart is a bar chart arranged in descending order of frequency and often paired with a cumulative percentage line. It helps you identify the “vital few and trivial many” in quality control and business applications. Pareto charts are useful when there are many categories and you want to focus attention on the most common causes or responses.\n\n\n\n\n\n\n\n\n\nTip:\n\n\n\n\n\nIn JMP Pro 17 you can create a frequency table by selecting Analyze → Distribution, assigning your categorical variable to the X role, and examining the resulting counts. To add relative frequencies, use the red triangle menu (▸) to choose Display Options → Show Percent. JMP’s Graph Builder will automatically construct a bar chart when you drag a categorical variable to the X‑axis and the count statistic to the Y‑axis.\n\n\n\n\n\n\nTwo‑way (contingency) tables\nWhat if you have two categorical variables and want to see how they interact? A contingency table (also called a two-way table) displays the counts for each combination of levels of the two variables. One variable defines the rows and the other defines the columns. Such tables are the starting point for examining associations and will underlie chi‑square tests in later chapters.\n\nAn example: symptom by age group\nSuppose we collect data on the same symptom question but also record each patient’s age group: “Under 30,” “30–50,” or “Over 50.” We can summarize the joint distribution in a two‑way table.\n\n\n\n\n\nage_group\nBack pain\nFatigue\nHeadache\nNausea\n\n\n\n\n30–50\n3\n4\n5\n3\n\n\nOver 50\n2\n3\n1\n0\n\n\nUnder 30\n4\n2\n2\n1\n\n\n\n\n\nEach cell in the table shows the number of patients who fall into the corresponding combination of age group and symptom. We can also compute row or column relative frequencies to see percentages within each group; for example, dividing each row by its total gives the distribution of symptoms within each age group. Contingency tables allow us to see whether symptom patterns differ across age groups and serve as input for clustered or stacked bar charts (discussed in Section 3.2).\n\n\n\nWhy percentages matter\nBecause categorical variables can have different numbers of levels and sample sizes can vary, relative frequencies are essential for fair comparisons. Reporting only counts can be misleading: 20 supporters of a movie genre in a survey of 50 people represent a large fraction, while 20 supporters in a survey of 500 people represent a much smaller fraction. Percentages standardize the scale.\nWhen displaying percentages, make sure they add to 100%. In a pie chart (a circular graph we’ll describe in the next section), each slice represents a category’s percentage of the whole. Pie charts are useful for showing how the total is divided among categories, but they become cluttered with too many slices. Bar charts are more flexible: you can reorder the bars, show counts or percentages, and compare multiple groups using side‑by‑side or stacked bars.\n\n\nWorking in JMP Pro 17\nIn JMP, tables and graphs for categorical variables are straightforward:\n\nTo create a frequency table, go to Analyze → Distribution, assign your categorical variable to X, and click OK. The report shows counts and percentages; use the red triangle (▸) menu to toggle percentages, counts, or both.\nFor two categorical variables, use Analyze → Fit Y by X and assign one variable to Y and the other to X. Choose Contingency Table from the platform to see the two‑way counts and associated statistics.\nTo visualize categorical distributions, open Graph Builder, drag the categorical variable to the X‑axis, and drop the N summary statistic onto the Y‑axis. You can change the chart type to “Bar” or “Pie.” Dragging a second categorical variable onto the Group drop zone will create clustered or stacked bars.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nFrequency table\nA table that lists each category of a variable and the number of observations in that category.\n\n\nRelative frequency\nThe proportion or percentage of observations in a category, equal to the category’s count divided by the total count.\n\n\nRelative frequency table\nA frequency table with an additional column showing the relative frequency of each category.\n\n\nTwo‑way (contingency) table\nA table that displays the counts for each combination of levels of two categorical variable.\n\n\n\n\n\nCheck your understanding\n\nIn a survey of 80 households, 32 own a dog, 20 own a cat, 12 own both, and the remainder own no pets. Construct a frequency table that shows the number and percentage of households in each pet ownership category (Dog only, Cat only, Both, None). Which visualization—a bar chart or a pie chart—would you choose, and why?\nExplain the difference between a frequency table and a relative frequency table. In what situations is it more informative to look at relative frequencies rather than absolute frequencies?\nWhat is a two‑way (contingency) table? Describe a scenario where a two‑way table could help you explore the relationship between two categorical variables.\nBar charts have spaces between bars and can be drawn in any order. Why are these design choices appropriate for categorical variables? What might go wrong if you drew the bars touching or forced them into a numerical order?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nPet ownership table. The four categories and their counts are: Dog only (20), Cat only (8), Both (12), None (40). The total number of households is 80. The relative frequencies are 25% dog only, 10% cat only, 15% both, and 50% none. A bar chart would be preferable here because it allows you to order the bars from most to least common and makes it easy to compare magnitudes. A pie chart could work for four categories, but it becomes harder to read when slices are similar in size or when there are many categories.\nFrequency vs. relative frequency. A frequency table reports the counts of observations in each category. A relative frequency table adds a column showing the proportion or percentage of observations in each category. Relative frequencies are more informative when comparing groups of different sizes or when you want to focus on the distribution rather than the sample size—for example, comparing survey results from two classes of different sizes.\nContingency table example. A two‑way table displays counts for each combination of levels of two categorical variable. For instance, you could record whether each patient in a clinic has insurance (Yes/No) and whether they arrived on time (On time/Late). A contingency table would show how many patients fall into each combination (e.g., insured & on time, insured & late, uninsured & on time, uninsured & late), helping you explore whether punctuality differs by insurance status.\nDesign choices. Categories have no intrinsic numeric order, so bars in a bar chart can be arranged in any order without misrepresenting the data. Leaving space between bars reinforces that the categories are distinct and unordered. If you drew the bars touching, it might suggest a continuous scale (like a histogram), which could confuse readers. Forcing categories into a numerical order might imply ranking where none exists.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Describing Data with Tables and Graphs</span>"
    ]
  },
  {
    "objectID": "03.html#sec-03_02",
    "href": "03.html#sec-03_02",
    "title": "3  Describing Data with Tables and Graphs",
    "section": "3.2 Bar Charts and Pie Charts",
    "text": "3.2 Bar Charts and Pie Charts\n\n“`Normally if given a choice between doing something and nothing, I’d choose to do nothing. But I would do something if it helps someone do nothing. I’d work all night if it meant nothing got done.” – Ron Swanson\n\nGuiding question: How do we make clear, truthful bar charts and pie charts?\nWhen you’ve tallied the counts of a categorical variable, your next job is to turn those numbers into a picture. Two of the simplest pictures—bar charts and pie charts—seem deceptively alike: each shows categories and their sizes. But as we’ll see, they serve different purposes and come with different design rules.\n\nWhat is a bar chart?\nAs we have already seen in Section 3.1, bar chart displays categories along one axis and uses the length of a bar on the other axis to represent a numerical value. Bars can be vertical (a “column” chart) or horizontal, and they are separated by gaps to emphasise that the categories are discrete. Because humans are good at comparing lengths that share a common baseline, bar charts are our go‑to tool for comparing counts, percentages or other statistics across categories.\nExample: distribution of blood types. Suppose a hospital records the blood type (A, B, AB or O) of 200 randomly chosen donors. The counts are shown in the table below along with a bar chart. Notice that the bars are separated and can be reordered to make patterns easy to see.\n\n\n\n\n\ntype\ncount\nprop\n\n\n\n\nA\n66\n0.330\n\n\nAB\n9\n0.045\n\n\nB\n31\n0.155\n\n\nO\n94\n0.470\n\n\n\n\n\n\n\n\n\n\n\n\nThe vertical bar chart emphasizes how common type O is relative to the others. You could flip the axes to make a horizontal bar chart if your category names are long or if you prefer to read labels on the y‑axis.\nDesign tips for bar charts. A few simple rules help make bar charts honest and clear:\n\nStart the axis at zero. Because bar length encodes value, truncating the axis exaggerates differences.\nKeep consistent spacing. Leave a gap between bars—about half the width of a bar is a good rule of thumb.\nSort deliberately. Arrange bars in a logical order (alphabetical, chronological, or by size) to help the reader scan.\nAvoid clutter and gimmicks. Skip 3‑D effects and decorative icons; they distort perception and add no information.\n\n\n\nWhat is a pie chart?\nA pie chart shows how a total is divided among categories. A circle is divided into slices; each slice represents a category and its angle corresponds to the category’s proportion of the whole. Pie charts are familiar and immediately signal a “part of a whole” story.\nExample: reasons for missing an appointment. A dental clinic tracks why patients miss scheduled cleanings. Out of 100 missed appointments, 50 were due to forgetfulness, 20 to fear, 15 to cost, and 15 to other reasons. A pie chart makes the share of each reason obvious.\n\n\n\n\n\n\n\n\n\nThe slices emphasize that half of the missed appointments were simply forgotten. However, imagine adding three more reasons of similar size. The slices would become crowded and hard to compare. Pie charts work only when the categories sum to a meaningful whole and there are no more than a few slices.\n\n\nWhen to use bar charts vs. pie charts\nAlthough you can plot the same data with either chart, they are not interchangeable. Use a bar chart when you want to:\n\nCompare values across categories or between groups.\nDisplay a statistic that does not sum to a meaningful whole (e.g., average pain scores by medication).\nShow many categories, even if some are small.\n\nUse a pie chart only when:\n\nThe values represent parts of a whole that add up to 100%.\nThe number of categories is small (ideally no more than five).\nYou care more about conveying the big picture of how the whole is divided than about exact comparisons.\n\nWhen you find yourself squinting at a pie chart to see which slice is bigger, switch to a bar chart; our brains judge lengths more accurately than angles.\n\n\nClustered and stacked bar charts\nSometimes you have two categorical variables and want to see how their categories interact. We introduced two‑way tables in Section 3.1; here’s how to graph them.\nA clustered (side‑by‑side) bar chart groups bars for each level of a second variable next to each other so you can compare across groups. For example, imagine you survey 120 patients about how satisfied they were with a new physical therapy program (satisfied, neutral, dissatisfied) and record whether they were in the treatment or control group. A clustered bar chart shows differences in satisfaction between the two groups.\n\n\n\n\n\n\n\n\n\nIn a stacked bar chart, bars for each category are stacked atop one another. This emphasizes the total size of each category but makes it harder to compare the segments across stacks. You might use a stacked chart to show how types of injuries (sprain, fracture, other) contribute to emergency visits across departments; if you convert each bar to 100% of its height, you get a 100% stacked bar chart that highlights composition within each group.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautions with stacked bars\nStacked bars are useful when you care about the total across categories, but they hide patterns in the middle segments. In the last plot, you can easily compare the overall emergency visits across departments and the share of fractures, but it’s harder to compare the “other” injuries across departments because their segments float at different heights. If your goal is to compare subgroups, a clustered bar chart is usually better.\n\n\n\nWorking in JMP Pro 17\nIn JMP, bar and pie charts live in the Graph Builder. Drag your categorical variable to the X‑axis and drop the N or % statistic onto the Y‑axis to create a bar chart. To cluster by a second categorical variable, drop it in the Group or Overlay zone, and choose Bar (Horizontal) or Bar (Vertical) from the chart palette. To stack, use the Stack option in the legend. To make a pie chart, drag the categorical variable to a blank canvas and choose Pie; JMP will automatically convert counts to percentages and label the slices. Use the red triangle (▸) menu to display data labels, reorder slices, or combine small categories into an “Other” slice.\n\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition or note\n\n\n\n\nBar chart\nGraph that displays categories along one axis and uses the length of bars to represent numeric values; great for comparing counts or percentages across categories.\n\n\nPie chart\nCircular chart in which slices represent how a total is divided among categories; appropriate only when values sum to a meaningful whole and the number of categories is small.\n\n\nClustered (side‑by‑side) bar chart\nBar chart where categories are grouped side by side for levels of a second variable; useful for comparing groups across categories.\n\n\nStacked bar chart\nBar chart where bars for each subgroup are stacked; shows composition and totals but makes it harder to compare individual segments.\n\n\n\n\n\n\n\n\n\nCheck your understanding\n\nA clinical trial reports the average pain score (on a 0–10 scale) for three physical therapy programs. Should you use a bar chart or a pie chart? Explain your reasoning.\nA nutritionist surveys 500 patients about their preferred breakfast type: cereal, fruit, eggs, or none. The counts are 150, 120, 80, and 150. Sketch how you would display this information with a pie chart. When might a bar chart be preferable?\nIn a mental health study, participants are classified into stress levels (low, moderate, high) and whether they attended counseling (Yes/No). Which type of bar chart would you use to compare stress levels between counseling and non‑counseling participants? What pattern would indicate that counseling is associated with lower stress?\nA bar chart shows the average number of cavities per patient in three dental clinics: 1.2, 1.3 and 1.4 cavities. The y‑axis starts at 1.0. Explain why this design might mislead and how to fix it.\nGive two reasons why pie charts often make it harder to compare categories than bar charts.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nYou should use a bar chart because the numbers represent average pain scores and do not sum to a meaningful whole. Pie charts imply a part‑to‑whole relationship and would be misleading here.\nThe four breakfast types form parts of the whole sample (500 patients). In a pie chart, the slices would be 30% cereal, 24% fruit, 16% eggs, and 30% none. However, a bar chart may be preferable because it allows you to order the categories from most to least common and makes it easier to compare the cereal and none categories, which are equal in size.\nA clustered (side‑by‑side) bar chart would let you compare the counts (or proportions) of low, moderate, and high stress within the counseling and non‑counseling groups. If counseling is associated with lower stress, you would expect the “low” bar to be taller (or the “high” bar shorter) in the counseling group than in the non‑counseling group.\nStarting the y‑axis at 1.0 truncates the bars and exaggerates small difference. To avoid misleading readers, start the axis at zero. Alternatively, use a dot plot or annotate the differences directly if the differences are small but meaningful.\nFirst, people judge lengths more accurately than angles; in a pie chart it is hard to gauge the exact size of a slice. Second, when slices are similar in size or there are many categories, comparing slices becomes difficult and the chart becomes cluttered. Bar charts avoid these issues by using a common baseline and allowing many bars.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Describing Data with Tables and Graphs</span>"
    ]
  },
  {
    "objectID": "03.html#sec-03_03",
    "href": "03.html#sec-03_03",
    "title": "3  Describing Data with Tables and Graphs",
    "section": "3.3 Organizing Quantitative Data",
    "text": "3.3 Organizing Quantitative Data\n\n“It’s not what you look at that matters, it’s what you see.” - Henry David Thoreau\n\nGuiding question: How should we organize quantitative data?\nCategorical variables live in their own world of discrete groups, but numerical variables—such as heights, blood pressures, cholesterol levels, or reaction times—can take on a continuum of values. To make sense of a list of numbers, we need to group them in a sensible way. In this section we learn how to organize quantitative data into tables that lay the foundation for graphs like histograms, stem‑and‑leaf plots, and dotplots. These tables help us see where the data concentrate, how spread out they are, and how the numbers accumulate.\n\nConstructing frequency distributions\nA frequency distribution for quantitative data is a table that divides the range of the data into non‑overlapping intervals (called classes) and lists the number of observations (the frequency) falling into each class. Each class has a lower limit and an upper limit, the class width is the difference between consecutive lower limits, and the range of the dataset is the difference between the maximum and minimum values. The goal is to choose enough classes to reveal the shape of the distribution without obscuring details.\nTo build a frequency distribution:\n\nDecide on the number of classes. A common guideline is to use between 5 and 20 classes; fewer classes oversimplify the data, while too many classes produce a noisy table.\nCompute the class width. Subtract the minimum value from the maximum value to obtain the range, divide by the chosen number of classes, and round up to a convenient number. This ensures all data points fit into a class without overlap.\nDetermine class limits. Begin at or slightly below the minimum value and add the class width repeatedly to set the lower and upper limits of each class.\nTally the frequencies. Count how many observations fall into each class.\n\nLet’s see these steps with an example from dentistry. Suppose a dental researcher records the number of cavities filled for 40 patients in a clinic over the past month. We want to summarize the distribution of cavities per patient.\n\n\n\n\n\nclass\nfrequency\n\n\n\n\n(-0.5,0.5]\n6\n\n\n(0.5,1.5]\n8\n\n\n(1.5,2.5]\n13\n\n\n(2.5,3.5]\n7\n\n\n(3.5,4.5]\n4\n\n\n(4.5,5.5]\n1\n\n\n(5.5,6.5]\n1\n\n\n\n\n\nThe table lists each class of cavities (0, 1, 2, 3, …) along with the number of patients in that class. For example, if 13 patients had exactly two cavities, the class “(1.5, 2.5]” (interpreted as 2 cavities) would have a frequency of 13. When the data are counts like this, classes of width 1 make sense; for continuous measurements like blood pressure, you would choose wider classes to group the data into ranges.\n\n\nRelative and cumulative frequencies\nRaw counts are helpful, but sometimes we want to know what proportion of observations fall into each class. The relative frequency of a class is the class frequency divided by the total number of observations. If you add successive relative frequencies as you move across the classes, you get the cumulative frequency, which tells you the proportion of observations less than or equal to a given class boundary.\nWe can extend our cavities example to compute relative and cumulative frequencies:\n\n\n\n\n\nclass\nfrequency\nrelative_frequency\ncumulative_frequency\n\n\n\n\n(-0.5,0.5]\n6\n0.150\n0.150\n\n\n(0.5,1.5]\n8\n0.200\n0.350\n\n\n(1.5,2.5]\n13\n0.325\n0.675\n\n\n(2.5,3.5]\n7\n0.175\n0.850\n\n\n(3.5,4.5]\n4\n0.100\n0.950\n\n\n(4.5,5.5]\n1\n0.025\n0.975\n\n\n(5.5,6.5]\n1\n0.025\n1.000\n\n\n\n\n\n\n\n\n\n\n\n\nThe cumulative frequency column increases steadily and reaches 1 at the end of the table. A graph of cumulative frequencies, called an ogive (pronounced “O-jive”), plots the cumulative proportion versus the class.\n\n\nChoosing the number of classes\nSelecting an appropriate number of classes is more art than science, but a few guidelines help. Many textbooks suggest between 5 and 20 classes; an alternative rule of thumb for histograms is to use 5 to 15 bars. As the sample size grows, more classes allow finer detail, but too many classes produce a sparse table. In practice, try several class widths and see which one provides a clear picture of the data. Software like JMP and R will automatically suggest class widths, but you can adjust them manually.\n\n\nWorking in JMP Pro 17\nTo organize quantitative data in JMP:\n\nUse Analyze → Distribution and assign your quantitative variable to the Y‑axis. JMP will display a histogram along with a frequency table showing counts, percentages, and cumulative percentages. The red triangle (▸) menu lets you change the number of bins or show the cumulative distribution.\nTo extract a frequency table without a histogram, go to Analyze → Tabulate, drag your quantitative variable to the Drop Zone for columns, and select a statistic such as N or Percent. JMP’s Tabulate platform can create grouped summaries by another variable (e.g., cavities by gender).\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nClass\nAn interval into which quantitative data are grouped in a frequency distribution; each class has lower and upper limits.\n\n\nFrequency distribution\nA table that lists classes of quantitative data and the number of observations in each class.\n\n\nClass width\nThe difference between consecutive lower class limits, computed by dividing the data range by the number of classes.\n\n\nRelative frequency\nThe fraction or percentage of observations in a class, equal to the class frequency divided by the total sample size.\n\n\nCumulative frequency\nThe running total of frequencies (or relative frequencies) up to a given class, used to construct an ogive.\n\n\nOgive\nA graph of cumulative frequencies or cumulative relative frequencies versus the upper class boundary, useful for identifying percentiles.\n\n\n\n\n\nCheck your understanding\n\nThe heights (in centimeters) of 50 adolescents undergoing orthodontic treatment are recorded. Explain how you would construct a frequency distribution for these heights. How might your choice of the number of classes affect your ability to see the shape of the distribution?\nDifferentiate between a relative frequency distribution and a cumulative frequency distribution. In what situations would you prefer to look at cumulative frequencies instead of simple frequencies?\nSuppose a hospital’s quality control team records the time (in minutes) it takes to triage 100 emergency room patients. The times range from 1 to 28 minutes. If you decide to use eight classes in your frequency distribution, what is the class width? After building the frequency table, describe how you would create an ogive for the data.\nA dataset contains the number of decayed teeth per child in a dental study. Why might a class width of 1 be appropriate for this frequency distribution? When might you choose a wider class width?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nConstructing a frequency distribution. List the minimum and maximum heights, decide on the number of classes (e.g., between 5 and 15), compute the class width by dividing the range by the number of classes, and set class limits starting at or below the minimum. Tally the number of heights falling into each class. Fewer classes smooth out details; more classes reveal fine structure but may result in many empty or low‑frequency intervals. Experimenting with different numbers of classes helps you see the underlying shape.\nRelative vs. cumulative frequency. A relative frequency distribution reports the proportion of observations in each class. A cumulative frequency distribution adds the proportions successively to show the total proportion up to each class. Cumulative frequencies are useful when you care about percentiles or thresholds—for example, determining the percentage of patients whose heights are below a certain value.\nCalculating class width and drawing an ogive. The range is 28 − 1 = 27 minutes. Dividing by eight classes gives 3.375 minutes; rounding up to 4 minutes yields a class width of 4. You would start your first class at or below 1 minute and create successive 4‑minute intervals (e.g., 0–4, 4–8, … ). After tallying frequencies, compute cumulative frequencies and plot them against the upper class boundaries to form the ogive.\nChoosing class width for count data. Decayed teeth are counted in whole numbers, so using a class width of 1 preserves each distinct count (0, 1, 2, …) and yields an easy‑to‑interpret distribution. If the counts range widely or if some counts are rare, a wider class width (e.g., grouping 4–6 teeth together) could reduce sparsity and simplify the table.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Describing Data with Tables and Graphs</span>"
    ]
  },
  {
    "objectID": "03.html#sec-03_4",
    "href": "03.html#sec-03_4",
    "title": "3  Describing Data with Tables and Graphs",
    "section": "3.4 Histograms, Stem‑and‑Leaf Plots, and Dotplots",
    "text": "3.4 Histograms, Stem‑and‑Leaf Plots, and Dotplots\nGuiding question: What’s the right way to visualize quantitative data?\nOnce quantitative data are grouped into classes, we can visualize the distribution using several complementary plots. Histograms show how the data are distributed across intervals; stem‑and‑leaf plots preserve individual values while offering a quick visual summary; and dotplots display each observation as a dot. Choosing the right tool depends on your sample size, measurement scale, and the story you want to tell.\n\nHistograms\nA histogram is a bar‑like graph in which adjacent rectangles represent the classes of a quantitative variable. The horizontal axis is measured in the units of the data, and the vertical axis shows the frequency or relative frequency in each class. Unlike bar charts for categorical data, the bars in a histogram touch to indicate that the underlying scale is continuous. Histograms are excellent for revealing the shape, center, and spread of a distribution.\nTo construct a histogram:\n\nCreate a frequency distribution with classes of equal width (see Section 3.3).\nOn the horizontal axis, draw the class boundaries; on the vertical axis, mark the frequencies or relative frequencies.\nDraw adjacent rectangles whose widths correspond to the class widths and whose heights correspond to the class frequencies. For a density histogram, scale the heights so that the area of each rectangle equals the class’s relative frequency; the total area of the histogram equals one】.\n\nThe number of bins (rectangles) influences the appearance of the histogram. A rule of thumb is to choose between 5 and 15 bins; using more bins adds detail but may create spiky plots. Avoid decorative 3‑D effects and keep the bars aligned on a common baseline.\n\nExample: Resting heart rates\nConsider a study of the resting heart rates (in beats per minute) of 100 individuals undergoing physical therapy.\n\n\n\n\n\n\n\n\n\nThe histogram reveals the overall shape, identifies any outliers, and helps us decide whether certain distribution assumptions might be reasonable. In JMP, you can create a histogram by assigning the variable to Y in the Distribution platform; use the red triangle (▸) menu to adjust the binning.\n\n\n\nStem‑and‑leaf plots\nA stem‑and‑leaf plot divides each observation into a “stem” (the leading digits) and a “leaf” (the last digit. You list the possible stems in order and write the leaves next to their stems. This simple display retains the actual data values while giving a visual impression of the distribution. It’s particularly useful for small to moderate sample sizes (roughly under 100 observations). Unlike histograms, a stem‑and‑leaf plot shows every data point, so you can reconstruct the original dataset if needed.\nFor example, suppose an orthopedic clinic records the times (in days) for 20 patients to regain full range of motion after knee surgery. We can construct a stem‑and‑leaf plot as follows:\n\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  0 | 88899\n  1 | 000002224\n  1 | 5578\n  2 | 14\n\n\nIn a manual stem‑and‑leaf plot, the stems would be 2, 3, 4, … representing tens of days, and the leaves would be the units digit. Many statistical software packages split each stem into two rows—one for leaves 0–4 and one for leaves 5–9—to provide more detail. Reading the plot, you can quickly see that most patients recover in the 10–14 day range and that the longest recovery took about 24 days. Because the stems are ordered, the plot shows the shape of the distribution and highlights extremes.\n\n\nDotplots\nA dotplot places a dot above a number line for each observation; if multiple observations share the same value, the dots stack vertically. Dotplots are easy to construct and interpret for small datasets and can handle discrete and continuous variables. They show clusters, gaps, and outliers while preserving individual data points.\nContinuing our dental theme, imagine researchers record the number of clinic visits per patient during a year for 25 orthodontic patients. A dotplot can reveal whether most patients require only a few visits or if some need frequent check‑ups.\n\n\n\n\n\n\n\n\n\nThe plot makes it clear that most patients had between one and three visits, while a few had zero or five visits. Dotplots are an excellent choice when exact values matter, such as showing distribution of test scores or daily step counts.\n\n\nChoosing the right display\n\nHistograms reveal the overall shape of a distribution and are most useful for moderate to large samples. They sacrifice individual data values to provide a smooth picture of the distribution.\nStem‑and‑leaf plots preserve actual data values and work well for small to moderate samples. They quickly show the shape and allow you to recover the raw data.\nDotplots also preserve individual values and are easy to interpret for small datasets; they can handle discrete counts and continuous measurements.\n\n\n\nWorking in JMP Pro 17\nIn JMP:\n\nTo create a histogram or stem‑and‑leaf plot, use Analyze → Distribution. For stem‑and‑leaf plots, JMP automatically splits stems if needed and prints the “leaf unit.” The red triangle (▸) menu lets you toggle between histogram and stem‑and‑leaf views.\nTo build a dotplot, open Graph Builder, drag your quantitative variable to the X‑axis, and choose “Dot Plot” from the element palette.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nHistogram\nA graph of adjacent rectangles representing classes of a quantitative variable; the horizontal axis is numeric and the bars touch.\n\n\nRelative frequency histogram\nA histogram in which the height of each bar shows the relative frequency (proportion) in that class, so the sum of the bar heights equals 1.\n\n\nStem‑and‑leaf plot\nA display that splits each data value into a stem (leading digits) and a leaf (trailing digit), preserving the original data.\n\n\nDotplot\nA plot that places a dot above each data value on a number line, with stacked dots representing repeated values.\n\n\n\n\n\nCheck your understanding\n\nExplain why a histogram is drawn with adjacent bars touching. How does this design reinforce the type of variable being plotted?\nFor a set of 15 recovery times (in days) after a dental procedure, would you prefer a histogram, a stem‑and‑leaf plot, or a dotplot? Justify your choice.\nIn a stem‑and‑leaf plot, what do the stems and leaves represent? Why might you choose to split the stems into two rows for certain datasets?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nWhy bars touch. The bars in a histogram are adjacent because the underlying variable is continuous, and there are no gaps between the classes. Drawing the bars together emphasizes that each class covers an interval on the number line; a gap would incorrectly suggest a break between values.\nChoosing a plot for 15 observations. For a sample of 15 recovery times, a stem‑and‑leaf plot or dotplot would preserve the individual values and make it easy to see exact times. A histogram could also work, but with so few observations its appearance would depend heavily on the chosen bin width. Stem‑and‑leaf plots are especially useful here because they reveal the shape and extremes while retaining the data.\nInterpreting stems and leaves. In a stem‑and‑leaf plot, the stem contains the leading digits of each number and the leaf is the final digit. Splitting stems into two rows—one for leaves 0–4 and one for leaves 5–9—provides more detail and smooths out long runs of leaves, making the distribution easier to read.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Describing Data with Tables and Graphs</span>"
    ]
  },
  {
    "objectID": "04.html",
    "href": "04.html",
    "title": "4  Describing Data with Numbers",
    "section": "",
    "text": "4.1 Measures of Center (Mean, Median, Mode)\nGuiding question: What’s the “typical” value in a dataset?\nOne of the first things we usually want to know about a numeric variable is where it “centers.” When we say “typical,” we’re asking: if we had to pick one number to represent the group, what would it be? In this section we’ll explore three common answers—mean, median, and mode—and think about when each one makes sense.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Describing Data with Numbers</span>"
    ]
  },
  {
    "objectID": "04.html#measures-of-center-mean-median-mode",
    "href": "04.html#measures-of-center-mean-median-mode",
    "title": "4  Describing Data with Numbers",
    "section": "",
    "text": "“You keep using that word. I do not think it means what you think it means.” - Inigo Montoya\n\n\n\n\nWhat do we mean by “typical”?\nA good summary of the center should capture the central tendency of the data without being too distracted by the extremes. Depending on how the data are shaped, different summaries will feel more or less representative. Here are the big three:\n\nThe mean (arithmetic average)\nThe mean of a sample, often denoted \\(\\bar{x}\\), is the sum of all observations divided by the number of observations. If you place each data point as a weight on a number line, the mean is the balance point. Formally, for sample values \\(x_1, x_2,\\dots,x_n\\), the mean is\n\\[\n\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i.\n\\]\nBecause it uses every value, the mean incorporates all information—but it can be sensitive to outliers. A single extreme value can pull the mean far from where most data lie.\nExample (Scutari data). Florence Nightingale is well known for her work in promoting hospital conditions for British soldiers in the Crimean War. One set of data she explored involved patient outcomes in the hospitals in Crimea versus those in hospitals in Scutari, Turkey1.\nThe data file scutari.jmp contains the number of soldiers in each regiment2 who were hospitalized in Crimea and Scutari and the number who died.\nLet’s examine only the number of soldiers who died in Scutari for the first five regiments in the dataset. These values are\n\n\n\n\n\n93\n96\n42\n155\n94\n\n\n\n\n\nTo find the mean we have \\[\n\\begin{align*}\n  \\bar{x} = & \\frac{1}{5}(480)\\\\\n= & 96  \n\\end{align*}\n\\]\nSo we have the “typical” value for this group of data as 96.\nNow, suppose we had another value added to this data, 500. We now have the data values\n\n\n\n\n\n93\n96\n42\n155\n94\n500\n\n\n\n\n\nThe mean is now \\[\n\\begin{align*}\n  \\bar{x} = & \\frac{1}{6}(980)\\\\\n=& 163.3\n\\end{align*}\n\\]\nDoes 163.3 feel “typical” for this group? Not at all—the single outlier drags the mean upward. This is a case where the mean alone can be misleading.\nSometimes we adjust the mean to reduce the influence of extremes. A trimmed mean drops a certain percentage of the smallest and largest values before averaging; this can provide a compromise between mean and median. In a 10% trimmed mean, for example, you remove the lowest 10% and the highest 10% of observations, then compute the mean of what remains.\n\n\nThe median (middle value)\nThe median is the midpoint of the data when they are ordered from smallest to largest. Half the observations are at or below the median and half are at or above. For an odd number of observations, the median is the middle value; for an even number, it is the average of the two middle values.\nBecause it depends only on order, the median is resistant to extreme values. In the Scutari example above, ordering the values gives\n\n\n\n\n\n42\n93\n94\n96\n155\n500\n\n\n\n\n\nThe median (the average of the third and fourth values) is 95—much more representative of the number of deaths the regiments had in Scutari. This robustness is why medians are often reported for skewed data such as incomes or home prices.\n\n\nThe mode (most frequent value)\nThe mode is the value that occurs most often. A dataset can be unimodal (one most common value), bimodal (two peaks), or multimodal. The mode is particularly useful for categorical or discrete data where the “most common category” is informative. For continuous variables measured with fine precision, exact ties are rare, so we often look at histogram peaks as the mode rather than a single numerical mode.\n\n\n\nThe shape of a distribution\nWhen examining a data distribution of a quantitative variable, whether portrayed by a frequency table or by a graph such as a histogram, we should look for clear peaks. Does the distribution have a single mound? A distribution of such data is called unimodal.\nA distribution with two distinct mounds is called bimodal. A bimodal distribution can result, for example, when a population is polarized on a controversial issue. Suppose each subject is presented with ten scenarios in which a person found guilty of murder may be given the death penalty. If we count the number of those scenarios in which subjects feel the death penalty would be just, many responses would be close to 0 (for subjects who oppose the death penalty generally) and many would be close to 10 (for subjects who think it’s always or usually warranted for murder).\nA bimodal distribution can also result when the observations come from two different groups. For instance, a histogram of the height of students at a university might show two peaks, one for females and one for males.\nWhat is the shape of the distribution? The shape of the distribution is often described as symmetric or skewed.\nA distribution is symmetric if the side of the distribution below a central value is a mirror image of the side above that central value. The distribution is skewed if one side of the distribution stretches out longer than the other side.\nTo skew means to stretch in one direction. A distribution is skewed to the left if the left tail3 is longer than the right tail. A distribution is skewed to the right if the right tail is longer than the left tail.\n\n\nChoosing the right measure\nHow do you decide which summary to use? A few guidelines can help:\n\nUse the mean when the distribution is roughly symmetric without outliers. The mean connects nicely to many statistical models and formulas.\nUse the median when the distribution is skewed or contains outliers. The median provides a better sense of the typical case when extremes are present.\nUse the mode when describing the most common category or when the data are naturally discrete. For continuous variables, speak of the “modal class” (the bin with the highest frequency).\n\nYou can also look at the relationship among mean, median, and mode to diagnose shape. In a symmetric distribution these summaries coincide. In a right‑skewed distribution the mean typically lies to the right of the median, and the mode is the smallest of the three; in a left‑skewed distribution the order reverses.\n\nIllustration: skewness and the mean–median comparison\nThe following simulates data from a right‑skewed distribution and a symmetric distribution and plots them side by side. Notice how the mean and median behave.\n\n\n\n\n\n\n\n\n\nIn the right‑skewed distribution, the mean lies to the right of the median, reflecting the pull of larger values. In the nearly symmetric distribution, the mean and median are close together.\n\n\n\nWorking in JMP Pro 17\nJMP makes it easy to compute and compare these summaries.\n\nUse Analyze→Distribution on a single numeric column. The report shows the Mean and Median under the “Summary Statistics” section, and a “Quantiles” table lists the median explicitly. The bar under the histogram marks the median with a vertical line.\nTo find the mode, examine the histogram (bins with the highest bars) or create a Tabulate table. Because continuous measurements rarely tie exactly, the notion of a mode is approximate.\nFor a trimmed mean, click the red triangle ▶ next to the variable name in the Distribution platform, choose Nonparametric→TrimmedMean, and select the trimming proportion.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nMean\nThe arithmetic average: sum of all observations divided by the number of observations; sensitive to extreme values.\n\n\nMedian\nThe middle value when data are ordered; half the observations are at or below it; resistant to outliers.\n\n\nMode\nThe most frequently occurring value (or class) in the data; useful for categorical or discrete variables.\n\n\nRight-skewed\nA distribution where the tail extends to the right; typically mean&gt;median&gt;mode.\n\n\nLeft-skewed\nA distribution where the tail extends to the left; typically mean&lt;median&lt;mode.\n\n\n\n\n\nCheck your understanding\n\nA sample of commuting times (in minutes) for twelve workers is \\[\n10,\\ 12,\\ 12,\\ 15,\\ 16,\\ 17,\\ 18,\\ 18,\\ 18,\\ 20,\\ 22,\\ 90.\n\\] Use these value for the following:\n\nFind the median commuting time.\nFind the mean commuting time. How does the outlier affect the mean?\nIf you were advising a city planner about what most commuters experience, which summary (mean or median) would you report? Why?\n\nFor which of the following situations would the mode be a more informative summary than the mean or median? Explain your reasoning.\n\nThe test scores (0–100) from a class of 200 students.\nThe favorite ice-cream flavors (chocolate, vanilla, strawberry, etc.) of 150 customers.\nThe heights of 30 professional basketball players.\n\nThe median of five numbers is 8 and the mean is 10. If four of the numbers are 3,7,8, and 20, what is the fifth number? (Hint: Use the definitions of median and mean.)\nDescribe a situation in which the trimmed mean would be preferred over both the mean and the median.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\na)Ordering the commuting times yields 10,12,12,15,16,17,18,18,18,20,22,90. With 12 observations, the median is the average of the 6th and 7th values: \\((17 + 18)/2 = 17.5\\) minutes. b)The mean is \\((10 + 12 + 12 + 15 + 16 + 17 + 18 + 18 + 18 + 20 + 22 + 90)/12 = 268/12 \\approx 22.33\\) minutes. The 90‑minute commute pulls the mean upward by about 5minutes compared to the median. c)For summarizing what most commuters experience, report the median (17.5minutes). It better represents the typical commute and is not inflated by the one long commute.\na)Mean or median. Test scores on a bounded 0–100 scale often form a roughly bell‑shaped distribution; either mean or median can represent typical performance. The mode might be less stable because exact scores can vary. b)Mode. Favorite flavors are categorical; reporting the most popular flavor (mode) is more meaningful than attempting to average flavors. c)Mean or median. Heights are quantitative; the mean or median conveys typical height. The mode is less useful because exact duplicates are rare and height is nearly continuous.\nWith five numbers, the median is the third when ordered. Because the median is 8, the third value (when the numbers are sorted) must be 8. The numbers we know are 3,7,8,20 and an unknown \\(x\\). After sorting them, the median (middle value) must be 8, so the sorted list must be 3,7,8,\\(x\\),20 (if \\(x \\leq 20\\)) or 3,7,8,20,\\(x\\) (if \\(x \\geq 20\\)). To satisfy a mean of 10, the sum of all five numbers is 5×10=50. The known sum is 3+7+8+20=38, so \\(x = 12\\). Check the ordering: 3,7,8,12,20 has median 8 and mean 10. Thus the fifth number is 12.\nTrimmed means are useful when you expect a few extreme observations in both tails but still want to use most of the data. For example, in judging gymnastics or diving, a panel of judges gives scores; to guard against unusually high or low scores (perhaps due to bias), competitions often drop the highest and lowest score and average the rest. A trimmed mean removes these extremes, producing a fairer overall score.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Describing Data with Numbers</span>"
    ]
  },
  {
    "objectID": "04.html#measures-of-variability-range-iqr-standard-deviation",
    "href": "04.html#measures-of-variability-range-iqr-standard-deviation",
    "title": "4  Describing Data with Numbers",
    "section": "4.2 Measures of Variability (Range, IQR, Standard Deviation)",
    "text": "4.2 Measures of Variability (Range, IQR, Standard Deviation)\n\n“In statistics, variation is the name of the game – without it there would be nothing to explain.” – David Salsburg\n\nGuiding question: How do we measure variability?\nIf the center tells us what’s typical, measures of variability tell us how spread out the data are around that typical value. Two datasets can share the same mean yet be very different if one is tightly clustered and the other is widely scattered. We need tools to describe that spread and to compare it across groups.\n\nWhy variability matters\nUnderstanding variability helps us judge reliability. If commute times vary widely from day to day, a typical time doesn’t provide much certainty. In manufacturing, small variability in part dimensions indicates a stable process; large variability suggests something is wrong. In inferential statistics, measures of variability (especially variance and standard deviation) play a central role in quantifying uncertainty.\nWe’ll look at four common summaries: the range, the interquartile range (IQR), the variance, and the standard deviation. Each captures spread in a slightly different way.\n\nThe range\nThe range is the simplest measure of spread: it’s the difference between the largest and smallest values.\n\\[\n\\text{Range} = \\max(x) - \\min(x).\n\\]\nThe range is easy to compute and understand, but it depends only on two data points—making it very sensitive to outliers. If one observation is extreme, the range may exaggerate typical variability.\nTwo data sets can have the same range and be vastly different with respect to data variation.\nFor Example, Consider the data set A: \\[1, 3, 5, 6, 8, 9, 10, 15\\] and data set B: \\[1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 15\\]\nBoth data sets have the same range but are very different in how the data are spread out.\n\n\nQuartiles and the interquartile range (IQR)\nTo get a sense of spread that isn’t distorted by extremes, we can look at quantiles. The first quartile (\\(Q_1\\)) is the value such that about 25% of the data are below it; the third quartile (\\(Q_3\\)) has about 75% below it. The interquartile range (IQR) is the difference between these two quartiles:\n\\[\n\\text{IQR} = Q_3 - Q_1.\n\\]\nBecause it uses the middle half of the data, the IQR is a resistant measure of spread. It tells us how wide the “middle 50%” is, ignoring the tails. The IQR is central to boxplots and to identifying outliers (as we’ll see in the next section).\n\n\nVariance and standard deviation\nFor a different measure of variability, we can calculate the distance and direction from the mean for each individual measurement. This is known as the deviation of the measurement. \\[\n\\text{deviation} = x - \\bar x\n\\]\nUsing these deviations, we can construct a more sensitive (as compared to the range) measure of variation.\n\n\nExample:\nData set 1:\n1, 2, 3, 4, 5\nData set 2:\n2, 3, 3, 3, 4\nBoth datasets have a mean of 3.\nThe deviations for data set 1 are: \\[\n\\begin{align*}\n    (1-3), (2-3), (3-3), (4-3), (5-3) \\Longrightarrow -2, -1, 0, 1, 2\n\\end{align*}\n\\]\nThe deviations for data set 2 are: \\[\n\\begin{align*}\n    (2-3), (3-3), (3-3), (3-3), (4-3) \\Longrightarrow -1, 0, 0, 0, 1\n\\end{align*}\n\\]\nWhat information do these deviations contain?\nIf they tend to be large in magnitude, as in data set 1, the data are spread out, or highly variable.\nIf the deviations are mostly small, as in data set 2, the data are clustered around the mean, \\(\\bar x\\) , and therefore do not exhibit much variability.\nThe next step is to condense the information in these distances into a single numerical measure of variability.\nWhile not just average these values?\nYou see from the two example datasets above that some of the values are below the mean making the deviation negative. Other values are above the mean making the deviation positive. The negative deviations cancel out the positive deviations when you sum them up. In fact, the sum of deviations of values from the mean will always be zero.\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\n\\[\n\\begin{align*}\n    \\frac{1}{n}\\sum^n_{i=1}(x_i - \\bar x) &= {\\frac{1}{n}\\sum^n_{i=1}x_i - \\frac{1}{n}\\sum^n_{i=1}\\bar x}\\\\\n    &{ = \\bar x - \\frac{1}{n}n\\bar x}\\\\\n    & {= \\bar x - \\bar x}\\\\\n    &{ = 0}\n\\end{align*}\n\\]\n\n\n\nSo this won’t work. What we can do instead is square the deviations. By doing this and averaging4 them out, we will obtain what is known as the sample variance.\n\\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\n\\]\nBecause the variance uses the square of the units of measurement for the original data, its square root is easier to interpret. This is called the sample standard deviation.\n\\[\n\\begin{align*}\ns =& \\sqrt{s^2}\\\\\n=& \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2}\n\\end{align*}\n\\]\nSquaring the deviations ensures that positive and negative deviations don’t cancel out and gives more weight to larger deviations. Taking the square root returns the measure to the original units of the data. Standard deviation answers the question: on average, how far do observations fall from the mean? Unlike the IQR, the standard deviation is sensitive to outliers, because every deviation is squared. F\n\n\nIllustration: comparing spreads\nConsider two small sets of exam scores. GroupA scores are tightly clustered, and GroupB scores vary widely even though their means are similar.\n\n\n\nExam scores for the two groups\n\n\nGroupA\n70\n72\n73\n74\n75\n76\n78\n\n\nGroupB\n60\n65\n70\n75\n80\n85\n90\n\n\n\n\n\nWe’ll compute the range, IQR, and standard deviation for each group.\n\n\n\nComparing measures of spread for two groups\n\n\n\nmean\nrange\nIQR\nsd\n\n\n\n\nGroup_A\n74\n8\n3\n2.65\n\n\nGroup_B\n75\n30\n15\n10.80\n\n\n\n\n\nIn this example, both groups have means around the mid‑70s (mean≈74). However, GroupA’s range is 8 points and its IQR is 3 points, while GroupB’s range is 30 points and its IQR is 15 points. The standard deviation for GroupB is more than triple that of GroupA. Even without a graph you can see that GroupB’s scores are much more spread out.\n\n\n\nWorking in JMP Pro 17\nTo examine variability in JMP:\n\nRange and IQR. In the Distribution platform, click the red triangle ▶ next to the variable name and select Save Quantiles or Quantiles to see \\(Q_1\\), \\(Q_2\\), \\(Q_3\\) and compute the IQR (\\(Q_3 - Q_1\\)). The range is visible from the minimum and maximum shown in the summary.\nVariance and standard deviation. These appear automatically in the “Summary Statistics” section of the Distribution report. Standard deviation is labeled “Std Dev,” and variance is its square.\nMultiple groups. To compare groups, use Analyze→Fit Y by X with a continuous response and a categorical factor. The side‑by‑side boxplots display medians, quartiles, and potential outliers, and the “Means and Std Dev” table summarizes each group’s mean and standard deviation.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nRange\nThe difference between the maximum and minimum values in a dataset; very sensitive to outliers.\n\n\nQuartile\nA value that divides ordered data into four equal parts; \\(Q_1\\) is the first quartile (25% mark) and \\(Q_3\\) is the third quartile (75% mark).\n\n\nInterquartile range (IQR)\nThe difference \\(Q_3 - Q_1\\); measures the spread of the middle half of the data; resistant to outliers.\n\n\nVariance\nThe average squared deviation from the mean; units are squared.\n\n\nStandard deviation\nThe square root of variance; a typical distance from the mean; sensitive to outliers but commonly used in many statistical formulas.\n\n\n\n\n\nCheck your understanding\n\nFor the exam score data above (GroupA and GroupB), interpret the differences in standard deviation. Which group has more variability and why?\nA dataset of weekly hours spent exercising (in hours) for eight people is 2, 3, 3, 4, 4, 4, 5, 15.\n\nCompute the range, IQR, and standard deviation.\nHow does the outlier of 15 hours affect each measure of variability?\nIf you wanted to describe the spread for most people in this group, which measure would you report? Explain.\n\nExplain in your own words why we square deviations when computing variance. What would go wrong if we didn’t square them?\nTwo companies have average delivery times of 3 days. Company A has a standard deviation of 0.5 days, while Company B has a standard deviation of 2 days. Describe what this tells you about customer experiences with each company.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nGroupB has a standard deviation of about 10.808 points, whereas GroupA’s standard deviation is about 2.65 points (as computed in the example). The larger standard deviation means GroupB’s scores are more spread out around the mean—students in GroupB vary widely in performance compared to the tight clustering of GroupA.\na)The ordered data are 2,3,3,4,4,4,5,15. The range is 15−2=13. \\(Q_1\\) is halfway between the 2nd and 3rd observations (3 and 3), so \\(Q_1 = 3\\); \\(Q_3\\) is halfway between the 6th and 7th observations (4 and 5), so \\(Q_3 = 4.5\\). The IQR is 4.5−3=1.5 hours. The mean is \\((2+3+3+4+4+4+5+15)/8 = 40/8 = 5\\) hours. The standard deviation (using \\(n-1=7\\) in the denominator) is about 4.14 hours. b) The outlier of 15 hours has a huge impact on the range (it becomes 13) and the standard deviation (4.14), both of which rise substantially. The IQR remains 1.5 because quartiles ignore the extreme values—so the IQR is more robust. c) To describe the spread for most people, the IQR is most appropriate because it captures the middle 50% of values and is not distorted by the one extreme exerciser.\nIf we summed deviations from the mean without squaring them, positive and negative deviations would cancel out, giving zero. Squaring each deviation ensures that all contributions to variability are positive and that larger deviations are weighted more heavily, which reflects their greater contribution to the overall spread.\nAlthough the average delivery time is the same for both companies, Company A’s small standard deviation means most deliveries take close to 3 days—customers can expect consistent service. Company B’s larger standard deviation indicates delivery times vary widely; some packages may arrive much sooner or much later than 3 days. Customers may perceive Company A as more reliable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Describing Data with Numbers</span>"
    ]
  },
  {
    "objectID": "04.html#identifying-outliers",
    "href": "04.html#identifying-outliers",
    "title": "4  Describing Data with Numbers",
    "section": "4.3 Identifying Outliers",
    "text": "4.3 Identifying Outliers\n\n“Would I rather be feared or loved? Easy, both. I want people to be afraid of how much they love me.” – Michael Scott\n\nGuiding question: How do we detect and handle outliers?\nOutliers are observations that lie far away from the rest of the data. They can arise from data entry mistakes, instrument malfunctions, rare but real variation, or genuinely interesting cases. Outliers deserve attention: sometimes they reveal important phenomena; other times they signal problems. Our goal is to detect outliers systematically and decide how to treat them thoughtfully.\n\nWhat counts as an outlier?\nTwo common approaches to flag potential outliers are the IQR rule and the z‑score rule.\n\nThe 1.5×IQR rule\nFor a dataset with quartiles \\(Q_1\\) and \\(Q_3\\), the interquartile range is \\(\\text{IQR} = Q_3 - Q_1\\). Points are considered potential outliers if they fall beyond \\(1.5\\times \\text{IQR}\\) from the quartiles. These boundaries are sometimes referred to as the fences\n\nLower fence: \\(Q_1 - 1.5 \\times \\text{IQR}\\)\nUpper fence: \\(Q_3 + 1.5 \\times \\text{IQR}\\)\n\nAny observation below the lower fence or above the upper fence is flagged as a potential outlier. Because the IQR ignores the tails of the distribution, this rule is resistant—it’s less affected by the very points it’s trying to find.\n\n\nThe z‑score rule\nWe can standardize each value of a dataset using the sample mean \\(\\bar{x}\\) and standard deviation \\(s\\) to compute a z‑score\n\\[\nz = \\frac{x_i - \\bar{x}}{s}\n\\]\nFor many approximately bell-shaped distributions,\n\nabout 68% of the observations fall within 1 standard deviation of the mean, that is, between \\(\\bar x -s\\) and \\(\\bar x +s\\) (denoted \\(\\bar x \\pm s\\))\nabout 95% of the observations fall within 2 standard deviations of the mean \\((\\bar x \\pm 2s)\\)\nAll or nearly all (about 99.7%) observations fall within 3 standard deviations of the mean \\((\\bar x \\pm 3s)\\)\n\nThese results are known as the Empirical Rule.\nExample: Female heights\nMany human physical characteristics have bell-shaped distributions. Let’s explore height. The file heights.jmp contains heights of of 261 female students at the University of Georgia. Below is a histogram and summary statistics for these heights. Note that a height of 92 inches was not included below.\n\nThe histogram has approximately a bell shape. From the summary statistics, the mean and median are close, about 65 inches, which reflects an approximately symmetric distribution. The empirical rule is applicable. - About 68% of the observations fall between \\[\n\\begin{align*}\n\\bar x \\pm s &= 65.3 \\pm 3.0\\\\\n& = {(62.3, 68.3)}\n\\end{align*}\n\\]\n\nAbout 95% of the observations fall between \\[\n\\begin{align*}\n\\bar x \\pm 2s &= 65.3 \\pm 2(3.0)\\\\\n& = {(59.3, 71.3)}\n\\end{align*}\n\\]\nAbout 99.7% of the observations fall between \\[\n\\begin{align*}\n\\bar x \\pm 3s &= 65.3 \\pm 3(3.0)\\\\\n& = {(56.3, 74.3)}\n\\end{align*}\n\\]\n\nWe can examine the dataset and count how many observations fall in these intervals:\n\n187 observations, 72%, fall within (62.3, 68.3).\n248 observations, 95%, fall within (59.3, 71.3).\n258 observations, 99%, fall within (56.3, 74.3).\n\nIn summary, the percentages predicted by the empirical rule are near the actual ones.\n\n\nUsing the Empirical Rule to find outliers\nBased on the Empirical Rule, an observation with \\(|z| &gt; 3\\) might be considered an outlier. However, this method is not resistant—outliers inflate \\(s\\), which can mask their own detection—and it should be used only when the distribution is close to bell-shaped.\n\n\n\nWhat to do with outliers\nIdentifying an outlier is not the same as discarding it. Consider these steps:\n\nInvestigate. Check the raw data entry, measurement units, and collection context. Could the value be a typo or a mis-recorded unit? Is there an instrument calibration issue?\nUnderstand context. Sometimes extreme values are genuine and carry important information (e.g., rare species sightings, extreme weather events). Domain knowledge helps decide whether to keep them.\nReport and compare. It’s often useful to perform analyses both with and without the outlier to see how much it influences results. If conclusions change dramatically, acknowledge this in reporting.\nUse robust methods. When outliers are present and valid, robust statistics like the median, IQR, trimmed mean, or non‑parametric tests help mitigate their influence.\n\n\nIllustration: detecting outliers with both rules\nConsider a small dataset of reaction times (in milliseconds): 240, 250, 255, 260, 265, 270, 275, 280, 450. One value appears much larger than the others. We’ll compute the IQR fences, z‑scores, and use a boxplot to visualize.\n\n\n\nOutlier detection using IQR fences and z-scores\n\n\nValue\nz_score\nOutlier_IQR\nOutlier_z_score\n\n\n\n\n240\n-0.67\nFALSE\nFALSE\n\n\n250\n-0.51\nFALSE\nFALSE\n\n\n255\n-0.43\nFALSE\nFALSE\n\n\n260\n-0.36\nFALSE\nFALSE\n\n\n265\n-0.28\nFALSE\nFALSE\n\n\n270\n-0.20\nFALSE\nFALSE\n\n\n275\n-0.12\nFALSE\nFALSE\n\n\n280\n-0.04\nFALSE\nFALSE\n\n\n450\n2.62\nTRUE\nFALSE\n\n\n\n\n\nIn this example, the quartiles are \\(Q_1\\)≈255 and \\(Q_3\\)≈275, so \\(\\text{IQR} =\\) 20. The fences are 225 and 305. The value 450 is above the upper fence and is flagged as an outlier by the IQR rule. Its z‑score is about 2.62, which does not exceed 3, thus it is not marked an outlier by the z‑score rule. Because the sample is small and clearly skewed by the outlier, the IQR rule is the more reliable of the two.\n\n\n\nWorking in JMP Pro 17\n\nBoxplots and outliers. Use Graph→GraphBuilder to create a boxplot. JMP plots points beyond 1.5×IQR as small circles by default. Hover over a point to see its value; right‑click to exclude or include it.\nDistribution platform. In Analyze→Distribution, the boxplot includes the fences and outlier points. You can click the red triangle ▶ and choose Nonparametric→Outlier Box Plot for additional options.\nExploring z‑scores. To compute z‑scores in JMP, create a new column and use Formula→Standardize. Then filter rows with |z|&gt;3 to flag potential outliers.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nIQR rule\nPoints more than 1.5×IQR below \\(Q_1\\) or above \\(Q_3\\) are flagged as potential outliers\n\n\nLower/Upper fence\nThresholds used in the IQR rule: \\(Q_1 - 1.5 \\times \\text{IQR}\\)\n\n\nz‑score\nStandardized value \\(z = (x - \\bar{x})/s\\); indicates how many standard deviations a point is from the mean.\n\n\nEmpirical Rule\nIn a normal distribution, about 68% of values lie within 1 standard deviation of the mean, 95% within 2 standard deviations, and 99.7% within 3 standard deviations.\n\n\n\n\n\nCheck your understanding\n\nFor the dataset \\(3, 4, 5, 6, 7, 8, 9, 100\\):\n\nCompute the quartiles, IQR, and the 1.5×IQR fences.\nIdentify any outliers using the IQR rule.\nCompute z‑scores for each observation. Which values, if any, have |z|&gt;3?\n\nUsing GroupB’s exam scores from earlier (60, 65, 70, 75, 80, 85, 90), apply the IQR rule to determine whether any scores are outliers. Explain why or why not.\nSuppose heights of adult men follow a normal distribution with mean 70 inches and standard deviation 3 inches. Using z‑scores, what heights would be considered extreme outliers (|z|&gt;3)?\nDescribe two different actions you might take after flagging an outlier in a dataset. When would each be appropriate?\nExplain why the z‑score rule may fail to detect outliers in skewed distributions.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\na)Ordering the data gives 3, 4, 5, 6, 7, 8, 9, 100. There are 8 values, so the median is the average of the 4th and 5th values: \\((6 + 7)/2 = 6.5\\). The lower half (3,4,5,6) has median \\((4 + 5)/2 = 4.5\\), so \\(Q_1 = 4.5\\). The upper half (7,8,9,100) has median \\((8 + 9)/2 = 8.5\\), so \\(Q_3 = 8.5\\). The IQR is 8.5−4.5=4. The fences are \\(Q_1 - 1.5\\times\\text{IQR} = 4.5 - 6 = -1.5\\) and \\(Q_3 + 1.5\\times\\text{IQR} = 8.5 + 6 = 14.5\\). b) Values below −1.5 or above 14.5 would be considered outliers. In this dataset, 100 &gt; 14.5, so 100 is an outlier. No value is below −1.5, so there are no lower outliers. c) The mean is \\((3+4+5+6+7+8+9+100)/8 = 142/8 = 17.75\\) and the standard deviation is about 31.63. The z‑score for 100 is \\((100 - 17.75)/31.63 \\approx 2.59\\). Because |2.59| is less than 3, the z‑score rule does not flag 100 as an outlier. This illustrates how outliers inflate the standard deviation and hide themselves.\nGroupB’s scores (60, 65, 70, 75, 80, 85, 90) have \\(Q_1 = 67.5\\) and \\(Q_3 = 82.5\\) (see the previous section’s example). The IQR is 15. The fences are 67.5−22.5=45 and 82.5+22.5=105. All scores lie between 45 and 105, so there are no outliers under the IQR rule.\nHeights more than 3 standard deviations from the mean are below 70−9=61inches or above 70+9=79inches. Thus, heights less than 61inches or greater than 79inches would be considered extreme outliers by the z‑score rule.\nPossible actions include: (1) Correcting or removing the outlier if it is due to a data entry or measurement error. This is appropriate when you have verified the value is incorrect. (2) Reporting results with and without the outlier and using robust methods. This is appropriate when the outlier is genuine but influential; you can present both analyses or use methods less sensitive to extreme values.\nIn skewed distributions, the standard deviation can be inflated by the skew, so the threshold of 3 standard deviations may be too high. Outliers can inflate \\(s\\) and thus produce smaller z‑scores than expected. The z‑score rule also assumes symmetry; in a skewed distribution the tail on one side is longer, so using a symmetric cutoff like |z| &gt; 3 may miss unusual observations in the longer tail.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Describing Data with Numbers</span>"
    ]
  },
  {
    "objectID": "04.html#the-fivenumber-summary-and-boxplots",
    "href": "04.html#the-fivenumber-summary-and-boxplots",
    "title": "4  Describing Data with Numbers",
    "section": "4.4 The Five‑Number Summary and Boxplots",
    "text": "4.4 The Five‑Number Summary and Boxplots\n\n“If it’s green or wriggles, it’s biology. If it stinks, it’s chemistry. If it doesn’t work, it’s physics or engineering. If it’s green and wiggles and stinks and still doesn’t work, it’s psychology. If it’s incomprehensible, it’s mathematics. If it puts you to sleep, it’s statistics.” - Anonymous (in Journal of the South African Institute of Mining and Metallurgy (1978))\n\nGuiding question: How can we use the five‑number summary and boxplots?\nThe five‑number summary distills a numeric dataset into five key values:\n\nthe minimum,\nthe first quartile (\\(Q_1\\)),\nthe median (\\(Q_2\\)),\nthe third quartile (\\(Q_3\\)), and\nthe maximum.\n\nThese five numbers give you a sense of where the data begin, where most of them lie, and how far they stretch. A boxplot (also called a box‑and‑whisker plot) is a graphical display of the five‑number summary that makes comparisons across groups quick and intuitive.\n\nThe five numbers\nThere are slightly different conventions for how to find the median, \\(Q_1\\), and \\(Q_3\\), especially when working with even number of observations.However, most software (including JMP) uses consistent algorithms. The main idea is that about 25% of observations fall below \\(Q_1\\), about 50% fall below the median, and about 75% fall below \\(Q_3\\). We will let JMP determine these values for use instead of determining them by hand.\n\n\nThe boxplot: a visual summary\nA boxplot displays the five‑number summary in a simple diagram:\n\nThe box spans from \\(Q_1\\) to \\(Q_3\\); its length is the interquartile range (IQR). A line inside the box marks the median.\nWhiskers extend from the box to the smallest and largest data points that are not considered outliers. One common convention draws whiskers out to the last observation that is within 1.5×IQR below \\(Q_1\\) or above \\(Q_3\\). Points beyond these whiskers are plotted individually (usually as points) as potential outliers.\n\nBecause a boxplot is based on quartiles, it is a resistant summary: the box and median are not unduly affected by a handful of extremes. At a glance you can see the central 50% of the data (the box), the typical range (the whiskers), whether the distribution is symmetric or skewed (by looking at the position of the median and the lengths of the whiskers), and whether there are any unusual points.\nBoxplots shine when comparing distributions across several groups; you can line up multiple boxes in one plot and quickly judge differences in center, spread, and skewness.\n\nIllustration: boxplots for two groups\nLet’s revisit the exam score groups from the previous section. We’ll compute their five‑number summaries and draw side‑by‑side boxplots.\n\n\n\nFive-number summaries for Groups A and B\n\n\nStatistic\nGroup_A\nGroup_B\n\n\n\n\nMin\n70.0\n60.0\n\n\nQ1\n72.5\n67.5\n\n\nMedian\n74.0\n75.0\n\n\nQ3\n75.5\n82.5\n\n\nMax\n78.0\n90.0\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the table you can see that Group A’s scores run from 70 to 78 with a median of 74, while Group B’s scores run from 60 to 90 with a median of 75. The much longer whiskers and broader box of Group B reveal its greater variability.\n\n\n\nOutlier identification\nA data point is often flagged as a potential outlier if it falls more than 1.5×IQR below \\(Q_1\\) or above \\(Q_3\\). These points warrant a closer look: they could be genuine but rare observations or mistakes in data entry or measurement.\n\n\nWorking in JMP Pro 17\nYou can build boxplots quickly in JMP:\n\nOpen Graph→Graph Builder and drag your variable to the Y axis. Drag a grouping variable (if any) to the X axis. From the gallery of plots, select the boxplot icon (a box with whiskers) to overlay a boxplot on the graph. JMP uses the 1.5×IQR rule to draw whiskers and will display outliers as separate points.\nThe five‑number summary is visible in the Analyze→Distribution output. Click the red triangle ▶ next to the variable and choose Nonparametric→Quantiles to list the quartiles. JMP labels the minimum and maximum directly in the summary.\nRight‑click on the boxplot to toggle “Show Outliers” or adjust the whisker definition if needed.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nFive‑number summary\nThe five values \\(\\min, Q_1, Q_2, Q_3, \\max\\) summarizing the distribution’s location and spread.\n\n\nBoxplot\nA graphical display of the five‑number summary: a box from \\(Q_1\\) to \\(Q_3\\) with a line at the median, and whiskers extending to the min/max (or to 1.5×IQR).\n\n\nWhisker\nThe line segment extending from the box to the smallest or largest non‑outlier value.\n\n\n\n\n\n\n\n\n\nCheck your understanding\n\nConsider the dataset \\(5, 7, 8, 10, 12, 15, 18, 20, 25\\).\n\nCompute the five‑number summary (min, \\(Q_1\\), median, \\(Q_3\\), max).\nConstruct the corresponding boxplot. Describe its shape. Is the distribution symmetric or skewed?\n\nA boxplot shows \\(Q_1 = 60\\), \\(Q_2 = 75\\), \\(Q_3 = 90\\) with minimum 50 and maximum 120. The right whisker is much longer than the left whisker, and there are several points plotted individually above 105.\n\nWhat does the long right whisker indicate about the distribution’s skewness?\nUsing the 1.5×IQR rule, which observations (approximately) would be considered outliers?\n\nFor the dataset \\(40, 42, 44, 45, 48, 50, 51, 52, 90\\):\n\nCompute the five‑number summary.\nCalculate the IQR and determine the lower and upper fences for outliers (using 1.5×IQR).\nIdentify any outliers and explain how they would appear on a boxplot.\n\nExplain why boxplots are particularly useful for comparing multiple groups on the same scale. What aspects of the distributions can you quickly compare?\nDescribe at least one limitation of boxplots. When might they hide important features of the data?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\na)Ordering the data gives 5,7,8,10,12,15,18,20,25. The minimum is 5 and the maximum is 25. The median (the fifth value) is 12. The lower half (5,7,8,10) has median \\((7 + 8)/2 = 7.5\\), so \\(Q_1 = 7.5\\). The upper half (15,18,20,25) has median \\((18 + 20)/2 = 19\\), so \\(Q_3 = 19\\). The five‑number summary is (5, 7.5, 12, 19, 25). b) The box would span from 7.5 to 19 with a line at 12. The whiskers would extend to 5 and 25. The distribution is slightly right‑skewed because the upper whisker (19→25) is longer than the lower whisker (7.5→5).\na)A long right whisker suggests a right‑skewed distribution—there are some relatively high values pulling the upper tail outward. b) The IQR is \\(Q_3 - Q_1 = 90 - 60 = 30\\). The upper fence is \\(Q_3 + 1.5\\times\\text{IQR} = 90 + 45 = 135\\). Any observation above 135 would be considered an outlier. Since the maximum is 120 (below 135), the individual points above 105 are unusual but do not meet the 1.5×IQR rule; they are “mild” outliers by some conventions. If the maximum were above 135, those values would be flagged as outliers.\na)The ordered data are 40,42,44,45,48,50,51,52,90. The minimum is 40 and the maximum is 90. The median (the fifth value) is 48. The lower half (40,42,44,45) has median \\((42 + 44)/2 = 43\\), so \\(Q_1 = 43\\). The upper half (50,51,52,90) has median \\((51 + 52)/2 = 51.5\\), so \\(Q_3 = 51.5\\). The five‑number summary is (40, 43, 48, 51.5, 90). b) The IQR is 51.5−43=8.5. The lower fence is \\(Q_1 - 1.5 \\times \\text{IQR} = 43 - 12.75 = 30.25\\). The upper fence is \\(Q_3 + 1.5 \\times \\text{IQR} = 51.5 + 12.75 = 64.25\\). c) Any observation below 30.25 or above 64.25 would be flagged as an outlier. In this dataset, only 90 exceeds 64.25, so 90 is an outlier. On a boxplot, the upper whisker would extend to 52 (the largest non‑outlier), and the value 90 would appear as a separate point above the whisker.\nBoxplots align multiple groups on the same axis, allowing you to compare medians, the spread of the middle half (box length), the overall range (whisker length), and skewness (relative whisker lengths) at a glance. This makes it easy to see which group has a higher central tendency, more variability, or more extreme values.\nBoxplots summarize distributions succinctly but omit details like multimodality or the exact shape of the tails. Two very different distributions can share the same five‑number summary. When the sample size is small or when you need to see the full distribution (e.g., to spot bimodality), a histogram or dotplot may be more informative.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Describing Data with Numbers</span>"
    ]
  },
  {
    "objectID": "04.html#choosing-appropriate-summary-statistics",
    "href": "04.html#choosing-appropriate-summary-statistics",
    "title": "4  Describing Data with Numbers",
    "section": "4.5 Choosing Appropriate Summary Statistics",
    "text": "4.5 Choosing Appropriate Summary Statistics\n\n“It is not known how many office robberies occur every second because there is no Wikipedia entry for office robbery statistics.” -Michael Scott\n\nGuiding question: Which summary statistics fit different data types?\nNot all variables are created equal, and not all summaries make sense for every variable. The right summary statistic depends on both the type of variable (categorical, ordinal, or quantitative) and the shape of its distribution (symmetric, skewed, or subject to outliers). In this section we review guidelines for matching variables to appropriate measures of center and spread.\n\nCategorical and binary data\nFor categorical variables (nominal or qualitative), arithmetic operations have no meaning. The appropriate summaries are counts, proportions, and the mode (most common category). For example, summarizing people’s favorite ice‑cream flavors would involve tallying how many chose chocolate, vanilla, strawberry, and so on. Reporting the “mean flavor” makes no sense.\nA binary variable is a categorical variable with exactly two categories (e.g., pass/fail, disease/no disease). We often code the categories as 1 and 0. In that case, the proportion of 1’s is simply the mean of the coded variable. For example, if 30 out of 100 students pass an exam, the pass rate is 0.30; the mean of the 0/1 indicator is also 0.30.\n\n\nOrdinal data\nOrdinal variables have ordered categories (e.g., Likert ratings such as “Strongly disagree,” “Disagree,” “Neutral,” “Agree,” “Strongly agree”). Differences between levels are not necessarily equal, so computing a mean can be misleading—does “Agree” minus “Neutral” equal “Neutral” minus “Disagree”? A safer approach is to report mode, perhaps along with distribution of responses (a bar chart or counts).\n\n\nQuantitative data: symmetric vs. skewed\nFor quantitative variables, we choose summaries based on the shape of the distribution and the presence of outliers:\n\nSymmetric, no outliers. When the distribution is approximately symmetric and free of extreme values, the mean and standard deviation (or variance) are informative. They convey the central value and typical deviation. Example: exam scores on a test where most students scored near the middle.\nSkewed or with outliers. When the distribution is skewed or contains outliers, the median and IQR are better descriptors of the typical value and spread. The trimmed mean (e.g., a 10% trimmed mean) can also be used to down weight extremes while still reflecting most of the data. Example: household incomes, which often have a long right tail; the mean income is much larger than the median, whereas the median better represents what most households earn.\n\nQuantitative counts (discrete variables) can be summarized with means or medians, but because counts often have skewed distributions (e.g., number of doctor visits), it’s wise to check shape and consider medians.\n\nIllustration: incomes vs. test scores vs. Likert responses\nThe following compares summary statistics for three small datasets: incomes (right‑skewed), test scores (symmetric), and Likert ratings (ordinal on a 1–5 scale). We compute the mean, median, IQR, standard deviation, and mode where appropriate.\nHere are the values:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIncomes\n25\n28\n30\n32\n35\n40\n45\n50\n120\n\n\nTest_scores\n70\n72\n73\n75\n76\n77\n78\n80\n82\n\n\nLikert_ratings\nStrongly disagree\nDisagree\nDisagree\nNeutral\nNeutral\nNeutral\nAgree\nAgree\nStrongly agree\n\n\n\n\n\n\n\n\nSummary statistics for different data types\n\n\nDataset\nMean\nMedian\nIQR\nSD\nMode\n\n\n\n\nIncomes\n45.00\n35\n15\n29.28\n25\n\n\nTest Scores\n75.89\n76\n5\n3.86\n70\n\n\nLikert Ratings\nNA\nNA\nNA\nNA\nNeutral\n\n\n\n\n\nInterpretation:\n\nIncomes. The mean income ($45k) is much higher than the median ($35k) because of the 120k outlier. The IQR (15k) shows that the middle half of incomes lies between $30k and $45k. For skewed data like this, report the median and IQR rather than the mean and standard deviation.\nTest scores. The mean (75.89) and median (76) are similar, and the distribution is narrow (SD = 4.03). Here, the mean and standard deviation are appropriate summaries.\nLikert ratings. The mode is 3. Reporting the distribution of responses conveys the typical sentiment without implying equal spacing.\n\n\n\n\nWorking in JMP Pro 17\nJMP encourages choosing appropriate summaries by making you specify each column’s modeling type (Continuous, Nominal, Ordinal):\n\nNominal variables are summarized with counts and proportions. The Distribution platform shows frequency tables and bar charts; the mode is evident from the highest bar. Means are not computed.\nOrdinal variables can display medians and quartiles. In the Distribution platform, the Quantiles option lists \\(Q_1\\), median, and \\(Q_3\\). A Box Plot shows the spread without relying on means.\nContinuous variables get means and standard deviations by default. If the distribution is skewed, use the red triangle ▶ Transform options (e.g., Log) or the Nonparametric menu to request median and IQR. You can also save robust statistics using Save Summaries.\n\nIn JMP’s Graph Builder, dragging a categorical variable to the X axis and a continuous variable to Y will create boxplots by default, highlighting medians and quartiles rather than means.\n\n\nCheck your understanding\n\nA health‑care researcher records the number of emergency room visits last year for 50 individuals. The data are right‑skewed, with most people having 0–2 visits and a few having 5 or more. Which measure of center and spread would you report? Explain why.\nA survey asks respondents to rate their satisfaction on a 1–5 Likert scale (1 = Very dissatisfied, 5 = Very satisfied). For 300 respondents, the sample mean is 3.9, the median is 4, and the mode is 4. Which statistic(s) would you use to describe typical satisfaction? Why might the mean be misleading?\nFor a dataset of annual household incomes in a city, the mean is $65,000, the median is $45,000, the standard deviation is $40,000, and the IQR is $20,000.\n\nWhat does the large gap between the mean and the median tell you about the distribution?\nWhich measure(s) of center and spread would you report? Justify your choice.\n\nA manufacturer tracks the number of defective items per batch (a count variable). The mean number of defects per batch is 2.3 and the median is 1. What does this tell you about the distribution? Which summary statistic is more informative here?\nExplain why it doesn’t make sense to compute a mean for ZIP codes or product serial numbers. How would you summarize such data instead?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nCounts of emergency room visits are discrete and skewed to the right. The median number of visits and the IQR are appropriate measures. The mean would be pulled upward by a few high counts and would not represent the typical person.\nFor Likert responses, the mode (4) best describe typical satisfaction. A mean of 3.9 suggests “almost 4,” but interpreting fractional satisfaction levels can be misleading because the scale’s steps are not necessarily evenly spaced. Reporting the distribution of responses (e.g., “60% rated 4 or 5”) provides context.\na)The mean ($65k) is much higher than the median ($45k), indicating a right‑skewed distribution with some very high incomes. The large standard deviation ($40k) and moderate IQR ($20k) reinforce that incomes vary widely, especially in the upper tail. b) Report the median and IQR as primary summaries because they better reflect the typical household and are less affected by very high incomes. You could also mention the mean and SD to provide a sense of the overall level and variability, but note their sensitivity to outliers.\nA mean of 2.3 and a median of 1 suggest that most batches have 1 or 2 defects, but some batches have many more, creating a right‑skewed distribution. The median is more informative because it reflects what happens in a typical batch. The mean is inflated by the few batches with many defects.\nZIP codes and serial numbers are identifiers, not quantities. Arithmetic on them (like averaging) has no meaning. They should be summarized by counts or modes if you need to know which codes occur most often. Other summaries (like proportions of orders by ZIP code) can answer meaningful questions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Describing Data with Numbers</span>"
    ]
  },
  {
    "objectID": "04.html#footnotes",
    "href": "04.html#footnotes",
    "title": "4  Describing Data with Numbers",
    "section": "",
    "text": "Small, H. (2020). Nightingale’s overlooked Scutari statistics. Significance, 17(6), 28-33.↩︎\nSome regiments have been combined.↩︎\nThe tails of a distribution are the parts that are for the lowest values and the highest values.↩︎\nWhy do we divide by \\(n-1\\) in the variance and standard deviation instead of \\(n\\)? We said that the variance was an average of the \\(n\\) squared deviations, so should we not divide by \\(n\\)? Basically it is because the deviations have only \\(n - 1\\) pieces of information about variability: That is, \\(n - 1\\) of the deviations determine the last one, because the deviations sum to 0. For example, suppose we haven \\(n= 2\\) observations and the first observation has deviation \\((x - \\bar x) = 5\\). Then the second observation must have deviation \\((x - \\bar x) = -5\\) because the deviations must add to 0. With \\(n = 2\\), there’s only \\(n - 1 = 1\\) nonredundant piece of information about variability. And with \\(n = 1\\), the standard deviation is undefined because with only one observation, it’s impossible to get a sense of how much the data vary.↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Describing Data with Numbers</span>"
    ]
  },
  {
    "objectID": "05.html",
    "href": "05.html",
    "title": "5  Probability Concepts",
    "section": "",
    "text": "5.1 Basic Probability Rules\nGuiding question: What is probability, really?\nWhen people say “there’s a 70 percent chance of rain” or that a team has “even odds” of winning, they’re using probability to express how likely an outcome seems. In statistics, we give that vague notion a precise meaning. A probability is a number between 0 and 1 that measures how often we expect an event to occur in the long run.\nA probability of 0 means the event can never happen, a probability of 1 means it is certain, and values in between describe varying degrees of likelihood.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability Concepts</span>"
    ]
  },
  {
    "objectID": "05.html#sec-05_01",
    "href": "05.html#sec-05_01",
    "title": "5  Probability Concepts",
    "section": "",
    "text": "“The theory of probabilities is at bottom nothing but common sense reduced to calculus.” – Pierre‑Simon Laplace\n\n\n\n\n\nExperiments, outcomes and sample spaces\nProbability always starts with an experiment—a repeatable situation whose result is not predetermined.\nFlipping a coin, rolling a die or drawing a card are familiar examples. Each trial produces an outcome, and the set of all possible outcomes is called the sample space.\nWe often use \\(S\\) to denote the sample space. For instance, when you flip one fair coin, the sample space is \\(S=\\{\\text{H},\\text{T}\\}\\); when you roll a six‑sided die, \\(S=\\{1,2,3,4,5,6\\}\\).\nIt is essential to describe the sample space clearly because the probability of any event is computed relative to it. In practice, the sample space can be finite (like the faces of a die) or infinite (like all possible real‑valued measurements of temperature). Identifying \\(S\\) helps us keep track of what outcomes are possible before we assign probabilities.\n\n\nEvents and interpretations of probability\nAn event is any subset of the sample space. Events can be simple (containing one outcome) or compound (containing several outcomes).\nLet \\(A\\) be the event that a die roll is even. Then \\(A=\\{2,4,6\\}\\), which contains three outcomes from the sample space \\(S=\\{1,2,3,4,5,6\\}\\).\nThe probability of an event \\(A\\), written \\(P(A)\\), quantifies how likely \\(A\\) is to occur.\nWhen the outcomes in \\(S\\) are equally likely (as with a fair die or fair coin), the classical probability of \\(A\\) is\n\\[\nP(A) = \\frac{\\text{number of outcomes in }A}{\\text{number of outcomes in }S}\\,.\n\\]\nIn the die example above, there are three even numbers out of six, so \\(P(A)=3/6=0.5\\). This agrees with our intuition that even and odd numbers are equally likely on a fair die.\nProbability has several interpretations. In the frequentist view, \\(P(A)\\) is the long‑run relative frequency of \\(A\\)—the fraction of times \\(A\\) occurs if we repeat the experiment many times under identical conditions. For example, the probability of flipping a head on a fair coin is 0.5 because, in the long run, about half of the flips will show heads.\n\nExample: rolling a six-sided die\nSuppose we rolled a six-sided die 20 times. Using the idea of equally likely outcomes, the probability of rolling a six is \\[\n\\begin{align}\nP(6)=&\\frac{1}{6}\\\\\n\\approx& 0.167\n\\end{align}\n\\] Let’s see how many times we get a six when rolling the die 20 times.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\n6\n1\n5\n5\n4\n3\n2\n2\n6\n1\n2\n3\n3\n1\n1\n3\n5\n3\n2\n\n\n\n\n\nWe see that we rolled a six 2 out of the 20 times. The relative frequency is thus \\[\n\\frac{\\text{number of sixes}}{\\text{number of rolls}}=\\frac{2}{20}=0.1\n\\]\nLet’s look at a bar chart for all of the rolls:\n\n\n\n\n\n\n\n\n\nLet’s now roll the die 1,000 times:\n\n\n\n\n\n\n\n\n\nThe number of times we rolled each value is a little more even. Focusing only on six, we have \\[\n\\frac{\\text{number of sixes}}{\\text{number of rolls}}=\\frac{160}{1000}=0.16\n\\] which is close to the theoretical proportion of \\(1/6\\approx 0.167\\)\nWe can keep increasing the number of times we roll the die and we will see the number of each value “even out” across all values:\n\n\n\n\n\n\n\n\n\nEach value will converge to the theoretical value of \\(1/6\\).\n\n\nExample: Flipping a coin\nSuppose we flip a coin many times and track the proportion of heads over time. Early on, the proportion jumps around; as the number of flips grows, it tends to settle near the true probability of 0.5.\n\n\n\n\n\n\n\n\n\n\n\n\nLaw of Large Numbers\nIn 1689, the Swiss mathematician Jacob Bernoulli proved that as the number of trials increases, the proportion of occurrences of any given outcome approaches a particular number (such as 1/6 or 1/2) in the long run.\nTo show this, he assumed that the outcome of any one trial does not depend on the outcome of any other trial. Bernoulli’s result is known as the Law of Large Numbers.\nWe will interpret the probability of an outcome to represent long-run results. This is the frequentist approach to probability.\n\n\n\n\n\n\nFrequentist and Subjective probability\n\n\n\n\n\nProbability can be defined in different ways:\n\nFrequentist (long‑run relative frequency). For repeatable experiments, \\(P(A)\\) is the limit of the proportion of times \\(A\\) occurs in a large number of trials.\nSubjective. Probability reflects a personal belief based on available information; two people may assign different probabilities to the same event.\n\nNo matter how you interpret it, probability values lie between 0 and 1 and follow certain algebraic rules.\n\n\n\n\nComplements and the complement rule\nIf \\(A\\) is an event, the complement of \\(A\\) (denoted \\(A^c\\)) consists of all outcomes in \\(S\\) that are not in \\(A\\). In everyday terms, either \\(A\\) happens or it doesn’t, so the probabilities of \\(A\\) and its complement must sum to 1. This leads to the complement rule:\n\\[\nP(A^c) = 1 - P(A)\n\\]\nThe complement rule is especially handy for “at least one” problems. Suppose you toss a fair coin three times and want the probability of getting at least one head. Let \\(A\\) be the event “at least one head” and \\(A^c\\) be “no heads” (all tails). There are eight possible equally likely outcomes in the sample space \\[\nS=\\left\\{HHH, HHT, HTH, HTT, THH, THT, TTH, TTT\\right\\}\n\\]\nThere is only one outcome with no heads—TTT—so \\(P(A^c)=1/8\\).\nBy the complement rule, \\(P(A)=1-1/8=7/8\\).\n\n\nUnions, intersections and the addition rule\nWhen you combine events, you need to think about unions and intersections. The union of two events \\(A\\) and \\(B\\) (written \\(A\\cup B\\)) contains outcomes that are in \\(A\\) or in \\(B\\) (or in both).\nThe intersection of \\(A\\) and \\(B\\) (written \\(A\\cap B\\)) contains outcomes that are in both \\(A\\) and \\(B\\). The probability of the union is given by the general additive rule:\n\\[\nP(A\\cup B) = P(A) + P(B) - P(A\\cap B)\\,.\n\\]\n\n\nExample: Additive rule\nImagine a survey of 200 students about their beverage preferences:\n\n120 students like coffee.\n90 students like tea.\n60 students like both coffee and tea.\n\nDefine event \\(C\\) = “student likes coffee” and event \\(T\\) = “student likes tea.” Then\n\n\\(P(C) = 120/200 = 0.60\\)\n\\(P(T) = 90/200 = 0.45\\)\n\\(P(C\\cap T) = 60/200 = 0.30\\)\n\nThe probability that a randomly chosen student likes either coffee or tea (or both) is\n\\[\n\\begin{align*}\nP(C\\cup T) =& P(C) + P(T) - P(C\\cap T)\\\\\n           =& 0.60 + 0.45 - 0.30\\\\\n           =& 0.75\n\\end{align*}\n\\]\nSo 75% of students like at least one of the two beverages.\nNotice that this formula counts how many like coffee (\\(P(C)\\)). Then it adds how many like tea (\\(P(T)\\)). When these two are added together, the students who like both coffee and tea are added twice: once in \\(P(C)\\) and again in \\(P(T)\\). Therefore, those students must be subtracted off one time. That is why we subtract off the intersection \\(P(C\\cap T)\\).\n\n\n\nMutually Exlcusive\nIf \\(A\\) and \\(B\\) are mutually exclusive (also called disjoint)—meaning they have no outcomes in common—then \\(P(A\\cap B)=0\\) and the rule simplifies to \\(P(A\\cup B)=P(A)+P(B)\\). A simple example involves rolling a die. Let \\(A\\) be “rolling a 2” and \\(B\\) be “rolling an odd number.” These events are disjoint because 2 is even, so \\[\nP(A\\cup B)=P(\\text{2})+P(\\text{odd})=1/6+3/6=2/3\n\\]\n\nExample: Hospital patients\nHospital records show that 12% of all patients are admitted for surgical treatment, 16% are admitted for obstetrics, and 2% receive both obstetrics and surgical treatment.\nIf a new patient is admitted to the hospital, what is the probability that the patient will be admitted for surgery, for obstetrics, or for both?\nLet \\(A=\\{\\text{A patient admitted to the hospital receives surgical treatment.}\\}\\)\nand \\(B=\\{\\text{A patient admitted to the hospital receives obstetrics treatment.}\\}\\)\nWe have \\[\nP(A) = 0.12\\qquad P(B) = 0.16\\qquad P(A\\cap B) = 0.02\n\\] Since some patients receives both surgical and obstetrics treatments, these events are not mutually exclusive. The general addition rule gives\n\\[\n\\begin{align*}\nP(A\\cup B) &= P(A) +  P(B) -P(A\\cap B) \\\\\n& = 0.12+.016-.02\\\\\n& = 0.26\n\\end{align*}\n\\]\nThus, 26% of all patients admitted to the hospital receive either surgical treatment, obstetrics treatment, or both.\n\n\n\nProbility from contingency tables\nGiven a contingency table where the data is displayed by two categorical variables, we can estimate the probabilities by using the frequencies in the cells.\nThe table below involves diagnosing pulmonary embolism (PE) using a D‑dimer blood test. In a group of 10,000 patients suspected of PE, a perfusion scan was used as the gold‑standard diagnostic. Of the 1000 individuals who actually had PE, the D‑dimer test was positive in 700 cases (and negative in 300); of the 9000 individuals without PE, the test was negative in 7700 cases (and positive in 1300). Thus 2000 people had a positive D‑dimer test result, but only 700 of them truly had PE.\n\n\n\n\n\n\n\n\n\nPulmonary‐embolism status\nD‑dimer positive\nD‑dimer negative\nTotal\n\n\n\n\nPulmonary embolism present\n700\n300\n1000\n\n\nPulmonary embolism absent\n1300\n7700\n9000\n\n\nTotal\n2000\n8000\n10000\n\n\n\nWe can estimate probabilities by taking the frequencies in the cells and dividing by the total number of patients (10,000). For example, the probability that a patient had a pulmonary embolism is \\[\nP(PE)=\\frac{1000}{10000}=0.1\n\\]\nThe probability that a patient pas a pulmonary embolism and is D-dimer positive is \\[\nP(PE\\cap \\text{D-dimer positive})=\\frac{700}{10000}=0.07\n\\]\nFor an union, we can apply the additive rule. For example, the probaility that a patient has a pulmonary embolism or has a D-dimer positive result is \\[\n\\begin{align*}\nP(PE\\cup \\text{D-dimer positive})=&P(PE)+P(\\text{D-dimer positive})-P(PE\\cap \\text{D-dimer positive})\\\\\n=&\\frac{1000}{10000}+\\frac{2000}{10000}-\\frac{700}{10000}\\\\\n=&\\frac{2300}{10000}\\\\\n=& 0.23\n\\end{align*}\n\\]\n\n\nWorking in JMP Pro 17\nAlthough this book emphasizes concepts, it’s worth noting how you can explore probability in JMP. The Distribution Calculator (under Help → Probability Calculator) lets you compute probabilities and quantiles for many distributions without writing formulas. For custom experiments like coin tosses or die rolls, you can simulate data by adding a column of random values: use Rows → Add Rows to create the desired number of trials, add a column with a formula such as Random Integer(1,6) for die rolls, and then use Analyze → Distribution to summarise the results. This empirical approach reinforces the frequentist idea that probabilities emerge from long‑run relative frequencies.\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nProbability\nA number between 0 and 1 measuring how likely an event is to occur.\n\n\nExperiment\nA repeatable process with uncertain outcome (e.g., flipping a coin, rolling a die).\n\n\nOutcome\nA single possible result of an experiment; elements of the sample space.\n\n\nSample space\nThe set of all possible outcomes of an experiment.\n\n\nEvent\nA subset of the sample space; may include one or many outcomes.\n\n\nFrequentist interpretation\nDefines probability as the long‑run relative frequency of an event in repeated trial.\n\n\n\n\n\nCheck your understanding\n\nDescribe the parts of a probability model. In your own words, explain the difference between an outcome, an event and a sample space. Use the experiment of rolling a six‑sided die as an example.\nEqually likely probability. If you roll a fair die, what is the probability of getting a number greater than 4? Explain how you arrive at your answer.\nComplement rule in practice. You toss a fair coin four times. What is the probability of getting at least one tail? Use the complement rule to find the answer.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nParts of a probability model. A sample space is the set of all possible outcomes—for a six‑sided die, \\(S=\\{1,2,3,4,5,6\\}\\). An outcome is one specific element of \\(S\\), such as rolling a 4. An event is any subset of \\(S\\); it might contain one outcome (e.g., \\(\\{6\\}\\), “rolling a six”) or several (e.g., \\(\\{2,4,6\\}\\), “rolling an even number”). We assign probabilities to events based on the rules discussed above.\nProbability of a number greater than 4. The event \\(A\\) of rolling greater than 4 corresponds to outcomes \\(\\{5,6\\}\\). There are two favorable outcomes and six outcomes in total, so by the equally likely events \\(P(A)=2/6=1/3\\).\nAt least one tail in four tosses. Let \\(A\\) be “at least one tail.” The complement \\(A^c\\) is “no tails,” which means getting all heads (HHHH). Since the probability of all heads is \\((1/2)^4=1/16\\), the complement rule gives \\(P(A)=1-1/16=15/16\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability Concepts</span>"
    ]
  },
  {
    "objectID": "05.html#sec-05_02",
    "href": "05.html#sec-05_02",
    "title": "5  Probability Concepts",
    "section": "5.2 Conditional Probability and Independence",
    "text": "5.2 Conditional Probability and Independence\n\n“Is it probable that probability brings certainty?” – Blaise Pascal\n\n\nGuiding question: What does it mean for events to be independent?\n\nOur earlier discussion focused on single events and how to combine them using unions and complements. In many situations, we want to know the probability of an event given that another has already occurred. Thinking this way is essential when events are not isolated but are part of a process or sequence. For example, the probability that a randomly selected person is left‑handed might be different once you know the person is an artist. This leads us to the idea of conditional probability.\n\nConditional probability\nIf \\(A\\) and \\(B\\) are events with \\(P(A)&gt;0\\), the conditional probability of \\(B\\) given \\(A\\), written \\(P(B|A)\\), describes the chance that \\(B\\) occurs assuming that \\(A\\) has occurred or will occur. When we condition on \\(A\\), the sample space shrinks to just the outcomes where \\(A\\) happens. In terms of probabilities, this shrinking translates to the formula \\[\nP(B\\mid A) = \\frac{P(A\\cap B)}{P(A)}\n\\] which says that we compare how often both events occur to how often \\(A\\) occurs at all.\nYou can read \\(P(B|A)\\) as “the probability of \\(B\\) given \\(A\\).”\nRearranging the conditional probability formula yields the multiplication rule for any two events \\(A\\) and \\(B\\): \\[\n\\begin{align*}\nP(A\\cap B) =& P(A)\\,P(B\\mid A)\\\\\n=& P(B)\\,P(A\\mid B)\n\\end{align*}\n\\] This rule tells us how to find the probability that both events occur: multiply the probability of the first event by the conditional probability of the second event given the first.\n\nExample: smoking and cancer\nMany medical researchers have conducted experiments to examine the relationship between cigarette smoking and cancer.\nConsider an individual randomly selected from an adult male population.\nLet \\(A\\) represent the event that the individual smokes, and let \\(A^c\\) denote the complement of \\(A\\) (the event that the individual does not smoke).\nSimilarly, let \\(B\\) represent the event that the individual develops cancer, and let \\(B^c\\) be the complement of that event.\nSuppose a large sample from the population resulted in the proportions below:\n\n\n\n\n\\(B\\)\n\\(B^{c}\\)\n\n\n\n\n\\(A\\)\n.05\n.20\n\n\n\\(A^{c}\\)\n.03\n.72\n\n\n\nUse these probabilities to examine the relationship between smoking and cancer.\nFirst, find the probability that an individual smokes: \\[\n\\begin{align*}\nP(A) =& {P(A\\cap B)+P(A\\cap B^c)}\\\\\n=& {0.05+0.20}\\\\\n=& {0.25}\n\\end{align*}\n\\]\nNow find the probability that an individual doesn’t smoke: \\[\n\\begin{align*}\nP(A^c) =& {P(A^c\\cap B)+P(A^c\\cap B^c)}\\\\\n=& {0.03+0.72}\\\\\n=& {0.75}\n\\end{align*}\n\\]\nLet’s now determine the probability that an individual develops cancer given they are a smoker: \\[\n\\begin{align*}\nP(B\\mid A) =& \\frac{P(A\\cap B)}{P(A)}\\\\\n=& \\frac{0.05}{0.25}\\\\\n=& 0.2\n\\end{align*}\n\\]\nLet’s now determine the probability that an individual develops cancer given they are a smoker: \\[\n\\begin{align*}\nP(B\\mid A^c) =& \\frac{P(A^c\\cap B)}{P(A^c)}\\\\\n=& \\frac{0.03}{0.75}\\\\\n=& 0.04\n\\end{align*}\n\\]\n\n\n\nIndependent and dependent events\nWhen the occurrence of one event does not change the probability of another, we call the events independent.\nFormally, \\(A\\) and \\(B\\) are independent if \\[\nP(B\\mid A)=P(B)\n\\] or equivalently \\[\nP(A\\mid B)=P(A)\n\\] If either holds, then the other does too.\nFor independent events, the multiplication rule simplifies to \\[\nP(A\\cap B)=P(A)\\,P(B)\n\\] If knowing that \\(A\\) has occurred does change the probability of \\(B\\), the events are called dependent.\n\nExample: flipping coins\nThink about tossing a fair coin twice. Let \\(H_1\\) be the event “the first toss is heads” and \\(H_2\\) be “the second toss is heads.” The probability of a head on each toss is 0.5, and the outcome of the first toss does not affect the outcome of the second. Therefore, \\(P(H_2\\mid H_1)=P(H_2)=0.5\\), and the events are independent. Using the multiplication rule, \\[\nP(H_1\\cap H_2) = P(H_1)\\,P(H_2) = 0.5\\times0.5 = 0.25\\\n\\]\n\n\nExample: sampling without replacement\nIndependence can break down when we sample from a finite population without replacement. Suppose a box contains 10 batteries, 4 of which are defective. You draw two batteries at random without replacement. Let \\(D_1\\) be the event “the first battery is defective” and \\(D_2\\) be “the second battery is defective.” The probability the first battery is defective is \\(P(D_1)=4/10=0.4\\). If the first battery is defective, there are now 3 defectives among 9 remaining batteries, so \\(P(D_2\\mid D_1)=3/9=0.333\\). If the first battery is good, there are still 4 defectives among 9 batteries, so \\(P(D_2\\mid D_1^c)=4/9\\approx0.444\\). Because \\(P(D_2)\\) depends on whether \\(D_1\\) happened, the events are dependent. When we sample with replacement, the draws are independent, since after each draw the composition of the box resets to 4 defectives out of 10.\nIn practice, when the sample size is much smaller than the population, the difference between sampling with and without replacement becomes negligible and we can treat the draws as independent.\n\n\n\nMutually Exclusive Events versus Independent Events\nThere is often confusion between the concepts of independent events and disjoint events.\nActually, these are quite different notions, and perhaps this is seen best by comparisons involving conditional probabilities.\nSpecifically, if \\(A\\) and \\(B\\) are mutually exclusive, then \\[\nP(A \\cap B) = 0\n\\] whereas for independent events, we have \\[\nP(A \\cap B)  = P(A) P(B)\n\\]\nIf \\(P(A)&gt;0\\) and \\(P(B)&gt;0\\), then how can \\[\nP(A) P(B) = 0 ?\n\\] This is impossible.\nIn other words, the property of being mutually exclusive involves a very strong form of dependence, because the occurrence of one event mean the other event cannot occur.\nThus, if two events are mutually exclusive, they cannot be independent.\nIf two events are independent, they cannot be mutually exclusive.\n\n\nConditional probability from contingency table\nLet’s go back to the pulmonary embolism example from last section.\nHere is the table again:\n\n\n\n\n\n\n\n\n\nPulmonary‐embolism status\nD‑dimer positive\nD‑dimer negative\nTotal\n\n\n\n\nPulmonary embolism present\n700\n300\n1000\n\n\nPulmonary embolism absent\n1300\n7700\n9000\n\n\nTotal\n2000\n8000\n10000\n\n\n\nSuppose we wanted to estimate the probability of a pulmonary embolism given the patient had a D-dimer positive result. When looking for conditional probability from a contingency table, remember that the conditioned event is all we are focusing on . In this case, the conditioned event is D-dimer positive. There are a total of 2000 patients that are D-dimer positive. Of those 2000 patients, 700 had a pulmonary embolism. Thus, the conditional probability is\n\\[\nP(PE\\mid \\text{D-dimer positive})=\\frac{700}{2000}=0.35\n\\]\n\nWorking in JMP Pro 17\nJMP makes it easy to explore conditional probabilities and independence using contingency tables. Enter your data with one column for each categorical variable (for example, “Major” and “Year”). Under Analyze → Fit Y by X, choose one variable as the response and the other as the explanatory factor. The resulting mosaic or contingency table will show joint counts. Right‑click and choose Row Percents or Column Percents to view conditional proportions. If the row percentages are the same across all columns (and vice versa), the variables are approximately independent; if the percentages differ, there is evidence of dependence.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nConditional probability\nThe probability of one event given that another event has occurred; computed as \\(P(B\\mid A)=P(A\\cap B)/P(A)\\) when \\(P(A)&gt;0\\).\n\n\nMultiplication rule\nA rule that expresses the joint probability of two events: \\(P(A\\cap B)=P(A)\\,P(B\\mid A)=P(B)\\,P(A\\mid B)\\).\n\n\nIndependent events\nEvents \\(A\\) and \\(B\\) for which \\(P(B\\mid A)=P(B)\\) (equivalently \\(P(A\\mid B)=P(A)\\)) and \\(P(A\\cap B)=P(A)P(B)\\).\n\n\n\n\n\nCheck your understanding\n\nCalculus and statistics courses. At a certain college, 40% of students have taken calculus, 30% have taken statistics, and 15% have taken both.\n\nWhat is the probability that a student has taken statistics given they have taken calculus?\n\nAre taking calculus and statistics independent events? Explain your reasoning.\n\nDrawing marbles without replacement. A jar contains 5 red marbles and 3 green marbles. You draw two marbles at random without replacement.\n\nWhat is the probability that both marbles are red?\n\nIf you draw with replacement, what would be the probability of drawing two red marbles?\n\nWhich scenario involves independent events?\n\nPass rates. In a school, 60% of students pass mathematics, 50% pass physics, and 35% pass both subjects.\n\nWhat is \\(P(\\text{Physics} \\mid \\text{Math})\\)?\n\nAre passing mathematics and passing physics independent? Why or why not?\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nCalculus and statistics.\n\nThe conditional probability that a student has taken statistics given they have taken calculus is \\(P(\\text{Stats}\\mid \\text{Calc}) = P(\\text{Stats}\\cap \\text{Calc}) / P(\\text{Calc}) = 0.15/0.40 = 0.375\\).\n\nIf calculus and statistics were independent, we would have \\(P(\\text{Stats}\\cap \\text{Calc}) = P(\\text{Stats})\\,P(\\text{Calc}) = 0.30\\times0.40 = 0.12\\). Because the actual joint probability 0.15 is greater than 0.12, these events are not independent; students who take calculus are more likely to take statistics.\n\nDrawing marbles.\n\nWithout replacement, the probability both marbles are red is \\(P(\\text{first red})\\times P(\\text{second red}\\mid \\text{first red}) = (5/8)\\times (4/7) = 20/56 \\approx 0.357\\).\n\nWith replacement, the probability stays \\((5/8)\\times (5/8) = 25/64 \\approx 0.391\\) because the composition of the jar is reset after each draw.\n\nDrawing with replacement yields independent events. Without replacement the draws are dependent, since the first draw changes the composition for the second.\n\nPass rates.\n\nThe conditional probability that a student passes physics given they pass mathematics is \\(P(\\text{Physics}\\mid \\text{Math}) = 0.35/0.60 \\approx 0.583\\).\n\nFor independence we would need \\(P(\\text{Physics}\\mid \\text{Math}) = P(\\text{Physics}) = 0.50\\). Because \\(0.583\\ne 0.50\\), passing mathematics and passing physics are not independent; students who pass one subject are more likely to pass the other.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability Concepts</span>"
    ]
  },
  {
    "objectID": "05.html#sec-05_03",
    "href": "05.html#sec-05_03",
    "title": "5  Probability Concepts",
    "section": "5.3 Bayes Rule",
    "text": "5.3 Bayes Rule\n\n“Absence of evidence is not evidence of absence.” – Carl Sagan\n\nGuiding question: How can we use probability to assess diagnostic tests?\nWhen you receive a lab report or a rapid test result it is tempting to take the outcome at face value: a positive means you have the disease; a negative means you do not. In reality, every diagnostic test has some chance of making mistakes. To interpret a result responsibly we need to take two things into account:\n\nThe accuracy of the test itself. How often does it correctly flag someone who is sick? How often does it incorrectly flag someone who is healthy? These properties are captured by sensitivity and specificity.\nThe prevalence of the condition. How common is the disease in the population you are testing? This is the prior probability that a random person is infected before considering the test result.\n\nBayes’ Rule is the mathematical tool that combines those two pieces of information. It takes the evidence (the test result) and updates the prior probability to produce a new, posterior probability: the chance the person actually has the disease given the result. Bayes’ Rule is simply a rearrangement of the multiplication rule for probabilities; for events \\(A\\) and \\(B\\) with \\(P(B)&gt;0\\) it says\n\\[\nP(A\\mid B) = \\frac{P(B\\mid A)\\,P(A)}{P(B)}.\n\\]\nIn diagnostic testing we let \\(A\\) be the event “person has the disease” and \\(B\\) be “test is positive.” Then:\n\n\\(P(A)\\) is the prevalence of the disease (our prior belief). This is usually known or estimated by epidemiologists.\n\\(P(B\\mid A)\\) is the sensitivity: the probability the test correctly labels a diseased person as positive. This value is estimated in a clinical setting.\n\\(P(B\\mid A^c)\\) is the false–positive rate, which is \\(1\\) minus the specificity. Also estimated in a clinical setting.\n\\(P(A\\mid B)\\) is the quantity we really want: the probability a person is truly sick given they test positive. This is called the positive predictive value (PPV) of the test.\n\nNotice that Bayes’ Rule also works for a negative result. If \\(T^c\\) denotes a negative test, the probability of being healthy given a negative result, \\(P(A^c\\mid T^c)\\), is the negative predictive value (NPV).\n\nUnderstanding Bayes’ Rule through an example\nTo see how Bayes’ Rule plays out, imagine a disease that affects 1% of the population. A new screening test has 90% sensitivity and 95% specificity. In plain language:\n\nIf you have the disease, there is a 90% chance the test is positive (and a 10% chance of a false negative).\nIf you do not have the disease, there is a 95% chance the test is negative (and a 5% chance of a false positive).\n\nSuppose you go to the clinic and your test comes back positive. What is the probability you actually have the disease? Let \\(D\\) be “has the disease” and \\(T\\) be “test is positive.” We can use the conditional probability rule:\n\\[\nP(D\\mid T)=\\frac{P(T\\cap D)}{P(T)}\n\\] Unfortunately, we do not know \\(P(T\\cap D)\\) or \\(P(T)\\). All we know is\n\n\\(P(T\\mid D)\\), which is the sensitivity\n\\(P(T^c\\mid D^c)\\) which is the specificity. Sometimes we have instead the complement of specificity \\(P(T\\mid D^c)\\).\n\\(P(D)\\) which is the prevalence.\n\nIn terms of our example, we have\n\n\\(P(T\\mid D)=0.90\\)\n\\(P(T^c\\mid D^c)=0.95\\) which implies \\(P(T \\mid D^c)=1-P(T^c\\mid D^c)=0.05\\)\n\\(P(D)=0.01\\)\n\nIn the numerator above, we can use the multiplicative rule to find \\(P(T\\cap D)\\): \\[\nP(T\\cap D)=P(T\\mid D)\\,P(D)\n\\] In the denominator, we can write \\[\nP(T) = P(T\\cap D) + P(T\\cap D^c)\n\\] Now, let’s apply the multiplicative rule to both of these intersections using the information that we do know: \\[\nP(T) = P(T\\mid D)\\,P(D) + P(T\\mid D^c)\\,P(D^c)\n\\]\nLet’s put everything together to give us a formula that has quantities we do know: \\[\nP(D\\mid T)=\\frac{P(T\\mid D)\\,P(D)}{P(T\\mid D)\\,P(D) + P(T\\mid D^c)\\,P(D^c)}\n\\]\nThis equation is known as Bayes’ Rule.\nSubstituting our known values into the equation gives us \\[\n\\begin{align*}\nP(D\\mid T)=&\\frac{P(T\\mid D)\\,P(D)}{P(T\\mid D)\\,P(D) + P(T\\mid D^c)\\,P(D^c)}\\\\\n=&\\frac{(0.90)(0.01)}{(0.90)(0.01) + (0.05)(0.99)}\\\\\n=&0.1538\n\\end{align*}\n\\]\nEven though the test is fairly accurate, a *positive** result only raises the probability of disease to about 0.1538. The reason is that the disease is so rare that most positive tests are false alarms.\n\nA genetic screening example in biology\nBayesian thinking is not confined to clinical medicine. Geneticists often screen organisms for rare traits. Imagine a plant species in which 5% of individuals carry a gene conferring resistance to a particular fungus. A DNA test has 90% sensitivity and 80% specificity. If a randomly selected plant tests positive, how likely is it to possess the resistance gene?\nLet \\(R\\) be “has the resistance gene” and \\(T\\) be “test positive.” Then \\(P(R)=0.05\\), \\(P(T\\mid R)=0.90\\) and \\(P(T\\mid R^c)=0.20\\) (since specificity is 80%). Bayes’ Rule tells us\n\\[\nP(R\\mid T) = \\frac{0.90\\times 0.05}{0.90\\times 0.05 + 0.20\\times 0.95} \\approx 0.191.\n\\]\nSo even a positive test only implies about a 19% chance of carrying the gene. The key message is the same: when a condition is rare, false positives can overwhelm true positives, and Bayes’ Rule quantifies how evidence updates our beliefs.\n\n\n\nWorking in JMP Pro 17\nJMP does not have a dedicated “Bayes’ Rule” button, but you can apply the rule by constructing a contingency table and computing appropriate proportions. To explore the medicine example above in JMP:\n\nCreate a table with one column for the true condition (Diseased vs Healthy) and one column for the test result (Positive vs Negative). Enter the counts from the confusion matrix (for example, 90 true positives, 495 false positives, 9 405 true negatives and 10 false negatives per 10 000 screened).\nChoose Analyze → Fit Y by X and assign True condition to the Y role and Test result to the X role. The resulting contingency table shows the joint counts. Right‑click on the table and select Row Percents or Column Percents to view conditional probabilities.\nYou can then compute the PPV as the percentage in the Diseased row of the Positive column divided by the total percentage in the Positive column, and the NPV as the percentage in the Healthy row of the Negative column divided by the total percentage in the Negative column.\n\nThese steps replicate the Bayes’ Rule calculation without writing formulas. JMP’s probability calculators can also be used for more complex Bayesian analyses in later chapters.\n\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition/Meaning\n\n\n\n\nBayes’ Rule\nFormula to update probabilities in light of new evidence\n\n\nSensitivity\nProbability that a diagnostic test correctly identifies a diseased individual as positive; a high sensitivity means few false negatives.\n\n\nSpecificity\nProbability that a diagnostic test correctly identifies a healthy individual as negative; a high specificity means few false positives.\n\n\nPositive predictive value\nThe probability that a person actually has the condition given a positive test result, \\(P(D\\mid T)\\); depends on sensitivity, specificity and the disease prevalence.\n\n\nNegative predictive value\nThe probability that a person is disease‑free given a negative test result, \\(P(D^c\\mid T^c)\\); often high when the disease is rare and the test has good sensitivity.\n\n\nPrevalence\nThe proportion of individuals in the population who truly have the disease before any testing; the prior belief in Bayes’ Rule.\n\n\n\n\n\n\n\n\n\nCheck your understanding\n\nInterpreting a positive screening result. Suppose 2% of a population has a certain virus. A test for the virus has 95% sensitivity and 90% specificity.\n\nUsing Bayes’ Rule, compute the probability that a randomly selected person actually has the virus given they test positive.\nWhat is the probability that a person who tests negative is truly virus‑free?\n\nUnderstanding predictive values. A small genetic screening test for a mutation has 80% sensitivity and 97% specificity. In the population being tested, 0.5% carry the mutation. Explain why the positive predictive value of this test is quite low, despite its high specificity. How would the predictive value change if the prevalence were higher?\nDesigning a screening program. A veterinary lab develops a new blood test for heartworm disease in dogs. Preliminary studies estimate the prevalence of heartworm in the region at 3%, and the test’s sensitivity and specificity are 92% and 94%, respectively. Explain how you would use Bayes’ Rule to decide whether a positive test result warrants immediate treatment or should be confirmed with a second test. What role does the disease prevalence play in your decision?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nInterpreting a positive screening result.\n\nLet \\(D\\) be “has the virus” and \\(T\\) be “test positive.” We are given \\(P(D)=0.02\\), \\(P(T\\mid D)=0.95\\) and \\(P(T\\mid D^c)=0.10\\) (because specificity 90% implies a 10% false‑positive rate). First compute \\(P(T)=0.95\\times0.02 + 0.10\\times0.98=0.019 + 0.098=0.117\\). Then \\[P(D\\mid T)=\\frac{0.95\\times0.02}{0.117}\\approx0.162.\\] So even with good sensitivity and decent specificity, a positive result only implies about a 16% chance of infection when prevalence is 2%.\nFor a negative result we look at the NPV. The probability of being virus‑free given a negative test is \\[P(D^c\\mid T^c)=\\frac{P(T^c\\mid D^c)\\,P(D^c)}{P(T^c)}.\\] Here \\(P(T^c\\mid D^c)=0.90\\) (the specificity) and \\(P(T^c\\mid D)=1-0.95=0.05\\). Compute \\(P(T^c)=0.05\\times0.02 + 0.90\\times0.98 = 0.019 + 0.882=0.901\\). Then \\[P(D^c\\mid T^c)=\\frac{0.90\\times0.98}{0.901}\\approx0.979.\\] Thus, a negative test is highly reassuring: about 98% of negatives are true negatives.\n\nUnderstanding predictive values. With sensitivity 80%, specificity 97% and prevalence 0.5%, we have \\(P(D)=0.005\\), \\(P(T\\mid D)=0.80\\) and \\(P(T\\mid D^c)=0.03\\). Using Bayes’ Rule, \\[P(D\\mid T)=\\frac{0.80\\times0.005}{0.80\\times0.005 + 0.03\\times0.995} \\approx 0.118.\\] So only about 12% of positive results reflect true mutation carriers. The test is quite specific, but the condition is extremely rare, so the few false positives swamp the handful of true positives. If the prevalence were higher—say 5%—the same calculation would yield \\[P(D\\mid T)=\\frac{0.80\\times0.05}{0.80\\times0.05 + 0.03\\times0.95} \\approx 0.585,\\] meaning a positive result would be far more convincing. The predictive value increases with prevalence.\nDesigning a screening program. Let \\(H\\) be “has heartworm” and \\(T\\) be “test positive.” We are given \\(P(H)=0.03\\), \\(P(T\\mid H)=0.92\\) and \\(P(T\\mid H^c)=0.06\\) (since specificity 94% means a 6% false‑positive rate). Bayes’ Rule tells us \\[P(H\\mid T)=\\frac{0.92\\times0.03}{0.92\\times0.03 + 0.06\\times0.97} \\approx 0.32.\\] A single positive test thus implies roughly a 32% chance of heartworm. Whether that warrants immediate treatment depends on the risks and costs of treating a false positive versus missing a true case. Because the prevalence is relatively low and the PPV modest, many veterinarians would confirm a positive with a second, more specific test or use a confirmatory diagnostic (like an antigen test or imaging). If the disease were more common in the region (higher prevalence), the PPV would rise and clinicians might opt to treat after one positive result. Bayes’ Rule helps quantify this trade‑off: as prevalence increases, positive tests become more trustworthy.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability Concepts</span>"
    ]
  },
  {
    "objectID": "06.html",
    "href": "06.html",
    "title": "6  Discrete Probability Distributions",
    "section": "",
    "text": "6.1 Random Variables\nWith proper methods of gathering data, the values that a variable assumes should be the result of some random phenomenon.\nThese outcomes may not be numeric. In fact, are any outcomes of an random phenomenon really numbers?\nIn reality, numbers are just abstractions that we assign to measurement.\nFor instance, what is weight?\nIt is a force put on an object due to the mass of the object and gravity.\nSo weight is not really a number, however, we assign numerical values as a representation of weight.\nHow about rolling a six-sided die?\nThe sample space contains\nWe have a rule for which we assign \\[\n\\left\\{1, 2, 3, 4, 5, 6\\right\\}\n\\]\nto the possible outcomes of rolling the die.\nThus we have a function whose domain is the sample space and whose range is the set of real numbers.\nThis function is called a random variable.\nLet’s think about it another way: When we talk about chance, we need a language for turning the messy, unpredictable outcomes of an experiment into numbers we can reason about. A random variable does exactly that: it assigns a numerical value to each outcome in the sample space of a random experiment. Think of it as a function that converts “what happened” into a number you can analyze.\nRandom variables come in two broad types:\nA random variable is not the data itself—it describes the process that produces the data. For example:\nMedicine. Suppose a clinic screens 20 patients for a rare side effect of a new drug. Let \\(X\\) denote the number of patients in the sample who experience the side effect. Before the experiment, \\(X\\) could be 0, 1, 2, …, up to 20. Each of those values corresponds to a different pattern of outcomes in the sample. Once the screening is done, we observe a single value of \\(X\\) (say 3), which is a realization of the random variable. Another clinic, under the same conditions, might see a different number of side effects because \\(X\\) is random.\nBiology. In a genetics lab you grow 10 plants from seeds, each of which may carry a mutation that confers resistance to a fungus. Let \\(Y\\) be the number of resistant plants. The possible values of \\(Y\\) are 0 through 10. The distribution of \\(Y\\) depends on the underlying mutation frequency, and we can use the tools of probability to model \\(Y\\) without observing every possible outcome.\nIn both cases, the random variable turns a complicated outcome (many patients or plants, each with its own result) into a single count. That count varies from trial to trial, but its behavior is governed by the probabilities of the underlying experiment.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "06.html#sec-06_01",
    "href": "06.html#sec-06_01",
    "title": "6  Discrete Probability Distributions",
    "section": "",
    "text": "“There are some statistics that I’d like to share with you now, and they are numbers.” - Perd Hapley (Parks and Recreation)\n\n\nGuiding question: What is a random variable, and how does it relate to data?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA discrete random variable has a countable set of possible values. Many of our examples in this course will involve counts, like the number of patients responding to a treatment or the number of mutations in a DNA sample.\nA continuous random variable can take any value on an interval, such as a person’s height or blood pressure. We will encounter continuous variables in the next chapter; in this chapter our focus is on discrete random variables.\n\n\n\n\n\n\nWorking in JMP Pro 17\nIn JMP you can work with discrete random variables by creating a column that represents the outcome of a chance process. For example:\n\nSimulate a random count. To explore the distribution of a count like \\(X\\) or \\(Y\\), use Rows → Add Rows to create a data table with a desired number of simulated trials (say 1000). Add a new column, choose Column → Formula, and use a random function such as Random Binomial(n, p) to generate counts of successes (e.g., the number of side effects among 20 patients with probability \\(p\\) of a side effect per patient). Each row in this column is one realization of your discrete random variable.\nExplore the distribution. Use Analyze → Distribution to see how often each value occurs. The histogram and summary tables show the possible values and their relative frequencies. This empirical distribution will get closer to the true probability distribution as you increase the number of simulated trials.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nrandom variable\nA function that assigns a numeric value to each outcome of a random experiment.\n\n\n\n\n\nCheck your understanding\n\nDescribe in your own words the difference between a random variable and the observed data.\n\nGive an example of a discrete random variable in a medical context and list its possible values.\n\nIs “blood type” (A, B, AB, O) a discrete random variable? Explain why or why not.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nThe random variable is a rule that maps every possible outcome of a random experiment to a number. The observed data are the actual values of that mapping in one experiment. Before you flip 10 coins, the random variable “number of heads” could be 0–10; after you flip, you observe a single number, such as 4.\nLet \\(X\\) be the number of patients out of 15 who respond favorably to a new therapy. The possible values of \\(X\\) are 0,1,2,…,15—one number for each possible count of responders.\nBlood type is categorical rather than numeric. To analyze it as a random variable you would typically convert it to a binary indicator (e.g., 1 if type A, 0 otherwise). The categories A, B, AB and O do not have a numerical order, so blood type on its own is not a discrete random variable in the sense used here.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "06.html#sec-06_02",
    "href": "06.html#sec-06_02",
    "title": "6  Discrete Probability Distributions",
    "section": "6.2 Probability Distributions and Their Properties",
    "text": "6.2 Probability Distributions and Their Properties\n\n“Iacta alea est. (The die is cast.)” - Julius Ceasar\n\n\nGuiding question: What is a probability distribution, and what makes it valid?\n\nOnce we have defined a discrete random variable, the next step is to describe how likely each of its possible values is. A probability distribution (also called a probability mass function) assigns a probability \\(P(X=x)\\) to every value \\(x\\) in the support of the random variable \\(X\\). A valid probability distribution must satisfy two simple but fundamental conditions:\n\nNon‑negativity: For every possible value \\(x\\), the probability satisfies \\(0 \\le P(X=x) \\le 1\\). Probabilities can’t be negative or exceed one.\nSum to one: The probabilities of all possible outcomes add up to one: \\[\n\\sum_{x} P(X=x) = 1\\,.\n\\] This reflects the fact that something in the sample space must occur on each trial.\n\nA probability distribution summarizes the long‑run behavior of a random variable. It tells you how often each value would appear if you repeated the experiment many times. Let’s look at an example.\n\nExample: number of mutated alleles\nSuppose biologists examine individual plants and record the number of copies of a recessive allele they carry. The random variable \\(X\\) takes the values 0, 1, 2 or 3 with the following probabilities:\n\n\n\n\\(x\\)\n0\n1\n2\n3\n\n\n\n\n\\(P(X=x)\\)\n0.50\n0.30\n0.15\n0.05\n\n\n\nEvery probability is between 0 and 1, and the probabilities sum to \\[\n0.50+0.30+0.15+0.05=1\n\\] Thus this table is a valid probability distribution. If you grew a very large number of plants from the same genetic cross, about 50% would carry no copies of the allele, 30% would have one copy, 15% two copies, and 5% three copies.\nCommonly, the probability distribution of a discrete random variable is displayed with a bar chart. As long as the bar chart displays all possible values of the random variable, and the probability of each value, then it displays the probability distribution.\nBelow is a bar chart for the probability distribution in the table above.\n\n\n\n\n\n\n\n\n\nExample: Medical testing In medical testing you often have a random variable that counts the number of positive results in a sample. For instance, if you screen 5 animals for a virus and each has a small probability of being infected, the distribution of \\(Y\\) = “number of infected animals” might look something like this:\n\n\n\n\\(y\\)\n0\n1\n2\n3\n4\n5\n\n\n\n\n\\(P(Y=y)\\)\n0.70\n0.21\n0.07\n0.015\n0.004\n0.001\n\n\n\nAgain, each probability is between 0 and 1 and the sum is 1. The table is more detailed because there are more possible counts, but the principles are the same.\n\n\nValidity checks\nSometimes you will be given a proposed distribution and asked whether it is valid. To check validity:\n\nMake sure all probabilities are non‑negative and at most 1.\nAdd them up. If they sum to 1 (allowing for small rounding error) the distribution is valid; otherwise it is not.\n\nIf a distribution is not valid, you cannot use it until it is corrected or rescaled.\n\n\nWorking in JMP Pro 17\nTo examine a discrete probability distribution in JMP:\n\nEnter the values and probabilities. Create a new data table with one column for the possible values and another for the corresponding probabilities. Be careful that your probabilities add to 1.\nVisualise the distribution. Use Graph → Graph Builder. Drag the value column to the X‑axis and the probability column to the Y‑axis. Choose Bar as the graph type. The resulting bar chart is your probability mass function. You can also label the bars with the probabilities by clicking the red triangle ▸ and selecting Label.\nCheck the sum. You can add a column formula using Column → New Column and formula sum(:Probability) to verify that the probabilities sum to one. JMP will report the sum in a single cell.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nprobability distribution\nA table, graph, or function assigning each possible value of a discrete random variable a probability between 0 and 1.\n\n\nprobability mass function (pmf)\nAnother name for the probability distribution of a discrete random variable.\n\n\n\n\n\nCheck your understanding\n\nConsider a random variable \\(X\\) that takes values 0, 1, 2 with \\(P(X=0)=0.2\\), \\(P(X=1)=0.5\\) and \\(P(X=2)=0.4\\). Is this a valid probability distribution? Explain.\n\nA diagnostic test counts the number of positive samples among three blood samples from the same patient. Suggest a possible probability distribution for the number of positives and describe how you might visualize it.\n\nThe following table describes a proposed distribution for a random variable \\(X\\):\n\n\n\n\\(x\\)\n0\n1\n2\n3\n\n\n\n\n\\(P(X=x)\\)\n0.3\n\\(–0.1\\)\n0.6\n0.2\n\n\n\nIs this distribution valid? Why or why not?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nThe probabilities must sum to 1. Here \\(0.2 + 0.5 + 0.4 = 1.1\\), which is greater than 1, so this is not a valid distribution. One or more probabilities needs to be adjusted or rescaled.\nThe number of positive samples (\\(Y\\)) can be 0, 1, 2 or 3. A plausible distribution might be \\(P(Y=0)=0.70\\), \\(P(Y=1)=0.20\\), \\(P(Y=2)=0.08\\) and \\(P(Y=3)=0.02\\), but the actual numbers depend on the underlying infection probability. To visualize the distribution, enter these values and probabilities into JMP and create a bar chart with categories 0–3 on the X‑axis and probabilities on the Y‑axis.\nA valid distribution cannot have negative probabilities. Because \\(P(Z=1)=-0.1\\) is negative, this table is invalid. You cannot assign negative weight to an outcome. The probabilities also sum to \\(0.3-0.1+0.6+0.2=1.0\\), but the negativity alone makes it invalid.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "06.html#sec-06_03",
    "href": "06.html#sec-06_03",
    "title": "6  Discrete Probability Distributions",
    "section": "6.3 Mean and Standard Deviation of Discrete Distributions",
    "text": "6.3 Mean and Standard Deviation of Discrete Distributions\n\n“The most important questions of life are, for the most part, really only problems of probability.” -Pierre Simon, Marquis de Laplace\n\n\nGuiding question: How do we find the mean and standard deviation of a discrete random variable?\n\nAs stated previously in this course, we want to examine the center, spread, and shape when we have a data distribution. In Chapter 4, we discussed doing this with sample data where we calculate statistics like \\(\\bar{x}\\), \\(s\\), and \\(s^2\\). These are sample statistics. The probability distribution tells us how likely each outcome of \\(X\\) is. Thus, we should view the probability distribution as a representation of the population.\nWe often want numeric values to describe the probability distribution in the same way we used statistics for the sample. Since these numeric values will be describing a population, these will be parameters.\nIt is common (but not always) to denote parameters with lowercase Greek letters. Below are some of the statistics we have discussed and the corresponding parameter.\n\n\n\n\nSampleStatistic\nPopulationParameter\n\n\n\n\nMean\n\\(\\bar{x}\\)\n\\(\\mu\\)\n\n\nStandard Deviation\n\\(s\\)\n\\(\\sigma\\)\n\n\nVariance\n\\(s^2\\)\n\\(\\sigma^2\\)\n\n\n\nThe mean or expected value (denoted \\(E(X)\\)) of a discrete probability distribution is a weighted average: \\[\n\\mu = E(X) = \\sum_x x \\cdot P(X=x)\n\\] You can think of the expected value as the long‑run average of \\(X\\) over many repetitions of the experiment.\n\nExample: Florida lottery\nThe Florida Lottery runs two popular daily games called Pick 3 and Pick 4.\nIn Cash 3, players pay $1 to select three numbers in order, where each number ranges from 0 to 9. If the three numbers selected (e.g., 2–8–4) match exactly the order of the three numbers drawn, the player wins $500. The probability of winning Pick 3 is 0.001.\nPlay 4 is similar to Cash 3, but players must match four numbers (each number ranging from 0 to 9). For a $1 Play 4 ticket (e.g., 3–8–3–0), the player will win $5,000 if the numbers match the order of the four numbers drawn. The probability of winning Pick 4 is 0.0001\nLet \\(X\\) be the random variable representing the amount of money you get for playing Pick 3.\nThe possible values of \\(X\\) is then\nIf you lose: \\(-\\$1\\)\nIf you win: \\(\\$500-\\$1=\\$499\\)\nWhat is the expected amount of money you get for playing this game? \\[\n\\begin{align*}\n    \\mu &= \\sum xP(x)\\\\\\\\\n    &{=-1(0.999)+499(0.001)}\\\\\n    &{=-0.999+.499}\\\\\n    &{=-0.5}\n\\end{align*}\n\\]\nLet \\(X\\) be the random variable representing the amount of money you get for playing Pick 4.\nThe possible values of \\(X\\) is then\nIf you lose: \\(-\\$1\\)\nIf you win: \\(\\$5000-\\$1=\\$4999\\) The possible values of \\(X\\) is then\nWhat is the expected amount of money you get for playing this game? \\[\n\\begin{align*}\n    \\mu &= \\sum xP(x)\\\\\\\\\n    &{=-1(0.9999)+4999(0.0001)}\\\\\n    &{=-0.9999+.4999}\\\\\n    &{=-0.5}\n\\end{align*}\n\\]\n\n\nExample: Life insurance\nSuppose you work for an insurance company and you sell a $10,000 one-year term insurance policy at an annual premium of $290. Actuarial tables show that the probability of death during the next year for a person of your customer’s age, sex, health, etc., is .001. What is the expected gain (amount of money made by the company) for a policy of this type? \\[X=\\text{money made by the company  per policy}\\] If the customer lives: \\(\\$290\\)\nIf the customer dies:\\(\\$290-\\$10,000=-\\$9710\\)\nThe expected gain for a policy of this type: \\[\n\\begin{align*}\n    \\mu &= \\sum xP(x)\\\\\\\\\n    &{=290(0.999)+(-9710)(0.001)}\\\\\n    &{=280}\n\\end{align*}\n\\]\n\n\nVariance\nThe variance of \\(X\\), denoted \\(\\sigma^2\\), measures how spread out \\(X\\) is around its mean. It is defined as \\[\n\\begin{align*}\n\\sigma^2 = \\sum_x (x - \\mu)^2  P(X=x)\n\\end{align*}\n\\]\nIt weights squared deviations from the mean by the probability of each value. The standard deviation \\(\\sigma\\) is simply the square root of the variance. It has the same units as \\(X\\) and describes the typical distance between \\(X\\) and its mean.\n\nExample: allele copies revisited\nUsing the allele copy distribution from the previous section, we can compute the mean and variance. Recall the probabilities:\n\n\n\n\\(x\\)\n0\n1\n2\n3\n\n\n\n\n\\(P(X=x)\\)\n0.50\n0.30\n0.15\n0.05\n\n\n\nThe expected value is\n\\[\n\\begin{align*}\n\\mu_X =& 0 \\cdot 0.50 + 1 \\cdot 0.30 + 2 \\cdot 0.15 + 3 \\cdot 0.05\\\\\n=& 0 + 0.30 + 0.30 + 0.15 \\\\\n=& 0.75\n\\end{align*}\n\\]\nOn average, a plant carries 0.75 copies of the allele. To compute the variance, we first calculate \\((x - \\mu)^2\\) for each \\(x\\):\n\n\n\n\\(x\\)\n\\((x - 0.75)^2\\)\n\\(P(X=x)\\)\nContribution\n\n\n\n\n0\n\\((0 - 0.75)^2 = 0.5625\\)\n0.50\n\\(0.5625 \\times 0.50 = 0.28125\\)\n\n\n1\n\\((1 - 0.75)^2 = 0.0625\\)\n0.30\n\\(0.0625 \\times 0.30 = 0.01875\\)\n\n\n2\n\\((2 - 0.75)^2 = 1.5625\\)\n0.15\n\\(1.5625 \\times 0.15 = 0.234375\\)\n\n\n3\n\\((3 - 0.75)^2 = 5.0625\\)\n0.05\n\\(5.0625 \\times 0.05 = 0.253125\\)\n\n\n\nThe variance is the sum of the contributions: \\[\n\\sigma^2 = 0.28125 + 0.01875 + 0.234375 + 0.253125 = 0.7875\n\\]\nThus the standard deviation is \\[\n\\sigma = \\sqrt{0.7875} \\approx 0.887\n\\]\nThis tells us that individual plants typically differ from the mean of 0.75 allele copies by about 0.89 copies. Remember that the mean and standard deviation summarize the distribution; they do not necessarily correspond to actual observed values (you cannot have 0.75 copies of a gene, but the average over many plants can be fractional).\n\n\n\nWorking in JMP Pro 17\nTo compute the mean and standard deviation of a discrete distribution in JMP:\n\nEnter the distribution. Create a table with columns for the values and their probabilities.\nCalculate the expected value. Add a new column and use Column → Formula to multiply each value by its probability. Then use Tables → Summary to sum that column. The sum is \\(\\mu_X\\).\nCompute the variance and standard deviation. Add a column for \\((x - \\mu)^2 \\times P(X=x)\\) (you can store \\(\\mu_X\\) in a script variable for reuse). Summarize that column to get the variance. Take the square root for the standard deviation. Alternatively, use Distribution on the simulated values (as described in Section 6.1) to see empirical estimates of the mean and standard deviation.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nexpected value\nWeighted average of a random variable: \\(\\sum_x x \\,P(X=x)\\). Another name for population mean.\n\n\n\n\n\nCheck your understanding\n\nA random variable \\(W\\) takes values 0, 1, 2, 3 with \\(P(W=0)=0.1\\), \\(P(W=1)=0.2\\), \\(P(W=2)=0.4\\) and \\(P(W=3)=0.3\\). Compute \\(E(W)\\) and \\(\\sigma_W\\).\n\nExplain in plain language what the standard deviation tells you about a random variable.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nCompute the mean: \\(E(W) = 0(0.1) + 1(0.2) + 2(0.4) + 3(0.3) = 0 + 0.2 + 0.8 + 0.9 = 1.9\\). The variance is \\[\n\\sum (w - 1.9)^2 P(W=w) = (0-1.9)^2\\cdot 0.1 + (1-1.9)^2\\cdot 0.2 + (2-1.9)^2\\cdot 0.4 + (3-1.9)^2\\cdot 0.3.\n\\] Numerically this is \\(3.61(0.1) + 0.81(0.2) + 0.01(0.4) + 1.21(0.3) = 0.361 + 0.162 + 0.004 + 0.363 = 0.890\\). Thus \\(\\sigma = \\sqrt{0.890} \\approx 0.944\\).\nThe standard deviation measures how far the values of a random variable typically fall from their average value. A small standard deviation means the values cluster tightly around the mean; a larger standard deviation means the values are more spread out and vary more from trial to trial.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "06.html#sec-06_04",
    "href": "06.html#sec-06_04",
    "title": "6  Discrete Probability Distributions",
    "section": "6.4 The Binomial Distribution",
    "text": "6.4 The Binomial Distribution\n\n“If people do not believe that mathematics is simple, it is only because they do not realize how complicated life is.” -John Louis von Neumann\n\n\nGuiding question: How does the binomial model real-world “success/failure” data?\n\nMany count variables in medicine and biology arise from a simple process: you perform the same experiment \\(n\\) times, each time there are only two possible outcomes (success or failure), and the probability of success stays the same from trial to trial. The random variable that counts the number of successes in these \\(n\\) trials is said to follow a binomial distribution. We denote it by \\(X \\sim \\mathrm{Bin}(n,p)\\), where:\n\n\\(n\\) is the number of independent trials.\n\\(p\\) is the probability of success on each trial.\n\nFor the binomial model to be appropriate we must have:\n\nA fixed number \\(n\\) of trials.\nEach trial results in a success or failure.\nThe probability \\(p\\) of success is the same on every trial.\nThe trials are independent of each other.\n\n\nExample:\nJohn Doe claims to possess extrasensory perception (ESP). An experiment is conducted in which a person in one room picks one of the integers 1, 2, 3, 4, 5 at random and concentrates on it for one minute. In another room, John Doe identifies the number he believes was picked. The experiment is done with three trials. After the third trial, the random numbers are compared with John Doe’s predictions. Doe got the correct result twice.\nIf John Doe does not actually have ESP and is merely guessing the number, what is the probability that he’d make a correct guess on two of the three trials?\nLet \\(X\\) = number of correct guesses in \\(n = 3\\) trials. Then \\(X = 0, 1, 2, \\text{ or } 3\\).\nLet \\(p\\) denote the probability of a correct guess for a given trial.\nIf Doe is guessing, \\(p = 0.2\\) for Doe’s prediction of one of the five possible integers. Then, \\(1 - p = 0.8\\) is the probability of an incorrect prediction on a given trial.\nDenote the outcome on a given trial by \\(S\\) or \\(F\\), representing success or failure for whether Doe’s guess was correct or not. The table below shows the eight outcomes in the sample space for this experiment. For instance, \\(FSS\\) represents a correct guess on the second and third trials. It also shows their probabilities by using the multiplication rule for independent events.\n\n\n\nThe three ways John Doe could make two correct guesses in three trials are \\(SSF\\), \\(SFS\\), and \\(FSS\\). Each of these has probability equal to \\[\n(0.2)^2(0.8) = 0.032\n\\]\nThe total probability of two correct guesses (note these are mutually exclusive outcomes) is \\[\n\\begin{align*}\nP(SSF\\cup SFS \\cup FSS) = & (0.2)^2(0.8) + (0.2)^2(0.8) + (0.2)^2(0.8)\\\\\n=& 3(0.2)^2(0.8)\\\\\n=& 3(0.032)\\\\\n=& 0.096\n\\end{align*}\n\\]\nWhen the number of trials \\(n\\) is large, it’s tedious to write out all the possible outcomes in the sample space. But there’s a formula you can use to find binomial probabilities for any \\(n\\).\n\n\nProbabilities for a Binomial Distribution\nDenote the probability of success on a trial by \\(p\\). For \\(n\\) independent trials, the probability of \\(x\\) successes equals \\[\nP(x) = \\binom{n}{x}p^x(1-p)^{n-x}, \\qquad x=0, 1, 2, \\ldots, n\n\\] where \\[\n\\binom{n}{x}=\\frac{n!}{x!(n-x)!}\n\\]\nLet’s use this formula to find the probability that Doe would get only one correct in the previous section: \\[\n\\begin{align*}\n    P(1) &= \\frac{3!}{1!(3-1)!}0.2^1(1-0.2)^{3-1}\\\\\\\\\n   & {=\\frac{6}{1}0.2(0.8)^2}\\\\\n   &{ =.3840}\n\\end{align*}\n\\]\n\n\nCheck to See If Binomial Conditions Apply\nBefore you use the binomial distribution, check that its three conditions apply. These are\n\nbinary data (success or failure),\nthe same probability of success for each trial (denoted by \\(p\\)), and\na fixed number \\(n\\) of independent trials.\n\nOne scenario where the assumptions do not apply is when sampling from a small population without replacement. When this occurs, the assumption of independence does not hold since the probability of success will change after each trial.\nGuideline: Population and Sample Sizes to Use the Binomial For sampling \\(n\\) separate subjects from a population (that is, sampling without replacement), the exact probability distribution of the number of successes is too complex to discuss here, but the binomial distribution approximates it well when \\(n\\) is less than 10% of the population size. In practice, sample sizes are usually small compared to population sizes, and this guideline is satisfied.\n\n\nMean and Standard Deviation of the Binomial Distribution\nRecall that the formula for the expected value of a discrete random variable is \\[\nE(X) = \\mu = \\sum xP(x)\n\\]\nIf we substitute in the binomial formula for \\(P(x)\\) in the formula for expected value we get1 \\[\n\\begin{align*}\nE(X) &= \\mu = \\sum_{x=0}^n x \\frac{n!}{x!(n-x)!}p^x(1-p)^{n-x}\\\\\n& {= np}\n\\end{align*}\n\\]\nSo if we know the random variable is binomial, the expected value is just \\(E(X) =np\\).\nThe same thing can be done for the variance. If we know the random variable is binomial, the variance and standard deviation are \\[\n{\\sigma^2=npq}\\qquad\\qquad{\\sigma=\\sqrt{npq}}\n\\]\n\nExample: vaccine side effects\nFor a vaccine trial: each of 10 volunteers has a 0.2 probability of experiencing a side effect. Let \\(X \\sim \\mathrm{Bin}(10, 0.2)\\) be the number of side effects. The probability of exactly \\(x\\) side effects is \\[\nP(X = x) = \\binom{10}{x} 0.2^x 0.8^{10 - x}.\n\\] For instance, the probability of exactly 3 participants experiencing side effects is \\[\n\\begin{align*}\nP(X=3) =& \\binom{10}{3} (0.2)^3 (0.8)^{7}\\\\\n=& 120 \\times 0.008 \\times 0.2097152\\\\\n\\approx& 0.2013\n\\end{align*}\n\\] The expected number of side effects is \\[\nE(X)=10\\times 0.2=2\n\\] and the standard deviation is \\[\n\\sigma = \\sqrt{10 \\times 0.2 \\times 0.8}\\approx 1.265\n\\]\nA bar chart of the distribution shows that most of the probability mass is centered around 2:\n\n\n\n\n\n\n\n\n\n\n\nExample: gene carriers\nSuppose a particular allele is present in 10% of a plant population. If you examine 12 randomly selected plants, the number \\(Y\\) of plants carrying the allele follows \\(\\mathrm{Bin}(n=12, p=0.10)\\).\nThe probability of exactly \\(x\\) carriers is \\[\n\\binom{12}{x} 0.1^x 0.9^{12-x}\n\\]\nThe expected number of carriers is \\(12 \\times 0.10 = 1.2\\) and the standard deviation is \\(\\sqrt{12 \\times 0.10 \\times 0.90} \\approx 1.039\\).\nThe probability of at least one carrier is \\[\n1 - P(Y=0) = 1 - (0.9)^{12} \\approx 0.717\n\\] meaning there is a 71.7% chance of finding at least one resistant plant in 12.\n\n\nCounting positive results\nBiologists often perform assays where each sample has a probability \\(p\\) of yielding a positive result. If you test \\(n\\) independent samples, the number of positives follows a binomial distribution. For example, if you test 6 fruit flies for a particular gene with presence rate 0.3, the distribution of \\(X \\sim \\mathrm{Bin}(6, 0.3)\\) is:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(x\\)\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(P(X=x)\\)\n\\((0.7)^6\\)\n\\(6(0.3)(0.7)^5\\)\n\\(15(0.3^2)(0.7^4)\\)\n…\n…\n…\n\\((0.3)^6\\)\n\n\n\nThe probabilities can be computed by hand or with software.\n\n\n\nWorking in JMP Pro 17\nJMP has built‑in tools for the binomial distribution:\n\nProbability Calculator. Choose Help → Probability Calculator, select Binomial as the distribution, and specify \\(n\\) and \\(p\\). You can then compute probabilities for specific values (e.g., \\(P(X=3)\\)), cumulative probabilities (e.g., \\(P(X \\le 3)\\)), and tail probabilities (e.g., \\(P(X \\ge 5)\\)). The calculator displays both numerical results and a dynamic bar chart.\nSimulate binomial counts. Add a new column and set its formula to Random Binomial(n, p). Generate many rows to see the empirical distribution, and analyse it with Distribution. This is a good way to build intuition for binomial variability.\nExpected value and standard deviation. JMP’s probability calculator reports the mean and standard deviation for the chosen \\(n\\) and \\(p\\). You can also compute them manually using formula columns (n*p and sqrt(n*p*(1-p))).\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nbinomial distribution\nThe distribution of the number of successes in \\(n\\) independent trials with success probability \\(p\\).\n\n\nbinomial pmf\n\\(P(X=x) = \\binom{n}{x} p^x (1-p)^{n-x}\\) for \\(x=0,\\dots,n\\).\n\n\n\n\n\nCheck your understanding\n\nAn antibiotic cures a bacterial infection in 75 % of cases. In a study of 8 independent patients, let \\(X\\) be the number of patients cured. Compute:\n\n\\(P(X=6)\\).\n\n\\(P(X\\le 4)\\).\n\n\\(E(X)\\) and \\(\\sigma\\)\n\nA gene occurs in 15% of the population. You sample 10 individuals at random. What is the probability that exactly 2 individuals carry the gene? What is the probability that at least one individual carries the gene?\n\nA researcher flips a biased coin (probability of heads is 0.6) 5 times. Does the number of heads follow a binomial distribution? Why or why not?\n\nDescribe a real‑world scenario where the binomial model would not be appropriate even though the outcome is a count of successes. Explain which assumption is violated.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nHere \\(X \\sim \\mathrm{Bin}(n=8,p=0.75)\\). a) \\(P(X=6) = \\binom{8}{6} 0.75^6 0.25^2 = 28 \\times 0.1779785 \\times 0.0625 \\approx 0.311\\). b) \\(P(X\\le 4) = \\sum_{x=0}^4 \\binom{8}{x} 0.75^x 0.25^{8-x} \\approx 0.0081\\) (you can compute this with a calculator or software). c) The mean is \\(E(X)=8\\times 0.75=6\\) and the standard deviation is \\(\\sigma_X=\\sqrt{8\\times 0.75 \\times 0.25}=\\sqrt{1.5}\\approx 1.225\\).\n\nLet \\(Y \\sim \\mathrm{Bin}(10,0.15)\\). Then \\(P(Y=2) = \\binom{10}{2} (0.15)^2 (0.85)^8 \\approx 45 \\times 0.0225 \\times 0.27249 \\approx 0.276\\). The probability of at least one carrier is \\(1 - P(Y=0) = 1 - 0.85^{10} \\approx 1 - 0.1969 = 0.8031\\).\n\nYes. Each flip is a trial with two outcomes (heads or tails), the probability of heads is the same (0.6) on each flip, and the flips are assumed independent. Therefore the number of heads in 5 flips follows \\(\\mathrm{Bin}(5,0.6)\\).\n\nSuppose we sample 10 patients from a small village where tuberculosis is contagious and individuals tend to be exposed through one another. Let \\(X\\) be the number of infected patients. Even if each individual infection has some probability \\(p\\), the infections are not independent: once one person is infected, the chance that their neighbor is infected rises. This dependence violates the independent‑trials assumption of the binomial model, so \\(X\\) would not follow a binomial distribution.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "06.html#footnotes",
    "href": "06.html#footnotes",
    "title": "6  Discrete Probability Distributions",
    "section": "",
    "text": "We will not prove this formula in this course. The proof does requires some algebra “tricks”.↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "07.html",
    "href": "07.html",
    "title": "7  Continuous Probability Distributions",
    "section": "",
    "text": "7.1 Continuous Probability Distributions and Their Properties\nSo far, we has focused on discrete outcomes: counts of patients, number of mutated alleles and so on. In those settings we could list the possible values, assign a probability to each one, and check that the probabilities summed to one. Many measurements in medicine and biology, however, can take any value within a range rather than a handful of distinct values. A person’s height could be 170.23 cm or 170.231 cm; the concentration of a hormone in blood plasma might be 2.7 or 2.701 ng/mL. When a random quantity can assume infinitely many values on an interval we call it a continuous random variable.\nBecause there are infinitely many possible values, we cannot find the probability that a continuous random variable takes any exact value. Instead of assigning probabilities to single points, we assign probabilities to intervals: the chance that a drug’s plasma concentration is between 2.5 and 3.5 ng/mL, for example.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "07.html#sec-07_01",
    "href": "07.html#sec-07_01",
    "title": "7  Continuous Probability Distributions",
    "section": "",
    "text": "“Statistics is the grammar of science.” – Karl Pearson\n\n\nGuiding question: How do probability distributions work for continuous random variables?\n\n\n\n\nProbability Distribution for Continuous Random Variables\nRecall in Chapter 3, we discussed histograms in which the width of the bars is some interval of values and the height is either the frequency or relative frequency of the observations that fall in that interval. We could examine the relative frequency of the data that fall between any two value by adding the relative frequencies of the bars in that interval. For example, suppose we are looking for the relative frequency of the data shaded in the histogram below.\n\nThus, the area of these bars is the relative frequency in the interval of interest. In Chapter 5, we stated that we are using the relative frequency interpretation of probability. Therefore, the area shaded in the histogram will estimate the probability of the random variable being in that interval.\nNow, let’s think of all of the possible data in the population. In this case, we can shrink the width of the bars to however small we wish. As we let the bar widths shrink to zero, then we end up with a smooth curve like below.\n\nThe smooth curve (probability distribution of a continuous random variable) is denoted by the symbol \\(f(x)\\) and is often called the probability density function (pdf). We can still view the area of the shaded region as the probability.\nBecause the area under the pdf represents probability, then by definition we have \\[\nP(X=x)=0\n\\] In other words, we assign a probability of zero at a point. This happens since there is no area under the curve at a point.\nWe do find the probability of a continuous random variable in an interval: \\[\nP(a&lt; X&lt; b)\n\\] How do we do this? We find the area under the curve between \\(a\\) and \\(b\\). We find the area under the curve by taking the integral \\[\nP(a &lt; X &lt; b) = \\int_a^b f(x) \\; dx.\n\\]\nThe cumulative distribution function (cdf) of \\(X\\), denoted \\(F(x)\\), gives the probability that \\(X\\) is less than or equal to \\(x\\); it is the area under the density to the left of \\(x\\).\nIn summary:\nThe probability distribution of a continuous random variable \\(X\\) * is represented by a smooth curve * the curve is called the probability density function (pdf) * the probability \\(P(a&lt;X&lt;b)=P(a\\le X\\le b)\\) is the area under the curve between \\(a\\) and \\(b\\) * the cumulative distribution function (cdf) give the area to the left of some value: \\(F(x)=P(X\\le x)\\)\n\n\nExamples from biology and medicine\n\nDrug metabolism. After an oral dose, the amount of a drug in the bloodstream rises and then falls over time. If we pick a random patient and record their peak plasma concentration, that value could be any number within a physiological range. The probability that the peak is exactly 3.000 µg/mL is zero; but we can talk meaningfully about the probability it lies between 2.8 and 3.2 µg/mL, which is the area under the density between those points.\nPlant heights. The height of a genetically identical group of plants grown under controlled conditions will vary due to micro‑environmental factors. Those heights are modeled as a continuous random variable. We might ask, for example, how likely it is for a plant to be taller than 15 cm; again, we look at the area under the density to the right of 15.\n\nWhen interpreting a pdf, remember that taller regions of the curve correspond to higher likelihood density, not to the probability of a specific value. Probability comes from the area, not the height at a point.\n\n\nMean, Variance, and Standard Deviation of a Continuous Random Variable\nRecall that the mean of a discrete random variable is \\[\n\\mu = \\sum_x xP(X=x)\n\\]\nand the variance is \\[\n\\sigma^2= \\sum_x (x-\\mu)^2P(X=x)\n\\]\nWhen working with continuous RVs, using the sum in the formulas above will not make any sense (since there are infinite number of value to sum over in any given interval).\nInstead, we will use integration. The expected value for a continuous RV is \\[\n{\n\\mu = \\int_{-\\infty}^{\\infty}x f(x) dx\n}\n\\]\nand the variance is \\[\n{\n\\sigma^2= \\int_{-\\infty}^{\\infty} (x-\\mu)^2f(x) dx\n}\n\\]\n\n\nWorking in JMP Pro 17\nJMP can help you explore continuous distributions experimentally. Here is a general workflow using an exponential example, but you can adapt it to other distributions:\n\nSimulate continuous data. Create a new data table and use Rows → Add Rows to add, say, 1 000 rows. Add a new column and choose Column → Formula. In the formula editor search for Random Exponential(rate) and specify a rate (e.g., 1/10). Each cell will then contain a simulated lifetime.\nVisualize the distribution. Use Analyze → Distribution and select your simulated column. JMP produces a histogram and summary statistics. You can overlay a smooth density by clicking the red triangle ▸ next to the variable name and choosing Continuous Fit → Exponential.\nCompute probabilities. JMP’s distribution calculator (found under Add‑ins → Calculators → Distribution Calculator in JMP Pro 17) lets you choose a distribution, enter parameter values, and compute the probability that a continuous random variable lies between two values. For the exponential example, choose Exponential, set the rate, and enter the lower and upper bounds to find \\(P(a ≤ X ≤ b)\\).\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\ncontinuous random variable\nA random variable that can take any value in an interval; probabilities are assigned to ranges of values rather than individual points.\n\n\nprobability density function (PDF)\nA non‑negative function \\(f(x)\\) such that \\(P(a ≤ X ≤ b)\\) equals the area under \\(f(x)\\) between \\(a\\) and \\(b\\) and the total area under the curve of \\(f(x)\\) is one.\n\n\ncumulative distribution function (CDF)\nThe function \\(F(x)=P(X ≤ x)\\) giving the area under the PDF to the left of \\(x\\). It increases from 0 to 1 as \\(x\\) goes from \\(-∞\\) to \\(∞\\).\n\n\n\n\n\nCheck your understanding\n\nExplain in your own words why the probability that a continuous random variable equals exactly 5 is zero. How, then, do we assign probabilities for continuous variables?\nSketch or describe the shape of a PDF that would model serum cholesterol levels in a population. Why can’t a PDF ever dip below the horizontal axis?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nA continuous random variable can take infinitely many values within any interval. Because the PDF spreads probability continuously across these values, the probability of landing on any single point is zero. We obtain meaningful probabilities by integrating the density over an interval to find the area under the curve between the limits.\nSerum cholesterol tends to cluster around an average value with fewer extremely low or high values. A plausible PDF would be unimodal and right‑skewed: low near 0, rising to a peak near the typical cholesterol level, and gradually decreasing. The density must always stay at or above zero because probabilities cannot be negative.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "07.html#sec-07_02",
    "href": "07.html#sec-07_02",
    "title": "7  Continuous Probability Distributions",
    "section": "7.2 The Uniform Distribution",
    "text": "7.2 The Uniform Distribution\n\n“Don’t mistake possibilities for probabilities. Anything is possible. It’s the probabilities that matter” – Ray Dalio\n\n\nGuiding question: How does an equally likely continuous random variable work?\n\nThe simplest continuous distribution is the uniform distribution. Imagine selecting a time uniformly at random within a two‑hour window; any minute in that window is just as likely as any other. More formally, a continuous random variable \\(X\\) has a Uniform\\((c,d)\\) distribution if its PDF is constant on the interval \\((c,d)\\) and zero elsewhere.\nSince all values in the interval \\((c,d)\\) are equally likely, the pdf of a uniform random variable appears as a horizontal line:\n\n\n\n\n\n\n\n\n\nNote that the area under the curve must equal 1 (since the area corresponds to probability). Therefore, the area between \\(c\\) and \\(d\\) \\[\n\\begin{align*}\n    \\text{area of rectangle} = \\text{base}\\times \\text{height} &\\Longrightarrow{ 1 = (d-c) \\times \\text{height}}\\\\\\\\\n    &\\Longrightarrow \\text{height} = \\frac{1}{d-c}\n\\end{align*}\n\\]\nSo, the pdf of a uniform random variable is \\[\nf(x) = \\frac{1}{d-c}\n\\]\n\n\n\n\n\n\n\n\n\nThe expected value is \\[\n\\begin{align*}\nE(X)=\\int_{c}^d xf(x)dx &= \\int_c^dx\\left(\\frac{1}{d-c}\\right)dx\\\\\n& {= \\left(\\frac{1}{d-c}\\right)\\int_c^dxdx}\\\\\n& {= \\left(\\frac{1}{d-c}\\right)\\left(\\frac{1}{2}\\right)x^2\\Big\\vert^d_c} \\\\\n& {= \\left(\\frac{1}{d-c}\\right)\\left(\\frac{1}{2}\\right)\\left(d^2-c^2\\right)} \\\\\n& {= \\left(\\frac{1}{d-c}\\right)\\left(\\frac{1}{2}\\right)\\left(d-c\\right)\\left(d+c\\right)} \\\\\n& {= \\frac{c+d}{2}}\n\\end{align*}\n\\]\nWe will not show the steps here but we could find the variance in a similar fashion to get \\[\n\\sigma^2 = \\frac{\\left(d-c\\right)^2}{12}\n\\] The standard deviation is then \\[\n\\sigma = \\frac{\\left(d-c\\right)}{\\sqrt{12}}\n\\]\nFor a uniform random variable \\(X\\), what is the probability \\(P(a&lt;X&lt;b)\\)? \\[\n\\begin{align*}\n    P(a&lt;X&lt;b)=\\int_{a}^b f(x)dx & = \\int_{a}^b \\frac{1}{d-c} dx\\\\\n   & {= \\left(\\frac{1}{d-c}\\right)x\\Big\\vert_a^b}\\\\\n    &{= \\left(\\frac{b-a}{d-c}\\right)}\\\\\n\\end{align*}\n\\]\nSummary of Uniform RVs:\n\nthe pdf is \\(f(x)=\\frac{1}{d-c}\\qquad c\\le X\\le d\\)\nthe mean is \\(\\mu = \\frac{c+d}{2}\\) and the standard deviation is \\(\\sigma=\\frac{d-c}{\\sqrt{12}}\\)\n\\(P(a&lt;X&lt;b)=\\frac{b-a}{d-c}\\)\n\n\nExamples from medicine and biology\n\nPatient arrival time. Suppose a clinic accepts blood samples from 8 am to 10 am and the phlebotomist expects donors to arrive at random. Let \\(T\\) be the arrival time after 8 am (in hours). If arrivals are equally likely at any moment, \\(T \\sim \\text{Uniform}(0,2)\\). The probability that a randomly arriving donor comes between 8:30 and 9:00 am (i.e., \\(0.5 ≤ T ≤ 1\\)) is \\[\n\\begin{align*}\nP(0.5 ≤ T ≤ 1)=&\\frac{1-0.5}{2-0}\\\\\n=&0.25\n\\end{align*}\n\\]\nRandomized drug administration. In a study, participants are randomly assigned to take a dose of medication at any time between noon and 3 pm. The time of ingestion is Uniform\\((0,3)\\) hours after noon. If we want the probability that a dose is taken in the first half‑hour, we compute \\[\n\\begin{align*}\nP(0 ≤ T ≤ 0.5)=&\\frac{0.5-0}{3-0}\\\\\n=&0.167\n\\end{align*}\n\\]\n\n\n\nWorking in JMP Pro 17\nUniform simulations are straightforward in JMP:\n\nGenerate uniform random values. In a new data table, choose Rows → Add Rows to add your desired number of observations. Use Column → Formula, find the function Random Uniform, and specify the lower and upper bounds \\(c\\) and \\(d\\).\nVisualize and compute probabilities. Use Analyze → Distribution to produce a histogram. Since the density is flat, the histogram should approximate a rectangle when you use many bins. To compute \\(P(a ≤ X ≤ b)\\) without simulation, use the distribution calculator: select Uniform from the list, enter \\(c\\) and \\(d\\), and set the lower and upper limits. JMP will report the probability \\((b-a)/(d-c)\\).\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nuniform distribution\nA continuous distribution on \\((c,d)\\) whose PDF is constant at height \\(1/(d-c)\\). All intervals of equal length within \\((c,d)\\) have equal probability.\n\n\n\n\n\nCheck your understanding\n\nSuppose \\(X\\sim\\text{Uniform}(0,10)\\). What is \\(P(3 ≤ X ≤ 7)\\)? Explain your reasoning.\nA researcher measures the pH of soil samples collected uniformly at random along a transect from 0 to 100 m. What is the probability that a randomly selected soil sample comes from between 20 m and 35 m? Express your answer numerically.\nIf \\(Y\\sim\\text{Uniform}(c,d)\\) and you know that \\(P(Y ≤ 5) = 0.5\\), what relationship does this imply between \\(c\\), \\(d\\) and 5?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nThe interval from 3 to 7 has length 4. Since the distribution is Uniform\\((0,10)\\), the probability of any subinterval equals its length divided by the total length: \\(4/10=0.4\\).\nThe transect is 100 m long. The segment from 20 to 35 m is 15 m long, so \\(P(20 ≤ X ≤ 35) = 15/100 = 0.15\\).\nFor a uniform distribution, \\(P(Y ≤ y) = (y-c)/(d-c)\\) for \\(c ≤ y ≤ d\\). Setting \\(P(Y≤5)=0.5\\) implies \\((5 - c)/(d - c) = 0.5\\). Equivalently, \\(5\\) is the midpoint of the interval and \\(5 = (c + d)/2\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "07.html#sec-07_03",
    "href": "07.html#sec-07_03",
    "title": "7  Continuous Probability Distributions",
    "section": "7.3 The Normal Distribution",
    "text": "7.3 The Normal Distribution\n\n“the normal distribution is seldom, if ever, observed in nature.” – Louis Guttman\n\n\nGuiding question: Why is the normal distribution so commonly used?\n\nThe normal distribution (also called the Gaussian distribution) is the most celebrated continuous distribution in statistics. It is the foundation for much of statistical inference.\nA \\(N(\\mu,\\sigma)\\) distribution is symmetric, bell‑shaped and centered at its mean \\(\\mu\\). In a normal distribution, the mean, median and mode all coincide, and the curve extends infinitely in both directions. The two parameters \\(\\mu\\) and \\(\\sigma\\) control the center and spread of the distribution:\n\n\\(\\mu\\) is the location of the peak and\n\\(\\sigma\\) measures the standard deviation (the “width” of the bell).\n\n\n\n\n\n\n\n\n\n\nThe density function of a \\(N(\\mu,\\sigma)\\) random variable can be written as\n\\[\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left( - \\frac{1}{2\\sigma^2}(x - \\mu)^2 \\right).\n\\]\nYou do not need to memorize this formula to use the normal model. More important are its qualitative properties: it is unimodal, symmetric, and tails off smoothly; the total area under the curve is one.\n\nExamples from medicine and biology\n\nAdult heights. Within a homogeneous population, adult heights tend to cluster around an average and taper off symmetrically. For example, men’s heights might follow a \\(N(175,7)\\). Values far below 175 cm or far above are increasingly rare.\nGene expression levels. In microarray experiments, the log‑transformed expression levels of many genes approximate normality. This allows researchers to use statistical tests that assume normally distributed data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking in JMP Pro 17\nTo explore normal distributions in JMP:\n\nSimulate normal data. Create a new column with Column → Formula and use Random Normal(µ, σ) to generate values. For example, Random Normal(175,7) will simulate heights in centimetres with mean 175 and standard deviation 7.\nVisualise the distribution. Use Analyze → Distribution to generate a histogram and overlay a fitted normal curve. Click the red triangle ▸ next to the variable and choose Continuous Fit → Normal. JMP displays parameter estimates and a density overlay.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nnormal distribution\nA continuous, symmetric bell‑shaped distribution defined by its mean \\(\\mu\\) and standard deviation \\(\\sigma\\); mean = median = mode.\n\n\nstandard normal distribution\nThe special case Normal\\((0,1)\\); its values are often called z‑scores, and any normal distribution can be standardized via \\(z = (x - \\mu)/\\sigma\\).\n\n\n\n\n\nCheck your understanding\n\nWhat does it mean that the mean, median and mode of a normal distribution are equal? How is this reflected in the shape of the curve?\nFor a \\(N(150,20)\\) distribution (representing, say, birth weights in grams), approximately what percentage of babies weigh between 110 g and 190 g?\nExplain why extreme values (more than 3 standard deviations from the mean) are considered unusual under the normal model.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nA normal curve is perfectly symmetric about its mean; the highest point occurs at \\(\\mu\\) and the curve declines equally on both sides. Because of this symmetry, the most typical value (the mode), the point dividing the distribution in half (the median) and the arithmetic average (the mean) coincide.\nTwo standard deviations on either side of the mean cover about 95% of the data. The interval from \\(\\mu-2\\sigma = 150 - 40 = 110\\) to \\(\\mu+2\\sigma = 190\\) therefore captures roughly 95% of birth weights.\nUnder the normal model, only about 0.3% of observations lie beyond three standard deviations from the mean by the empirical rule. Thus values outside that range are rare and often signal measurement error or a departure from normality.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "07.html#sec-07_04",
    "href": "07.html#sec-07_04",
    "title": "7  Continuous Probability Distributions",
    "section": "7.4 Finding Probability for a Normal Distribution",
    "text": "7.4 Finding Probability for a Normal Distribution\n\n“I once worked with a guy for three years and never learned his name. Best friend I ever had. We still never talk sometimes.” -Ron Swanson\n\n\nGuiding question: How can we find probabilities using the normal model?\n\nOnce we determine the normal model for a variable, we can compute probabilities by standardizing the variable to a z‑score. Given \\(X\\sim N(\\mu,\\sigma)\\), the z‑score corresponding to a value \\(x\\) is\n\\[\nz = \\frac{x - \\mu}{\\sigma}.\n\\]\nThis transformation rescales and recenters \\(X\\) so that \\(Z \\sim N(0,1)\\). We then look up the area under the standard normal curve up to \\(z\\) (or beyond) using tables, software or JMP. Because the normal distribution is continuous, we always compute probabilities for intervals, not exact points.\n\nStep‑by‑step procedure\n\nState the distribution. Identify \\(\\mu\\) and \\(\\sigma\\) for your normal variable.\nDraw a sketch. Label the mean and the point(s) of interest on a bell curve. Shading the region corresponding to the probability helps visualize whether you need the area to the left, right or between two points.\nCompute z‑scores. For each boundary \\(x\\) compute \\(z=(x-\\mu)/\\sigma\\).\nUse a table or software. For the standard normal distribution, tables give \\(P(Z ≤ z)\\) for many \\(z\\) values. For probabilities of the form \\(P(X ≥ x)\\) or \\(P(a ≤ X ≤ b)\\), convert to z‑scores and use the fact that \\(P(Z &gt; z) = 1 - P(Z ≤ z)\\) and \\(P(a ≤ X ≤ b) = P(z_a ≤ Z ≤ z_b)\\).\nInterpret in context. State your answer in terms of the original problem.\n\n\n\nExample: antihypertensive drug\nSuppose the reduction in systolic blood pressure (SBP) after taking a new antihypertensive follows a \\(N(10,4)\\) distribution. What is the probability that a randomly treated patient experiences a reduction of at least 15 mm Hg?\nLet \\(X\\) be the reduction in SBP. We want \\(P(X ≥ 15)\\).\n\nCompute the z‑score.\n\n\\[\nz=\\frac{15-10}{4}=1.25\n\\]\n\nFind the area to the right. Using the Normal Probability Calculator in JMP 18 Student Edition (Student → Applets → Distribution Calculator):\n\n\nFrom this calculator we have: \\[\nP(Z ≥ 1.25)=0.1056\n\\]\nUsing the Normal Calculator also allows us to find the probability without taking a z-score first. We just need to change the values of Mean and Std. Dev. in the applet.\n\n\nInterpretation. About 10.6% of patients have a reduction of at least 15 mm Hg.\n\n\n\nWorking in JMP Pro 17\nJMP’s distribution calculator makes these computations straightforward:\n\nOpen the distribution calculator. In JMP Pro 17 go to Add‑ins → Calculators → Distribution Calculator. Choose Normal from the list of distributions.\nEnter parameters. Enter the mean and standard deviation (e.g., 10 and 4) and specify whether you want the area Left, Right or Between two values. For a “greater than” probability like \\(P(X ≥ 15)\\), choose Right and enter 15. JMP will display the area to the right.\nVisual check. The calculator shows a graph of the normal curve with the relevant region shaded. Use this to verify that you selected the correct tail or interval.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nz‑score\nThe number of standard deviations a value \\(x\\) is from the mean: \\(z=(x-\\mu)/\\sigma\\). Converting to z‑scores allows probabilities from any normal distribution to be found using the standard normal distribution.\n\n\n\n\n\nCheck your understanding\n\nCholesterol reductions after a dietary intervention follow a \\(N(20,5)\\) distribution. What is the probability that a randomly selected participant’s reduction is less than 12 mg/dL? Show the z‑score and compute the probability.\nSerum calcium levels in a normal population have mean 9.5 mg/dL and standard deviation 0.4 mg/dL. What proportion of individuals have calcium levels between 9.1 and 9.9 mg/dL? Sketch the problem and find the probability.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nCompute \\(z=(12-20)/5=-1.6\\). From a standard normal table, \\(P(Z ≤ -1.6)\\approx0.0548\\). Therefore \\(P(X ≤ 12)=0.0548\\).\nFirst convert the endpoints to z‑scores: \\(z_1=(9.1-9.5)/0.4=-1.0\\) and \\(z_2=(9.9-9.5)/0.4=1.0\\). Using the calculator, we have \\(P(-1\\le Z \\le 1)=0.6827\\)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "07.html#sec-07_05",
    "href": "07.html#sec-07_05",
    "title": "7  Continuous Probability Distributions",
    "section": "7.5 Finding a Quantile for a Normal Distribution",
    "text": "7.5 Finding a Quantile for a Normal Distribution\n\n“Two things are infinite: the universe and human stupidity; and I’m not sure about the universe.” – Albert Einstein\n\n\nGuiding question: How can we find quantiles using the normal model?\n\nIn many applications we know a desired probability and wish to find the corresponding value of \\(x\\) such that \\(P(X ≤ x)=p\\). This value is called a quantile or percentile of the distribution. Mathematically, the quantile function \\(F^{-1}(p)\\) is the inverse of the cumulative distribution function. For a normal distribution, the quantile function returns the \\(x\\) value whose cumulative probability is \\(p\\).\n\nRelationship between the CDF and quantiles\nThe cumulative distribution function \\(F(x)\\) gives the probability that a random variable \\(X\\) is less than or equal to \\(x\\). The quantile function does the reverse: it takes a probability \\(p\\) and returns the threshold \\(x\\) such that \\(P(X ≤ x)=p\\). For example, the 0.5 quantile is the median. Because the normal CDF does not have a simple algebraic inverse, quantiles are typically obtained from tables or software.\nAt times, there are special quantiles that will show up in statistical methods that we will discuss later. We denote these as \\[\np = P(Z&gt; z_p)\n\\] In other words, \\(z_p\\) is the value of the standard normal distribution that will have \\(p\\) area to the right. For example, \\(z_{0.05}\\) is the value that has 0.05 area to the right.\n\n\n\n\n\n\n\n\n\nHere the value is \\[\nz_{0.05}=1.645\n\\]\n\n\nProcedure for finding quantiles\n\nSpecify the probability \\(p\\). Decide whether you want a lower tail (left‑side) quantile (e.g., the 5th percentile) or a two‑sided bound, or an upper tail area.\nFind the corresponding z‑score. For the standard normal distribution, tables and software can give you the quantile.\nTransform back to \\(x\\). If \\(X\\sim N(\\mu,\\sigma)\\), then \\(x=\\mu + z\\sigma\\) gives the desired quantile. Some software can find the quantile right from \\(x\\). In which case, there is no need to find the value for \\(z\\)-score first.\n\n\n\nExample: therapeutic drug monitoring\nSuppose therapeutic blood levels of a drug after dosing follow a \\(N(50,10)\\) distribution. Physicians want to define the upper control limit beyond which a concentration is considered dangerously high. If they choose the 97.5th percentile as the cut‑off, what value should they use?\n\nFind the 97.5th percentile for \\(z\\). Using the special quantile location, this would be \\(z_{0.025}\\). Using the Normal Probability Calculator in JMP 18 Student Edition (Student → Applets → Distribution Calculator): \nTransform back.\n\\[\n\\begin{align*}\nx=&\\mu + z\\sigma\\\\\n=& 50 + 1.96\\times10\\\\\n=& 69.6\n\\end{align*}\n\\]\nInterpretation. Only 2.5% of patients are expected to have concentrations above 69.6 ng/mL. Concentrations exceeding this threshold may warrant intervention.\n\n\n\nWorking in JMP Pro 17\nTo find quantiles in JMP:\n\nUse the distribution calculator. Open the distribution calculator and select Normal. Switch to the Percentile (or Inverse) mode. Enter the probability \\(p\\) (e.g., 0.975) and the distribution parameters \\(\\mu\\) and \\(\\sigma\\). JMP returns the corresponding \\(x\\) value.\nCheck with the CDF. You can verify your result by switching back to the probability mode and entering the value \\(x\\) you found. The calculator should return \\(p\\).\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nquantile (percentile)\nFor a continuous distribution with CDF \\(F\\), the value \\(x=F^{-1}(p)\\) such that \\(P(X ≤ x)=p\\).\n\n\n\n\n\nCheck your understanding\n\nBirth weights in a population follow a \\(zn(3.5,0.4)\\) distribution. What is the weight corresponding to the 90th percentile? Show your calculation.\nAn assay has measurement errors that are \\(N(0,2)\\) distributed. What cut‑off defines the central 80% of the error distribution (i.e., the range from the 10th to the 90th percentile)?\nExplain the relationship between the CDF and the quantile function in your own words. Why do we need tables or software to find quantiles for the normal distribution?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nFirst find the z‑score \\(z_{0.10}\\approx1.2816\\). Then \\(x=3.5 + 1.2816\\times0.4 = 4.0126\\) kg. Therefore, about 10% of babies weigh more than approximately 4.0 kg.\nThe central 80% corresponds to the interval between the 10th and 90th percentiles. From a standard normal table \\(z_{0.90}\\approx-1.2816\\) and \\(z_{0.10}\\approx1.2816\\). Multiply by \\(\\sigma=2\\) and add the mean 0: the interval is from \\(-1.2816×2≈-2.5632\\) to \\(1.2816×2≈2.5632\\).\nThe CDF gives the probability that a random variable is less than or equal to \\(x\\). The quantile function reverses this: given a probability \\(p\\), it returns the \\(x\\) for which the CDF equals \\(p\\). The normal CDF has no simple algebraic inverse, so we rely on tables or software to compute quantiles.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "08.html",
    "href": "08.html",
    "title": "8  Sampling Distributions",
    "section": "",
    "text": "8.1 Data, Probability and Sampling Distributions\nThe leap from describing data to making decisions about unknown populations hinges on an understanding of sampling distributions. In the previous chapters we learned how to describe single samples and probability distributions. Now we turn our attention to what happens when we look at all possible samples and the statistics that come from them.\nBefore we can talk about sampling distributions we need to distinguish three different kinds of distributions you’ll encounter:\nThe difference between data, probability and sampling distributions can be subtle at first, so let’s unpack them with an example from biology. Imagine an ecologist studying the length of salamanders in a certain swamp. The probability distribution for salamander length might be right‑skewed because most animals are small but a few grow unusually long. The data distribution is what the ecologist actually observes when she randomly captures, measures and releases 20 salamanders. If she were to repeat that 20‑animal study many times, computing the sample mean length each time, the histogram of those means would form the sampling distribution of the sample mean.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "08.html#sec-08_01",
    "href": "08.html#sec-08_01",
    "title": "8  Sampling Distributions",
    "section": "",
    "text": "“While nothing is more uncertain than a single life, nothing is more certain than the average duration of a thousand lives.” – Elizur Wright\n\n\nGuiding question: What is a sampling distribution?\n\n\n\n\nThe data distribution is the distribution of the raw observations you collect in a single sample. For example, if you measure the blood pressures of 50 patients, the histogram of those 50 numbers is your data distribution.\nA probability distribution is the theoretical model you assume for a population. The probability distribution describes how each individual in the population varies.\nThe sampling distribution is the distribution of a statistic (for example the mean, median or proportion) computed from all possible samples of a fixed size. It is a theoretical idea rather than something we observe directly. The sampling distribution of a statistic is the distribution of all possible values taken by that statistic when all possible samples of size \\(n\\) are taken from the population. Put another way, the sampling distribution is the probability distribution of that statistic.\n\n\n\nStatistics versus parameters\nIt helps to recall the distinction between parameters and statistics. A parameter is a summary number that describes a population (e.g. the true mean blood pressure of all adults in Waco), whereas a statistic is a summary computed from a sample (e.g. the mean blood pressure of 50 randomly chosen adults). The sampling distribution tells us how a statistic varies from sample to sample and therefore how well it estimates the parameter.\n\n\nSampling distribution from a small population\nSuppose we had a small population of 20 US adults. Furthermore, suppose we want to estimate the proportion of this population that are left eye dominate. Below are the values for this population1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL\nL\nR\nR\nL\nL\nL\nL\nR\nL\nR\nL\nL\nR\nR\nR\nR\nR\nR\nR\n\n\n\n\n\nOur goal is to estimate the proportion of this population who are left eye dominate. Clearly, we could easily determine this proportion by looking at this small population. By doing so, we see that the proportion that are left eye dominate is \\[\n\\begin{align*}\np =& \\frac{9}{20}\\\\\n=&0.45\n\\end{align*}\n\\] Suppose we don’t know this proportion and the only thing we can do is randomly sample of size of 5 from this population. Below is one such sample.\n\n\n\n\n\nL\nR\nR\nL\nR\n\n\n\n\n\nFrom this sample, we estimate the proportion to be \\[\n\\begin{align*}\n\\hat{p} =& \\frac{2}{5}\\\\\n= & 0.4\n\\end{align*}\n\\]\nThis is just one such sample. There are many more random samples of size 5 from this population of 20 that I could have gotten. In fact, there are \\[\n\\binom{20}{5} = 15{,}504\n\\] possible samples of size 5 from this population. Suppose we did all of these samples and each time calculated \\(\\hat{p}\\). Below is a histogram of all of these \\(\\hat{p}\\)’s.\n\n\n\n\n\n\n\n\n\nThe histogram above shows the sampling distribution of \\(\\hat{p}\\) for a sample of size 5 from a population of size 20. For most practical applications, the population is much bigger than 20. It is not uncommon to have population sizes in the tens of thousands or even millions. Even if we kept the population size relatively low, such as 1000, the number of possible samples of size 5 become massive. \\[\n\\begin{align*}\n\\binom{1000}{5} = 8{,}250{,}291{,}250{,}200\n\\end{align*}\n\\]\nEven if we wanted to examine all of the possible samples (if the population was known) of size 5, it would be unfeasible even with a computer.\nFortunately, we have some results to help us determine what these distributions look like without having to examine all of the possible samples.\n\n\nWorking in JMP Pro 17\nJMP can simulate sampling distributions without requiring you to know R. To explore the sampling distribution of a mean:\n\nUse Help → Sample Data Library to load a dataset (for example, “Body Measurements”). Then choose Analyze → Distribution and assign your variable to Y to visualize the data distribution.\nTo simulate a sampling distribution, go to Graph Builder and use Bootstrap from the red triangle menu. Specify your statistic (mean, median or proportion) and the number of bootstrap samples. JMP will draw many samples with replacement from your data and display the distribution of the chosen statistic. This bootstrap distribution approximates the sampling distribution we would get by taking many independent samples from the population.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nData distribution\nThe distribution of the observed values in a single sample.\n\n\nProbability distribution\nA theoretical model describing how a variable behaves in the population.\n\n\nSampling distribution\nThe probability distribution of a statistic computed from all possible samples of a fixed size.\n\n\n\n\n\n\n\n\n\nCheck your understanding\n\nExplain, in your own words, the difference between a data distribution and a sampling distribution. Why is the latter crucial for inference?\nWhat is the meaning of the phrase “the sampling distribution is a theoretical idea—we do not actually build it”?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nData vs. sampling distribution. A data distribution reflects the raw measurements from one sample. A sampling distribution reflects how a summary statistic (such as a mean or proportion) would vary if we repeatedly took new samples of the same size. The sampling distribution is crucial because it tells us how much our statistic is expected to fluctuate around the true parameter, and thus forms the basis for standard errors and confidence intervals.\nWhy theoretical? The number of possible samples of size \\(n\\) from a population is enormous, so we cannot literally take all of them. The sampling distribution therefore exists as a theoretical construct.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "08.html#sec-08_02",
    "href": "08.html#sec-08_02",
    "title": "8  Sampling Distributions",
    "section": "8.2 Sampling Distribution of the Sample Proportion",
    "text": "8.2 Sampling Distribution of the Sample Proportion\n\n“Math is the logic of certainty; statistics is the logic of uncertainty.” - Joe Blizstein\n\n\nGuiding question: How do we estimate the accuracy of our sample results?\n\nMany biological and medical studies involve categorical outcomes: the presence or absence of a gene mutation, success or failure of a treatment, or whether a patient exhibits a symptom. In these settings a common statistic is the sample proportion, denoted \\(\\hat{p}\\), which estimates the population proportion \\(p\\). The sampling distribution of \\(\\hat{p}\\) describes how that proportion varies across repeated random samples of the same size.\n\nProperties of the sampling distribution\nWhen the outcomes in the population are independent and the population proportion is \\(p\\), the sampling distribution of \\(\\hat{p}\\) has two key properties:\n\nCenter. The mean (or expected value) of \\(\\hat{p}\\) is the true population proportion. In other words, \\(E(\\hat{p}) = p\\).\n\nSpread. The standard deviation of \\(\\hat{p}\\) is \\[\n\\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}}\n\\]\nThis formula arises because the variance of a binomial random variable with parameters \\(n\\) and \\(p\\) is \\(np(1-p)\\), and dividing by \\(n^2\\) converts the count to a proportion. The standard deviation shrinks as the sample size increases, meaning larger samples give more precise estimates.\nShape. Under mild conditions (in particular, when the expected numbers of successes and failures both exceed about 15), the sampling distribution of \\(\\hat{p}\\) is approximately normal. Thus, for large enough \\(n\\) we can use a Normal model to approximate probabilities involving \\(\\hat{p}\\).\n\n\n\nExample: prevalence of blood type O\nConsider a large population of blood donors in which the true proportion with type O blood is 45%. Suppose we randomly sample \\(n=50\\) donors and record whether each has type O blood. The sample proportion of type O donors is \\(\\hat{p} = x/n\\), where \\(x\\) is the number of type O donors. Because \\(x\\) follows a binomial distribution with parameters \\((n,p) = (50,0.45)\\), we know that \\[\nE(\\hat{p}) = 0.45\n\\] and \\[\n\\sigma_{\\hat{p}} = \\sqrt{0.45\\times 0.55/50} \\approx 0.070\n\\]\nTo see the sampling distribution in action, we can simulate many samples and plot their proportions. Below is a histogram of \\(\\hat{p}\\) for 10,000 samples.\n\n\n\n\n\n\n\n\n\nThe histogram of the simulated proportions (blue bars) lines up closely with the red Normal curve predicted by the theory. Most sample proportions fall within roughly two standard errors (about ±0.14) of the true proportion 0.45.\n\n\nConditions for the normal approximation\nThe rule of thumb for using the Normal approximation to \\(\\hat{p}\\) is that both \\(np\\) and \\(n(1-p)\\) should be at least 15. This ensures there are enough successes and failures for the normal curve to be close. When this condition is not met, the sampling distribution can be noticeably skewed, and exact binomial calculations or bootstrap methods (Section 8.4) are preferable.\n\n\nWorking in JMP Pro 17\nTo explore sampling distributions for proportions in JMP:\n\nUse Analyze → Distribution on a binary variable (coded 1 for success and 0 for failure) to see the data distribution.\nUse Graph Builder with the Bootstrap option to resample your data with replacement. Specify the statistic as proportion of successes and the number of bootstrap samples. JMP will display the bootstrap distribution, which closely approximates the theoretical sampling distribution when the sample is random and unbiased.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nPopulation proportion \\(p\\)\nThe true fraction of individuals in the population with a certain characteristic.\n\n\nSample proportion \\(\\hat{p}\\)\nThe fraction of sampled individuals with the characteristic; an estimator of \\(p\\).\n\n\nExpected value of \\(\\hat{p}\\)\nThe expected value of the sampling distribution of \\(\\hat{p}\\), equal to \\(p\\).\n\n\nStandard deviation of \\(\\hat{p}\\)\nThe standard deviation of the sampling distribution of \\(\\hat{p}\\), equal to \\(\\sqrt{p(1-p)/n}\\).\n\n\nNormal approximation\nFor large \\(n\\) with \\(np\\ge15\\) and \\(n(1-p)\\ge15\\), the sampling distribution of \\(\\hat{p}\\) is approximately normal.\n\n\n\n\n\nCheck your understanding\n\nExplain why the sampling distribution of \\(\\hat{p}\\) has mean equal to the population proportion. What would it mean if the mean of \\(\\hat{p}\\) were systematically above or below \\(p\\)?\nSuppose the true prevalence of a rare mutation is 1%. If you sample \\(n=100\\) individuals, will the sampling distribution of \\(\\hat{p}\\) be well approximated by a Normal distribution? Why or why not? How might you proceed instead?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nUnbiasedness. If we took infinitely many random samples and averaged the sample proportions, we would recover the true population proportion. That is precisely what the expected value of \\(\\hat{p}\\) tells us: \\(E(\\hat{p})=p\\). If the mean of \\(\\hat{p}\\) were consistently above \\(p\\), our estimator would be biased, systematically overestimating the true proportion.\nRare mutation. When \\(p\\) is very small (0.01) and \\(n=100\\), the expected number of successes is \\(np=1\\) and the expected number of failures is 99. Because \\(np&lt;15\\), the sampling distribution of \\(\\hat{p}\\) is highly skewed and the normal approximation is poor. A better approach is to use the exact binomial distribution to compute probabilities or to use a bootstrap to approximate the sampling distribution (see Section 8.4).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "08.html#sec-08_03",
    "href": "08.html#sec-08_03",
    "title": "8  Sampling Distributions",
    "section": "8.3 Sampling Distribution of the Sample Mean",
    "text": "8.3 Sampling Distribution of the Sample Mean\n\n“The Scientist must set in order. Science is built up with facts, as a house is with stones. But a collection of facts is no more a science than a heap of stones is a house.” - Henri Poincare\n\n\nGuiding question: How does the Central Limit Theorem make statistics possible?\n\nThe sample mean \\(\\bar{x}\\) is the workhorse of quantitative inference. When we measure a quantitative trait—blood pressure, cholesterol level, enzyme activity—we often summarize it with an average. To use \\(\\bar{x}\\) for inference we must understand its sampling distribution. Under mild conditions, the sampling distribution of \\(\\bar{x}\\) has remarkable properties.\n\nMean and standard deviation\nIf the population has mean \\(\\mu\\) and standard deviation \\(\\sigma\\), and we take random samples of size \\(n\\), then the sampling distribution of the sample mean has\n\nMean: \\(E(\\bar{x}) = \\mu\\). There is no systematic tendency for the sample mean to over‑ or under‑estimate the population mean.\nStandard Deviation: \\(\\sigma_{\\bar{x}} = \\sigma/\\sqrt{n}\\). Averages are less variable than individual observations because the variation is reduced by the factor \\(\\sqrt{n}\\). Bigger samples give more stable means.\n\nThese facts do not require any particular distribution for the population; they follow from basic properties of expected value and variance. However, the shape of the sampling distribution does depend on the population distribution. If the population is normal, then the shape will be normal. But what if the population is not normal or the shape is unknown?\n\n\nThe Central Limit Theorem\nThe Central Limit Theorem (CLT) is one of the pillars of statistics. It states that when we randomly sample from any population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the sampling distribution of \\(\\bar{x}\\) becomes approximately normal as the sample size \\(n\\) grows. In symbols, \\[\n\\bar{X} \\overset{\\cdot}{\\sim} N\\bigl(\\mu,\\, \\sigma/\\sqrt{n}\\bigr)\n\\] for sufficiently large \\(n\\). Note that the dot (\\(\\cdot\\)) above \\(\\sim\\) means “approximately distributed as”.\nThe beauty of the CLT is that it does not require the underlying population to be normal; even strongly skewed or irregular distributions yield approximately normal sample means when \\(n\\) is large enough.\n\n\nIllustration with simulated enzyme activities\nEnzyme activity measurements often follow a skewed distribution because they cannot be negative but can have long right tails. Suppose the true activity in a population of cells follows an exponential distribution. We take repeated random samples of different sizes and compute the sample mean for each. The following plot shows 10,000 simulated sample means for \\(n=5\\), \\(n=20\\) and \\(n=50\\) to illustrate how the distribution of \\(\\bar{x}\\) evolves:\n\n\n\n\n\n\n\n\n\nThe three panels show that for very small samples (\\(n=5\\)) the sampling distribution of \\(\\bar{x}\\) still retains some skewness. For moderate samples (\\(n=20\\)) the distribution looks more bell‑shaped, and by \\(n=50\\) it is nearly indistinguishable from the Normal curve (red line). This behavior is exactly what the CLT predicts.\n\n\nPractical considerations\nThe CLT justifies using normal‑based methods for many statistics, but it is not a panacea. Large samples are not always attainable. Sometimes cost, difficulty or the preciousness of biological material limits the sample size. In such cases the sampling distribution of \\(\\bar{x}\\) may be far from normal, especially for very skewed or heavy‑tailed populations. Diagnostic plots and simulation can help you gauge whether normal approximations are reasonable.\nIf the data is not available, then the rule-of-thumb of \\(n\\ge 30\\) is adequate in most situations to determine if the sample size is large enough.\n\n\nWorking in JMP Pro 17\nTo explore the sampling distribution of the mean in JMP:\n\nUse Analyze → Distribution to visualise your quantitative data and estimate the population standard deviation.\nChoose Analyze → Resampling and select Bootstrap. Specify the statistic as the mean and set the number of resamples. JMP will generate a bootstrap sampling distribution of \\(\\bar{x}\\), plot it and report the standard error. You can compare the bootstrap distribution to a Normal distribution with mean equal to the observed \\(\\bar{x}\\) and standard deviation equal to the bootstrap standard error.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nStandard deviation of \\(\\bar{x}\\)\nThe standard deviation of the sampling distribution of \\(\\bar{x}\\), equal to \\(\\sigma/\\sqrt{n}\\).\n\n\nCentral Limit Theorem (CLT)\nStates that for large \\(n\\), the sampling distribution of the sample mean is approximately normal with mean \\(\\mu\\) and standard deviation \\(\\sigma/\\sqrt{n}\\).\n\n\n\n\n\nCheck your understanding\n\nA laboratory measures the enzyme activity of 10 randomly selected yeast cultures. The population distribution is known to be highly skewed with mean 50 units and standard deviation 20 units. Without doing any calculations, would you expect the sample mean to follow a Normal distribution? Explain your reasoning.\nA nutritionist samples 64 adults and measures their daily vitamin D intake. The population mean intake is 600 IU with standard deviation 200 IU. What is the mean and standard deviation of the sampling distribution of \\(\\bar{x}\\)? If the intake distribution is skewed, is the Normal approximation still reasonable? Why or why not?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nSmall, skewed samples. With a sample size of 10 and a highly skewed population, the sampling distribution of \\(\\bar{x}\\) will retain noticeable skewness. The Central Limit Theorem requires larger \\(n\\) before the distribution of the sample mean becomes approximately normal, so caution is warranted when applying Normal approximations.\nVitamin D intake. The sampling distribution has mean \\(\\mu = 600\\) and standard deviation \\(200/\\sqrt{64} = 25\\). Because \\(n=64\\) is reasonably large, the CLT suggests that the sample mean will be approximately normal even if the individual intakes are skewed. Therefore the Normal approximation should be adequate.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "08.html#sec-08_04",
    "href": "08.html#sec-08_04",
    "title": "8  Sampling Distributions",
    "section": "8.4 Bootstrap Sampling Distribution",
    "text": "8.4 Bootstrap Sampling Distribution\n\n“It is the mark of a truly intelligent person to be moved by statistics.” -George Bernard Shaw\n\n\nGuiding question: How to determine the sampling distribution with just the data?\n\nSometimes we cannot rely on formulas or the Central Limit Theorem to determine the sampling distribution. Maybe the statistic has a complicated distribution (like a median or a trimmed mean) or the sample size is too small for the normal approximation. In these cases we can use bootstrapping to approximate the sampling distribution directly from the data.\n\nWhat is bootstrapping?\nBootstrapping is a resampling procedure that uses the data from a single sample to generate a sampling distribution. The idea is simple:\n\nTreat your observed sample of size \\(n\\) as a stand‑in for the population.\nDraw a bootstrap sample of size \\(n\\) with replacement from the original data. Sampling with replacement means some observations may be selected multiple times while others may not appear at all.\nCompute the statistic of interest (mean, median, proportion, difference, etc.) on the bootstrap sample. This value is a bootstrap statistic.\nRepeat steps 2 and 3 many times (often thousands). The histogram of the bootstrap statistics forms the bootstrap distribution or bootstrap sampling distribution.\n\nBootstrapping gives us an empirical approximation to the true sampling distribution without making strong assumptions about the population. It is particularly useful for complex statistics, skewed data or small samples.\n\n\nExample: median tumor size\nImagine a study measuring the diameters (in millimeters) of 25 tumors detected in a mammogram screening. The sample is small and the data are skewed; we want to estimate the sampling distribution of the median tumor size. A bootstrap approach provides the following:\n\n\n\n\n\n\n\n\n\nThe histogram shows the bootstrap distribution of the median tumor size. From this distribution we can compute a bootstrap standard deviation (the standard deviation of the bootstrap medians) and make inferences.\n\n\nWhen to bootstrap\nBootstrapping is most useful when:\n\nThe sample size is small and the Central Limit Theorem may not hold.\nThe statistic has a complicated or unknown sampling distribution (medians, percentiles, regression coefficients, etc.).\nYou want to assess the uncertainty of a statistic without strong parametric assumptions.\n\n\n\nWorking in JMP Pro 17\nJMP has built‑in bootstrap tools that make resampling easy:\n\nAfter running an analysis (for example Analyze → Fit Y by X for comparing two groups), click the red triangle menu (▸) and select Bootstrap. Choose the statistic you wish to bootstrap and the number of resamples. JMP will create a bootstrap distribution, display it and report standard errors and confidence intervals.\nFor custom statistics, use Tables → Bootstrap Data to generate bootstrap samples from your dataset. You can then analyse each bootstrap sample using your preferred platform and collect the statistic of interest.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nBootstrapping\nA resampling method that uses the observed data to approximate a sampling distribution by repeatedly sampling with replacement.\n\n\nBootstrap sample\nA sample of size \\(n\\) drawn with replacement from the original sample.\n\n\nBootstrap statistic\nThe value of the statistic computed on a bootstrap sample.\n\n\nBootstrap distribution\nThe distribution of many bootstrap statistics; an empirical approximation to the sampling distribution.\n\n\n\n\n\nCheck your understanding\n\nWhy do we sample with replacement when constructing a bootstrap sample? What would go wrong if we sampled without replacement?\nCompare and contrast the bootstrap distribution with the theoretical sampling distribution. Under what circumstances do they coincide, and when might they differ?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nReplacement is essential. Sampling with replacement allows each observation to appear multiple times—or not at all—in a bootstrap sample. This mimics the variability of drawing new samples from the population. Sampling without replacement would simply rearrange the data and fail to capture the variability inherent in new samples.\nBootstrap vs. theoretical. The bootstrap distribution approximates the theoretical sampling distribution when the sample is random and representative, and when the number of bootstrap resamples is large. For statistics with simple known sampling distributions (like means and proportions), the bootstrap will agree closely with theory. For statistics whose sampling distributions are complicated or unknown, the bootstrap provides a practical alternative but may differ from the true sampling distribution, especially when the sample size is very small or the sample is biased.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "08.html#footnotes",
    "href": "08.html#footnotes",
    "title": "8  Sampling Distributions",
    "section": "",
    "text": "Data was obtained from Introductory Statistical Methods classes. Here, we are treating these values as a population.↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "09.html",
    "href": "09.html",
    "title": "9  Estimation and Confidence Intervals",
    "section": "",
    "text": "9.1 Point and Interval Estimates\nConfidence intervals lie at the heart of statistical inference. Instead of guessing a single number to represent an unknown parameter, we use data to construct a range of plausible values and attach a level of confidence to that range. This chapter builds on the sampling distribution ideas from Chapter 8 to develop practical methods for estimating proportions and means, and for planning an appropriate sample size. Calculus is not needed for these procedures, but a solid understanding of variability and distributions will make the ideas come alive.\nStatistical analysis begins with a point estimate, a single summary number computed from a sample to estimate an unknown population parameter. For example, if a business analyst records the daily revenue of a store for 30 days and finds the average to be \\(\\$2{,}500\\), that sample mean is a point estimator of the true average daily revenue. Similarly, if an epidemiologist surveys 200 patients and 36 carry a particular gene variant, the sample proportion \\(\\hat{p} = 36/200 = 0.18\\) is a point estimator for the unknown population proportion of carriers.\nWhile a point estimate is easy to compute and understand, it is only one value. Because every sample yields a slightly different statistic, point estimates vary from sample to sample. To convey that uncertainty, statisticians construct an interval estimate, called a confidence interval. A confidence interval provides a range of plausible values for the parameter together with a specified level of confidence. The margin of error quantifies how far the point estimate may be from the true parameter; larger samples produce smaller margins of error.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimation and Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "09.html#sec-09_01",
    "href": "09.html#sec-09_01",
    "title": "9  Estimation and Confidence Intervals",
    "section": "",
    "text": "“Facts are stubborn things, but statistics are pliable.” – Mark Twain\n\n\nGuiding question: What is the difference between a point estimate and an interval estimate?\n\n\n\n\nPoint Estimation: Making a Best Guess for a Population Parameter\nOnce we have collected data, how do we produce a single best guess for a population parameter? The answer is simple: we use an appropriate sample statistic. For example, for a population mean \\(\\mu\\) the sample mean \\(\\bar{x}\\) is a point estimate of \\(\\mu\\). For a population proportion \\(p\\) the sample proportion \\(\\hat{p}\\) is a point estimate of \\(p\\). In practice, the choice of statistic depends on the parameter we wish to estimate (mean, median, proportion, variance, etc.).\nNot all point estimates are equally good. Some statistics tend to be closer to the parameter than others. The subsection below outlines two desirable properties—center and spread—that make an estimator well suited for inference.\n\nProperties of point estimators\nBefore building interval estimates, it is helpful to reflect on why some statistics make better single‑number estimates than others. For any given parameter there may be multiple reasonable choices. For example, for a symmetric normal distribution the center is both the mean and the median. With sample data from a normal population, the sample mean and sample median both estimate this center.\nA good estimator should satisfy two criteria:\n\nCenter. The sampling distribution of the statistic should be centered at the parameter. Equivalently, the expected value (or mean) of that sampling distribution should equal the parameter. An estimator with this property is called unbiased. For random sampling the sampling distribution of the sample mean has mean \\[\n\\mu_{\\bar{x}} = \\mu\n\\] and the sampling distribution of the sample proportion has mean \\[\n\\mu_{\\hat{p}} = p\n\\] Thus \\(\\bar{x}\\) and \\(\\hat{p}\\) are unbiased estimators.\nSpread. Among unbiased estimators, a smaller standard deviation is preferred. An estimator with a small standard deviation has a sampling distribution that tends to fall closer to the true value. In the normal distribution example, both the sample mean and sample median are unbiased, but the sample mean has a smaller standard deviation and is therefore more efficient.\n\nIn this course we will favor estimators that are unbiased (or nearly so in practical terms) and that have relatively small variability. Understanding these properties helps explain why the sample mean and sample proportion play such a central role in statistical inference.\n\n\n\nWhy interval estimates matter\nImagine a pharmaceutical researcher measuring the reduction in blood pressure after administering a new drug to 15 patients. The sample mean reduction is 12 mmHg. Reporting only the point estimate may be misleading because the true mean reduction could be larger or smaller. An interval estimate, say 12 mmHg ± 3 mmHg at 95% confidence, communicates both the best guess and the uncertainty due to sampling.\nInterval estimates also help compare results across studies. Two companies might estimate the average time a customer spends on their website. Company A observes an average of 5.6 minutes with a 95% confidence interval of \\((4.9, 6.3)\\) minutes, while Company B observes 6.1 minutes with interval \\((5.5, 6.7)\\) minutes. The overlapping intervals suggest there may be no meaningful difference between the two sites. Without intervals, it would be tempting to conclude that 6.1 is larger than 5.6 and therefore Company B is performing better.\n\n\nWorking in JMP Pro 17\nJMP makes it straightforward to compute both point and interval estimates:\n\nFor a mean: After opening your data table, choose Analyze → Distribution and assign your variable to Y. The output displays the sample mean (the point estimate). To add a confidence interval for the mean, click the red triangle next to the variable name and select Confidence Interval. JMP will display the interval estimate along with the margin of error.\nFor a proportion: If your data are coded as 1 = success and 0 = failure, choose Analyze → Distribution and assign the binary variable to Y. Under the red triangle menu select Confidence Interval to obtain a confidence interval for the proportion. JMP uses the normal approximation when the number of successes and failures is sufficiently large.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nPoint estimator\nA single statistic that estimates a population parameter.\n\n\nInterval estimator\nA range of plausible values for a parameter formed by adding and subtracting a margin of error from the point estimate.\n\n\nMargin of error\nThe maximum expected difference between the point estimate and the true parameter; it determines the width of the confidence interval.\n\n\nConfidence interval\nAn interval estimate that includes the unknown parameter with a specified level of confidence.\n\n\nUnbiased estimator\nAn estimator whose sampling distribution is centered at the parameter (its expected value equals the parameter).\n\n\n\n\n\nCheck your understanding\n\nExplain why relying solely on a point estimate can be misleading. What extra information does a confidence interval provide?\nIn a pilot study of 20 patients, the average cholesterol reduction after a dietary intervention is 18 mg/dL. Describe the difference between reporting this number alone and reporting a 95% confidence interval of (15 mg/dL, 21 mg/dL).\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nWhy point estimates need help. A point estimate is only one number drawn from a single sample. Because different random samples produce different statistics, the point estimate alone does not convey how much uncertainty surrounds it. A confidence interval couples the point estimate with a margin of error, giving a range of plausible values for the parameter and a level of confidence. This range reflects the variability of the statistic due to sampling.\nReporting cholesterol reductions. Saying that the average reduction is 18 mg/dL hides the fact that the true mean reduction might be lower or higher. Reporting a 95% confidence interval of (15, 21) mg/dL communicates that, based on the data, we are 95% confident the true mean reduction lies between 15 and 21 mg/dL. The interval reveals both the estimate and the uncertainty, which is essential for informed decision making.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimation and Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "09.html#sec-09_02",
    "href": "09.html#sec-09_02",
    "title": "9  Estimation and Confidence Intervals",
    "section": "9.2 Confidence Intervals for a Population Proportion",
    "text": "9.2 Confidence Intervals for a Population Proportion\n\n“Be approximately right rather than exactly wrong.”” – John Tukey\n\n\nGuiding question: How do we build a confidence interval for a proportion?\n\nWhen outcomes are categorical—responders vs. non‑responders to a treatment, default vs. no default on a loan, presence or absence of a gene mutation—the parameter of interest is the population proportion, denoted \\(p\\).\nWe estimate \\(p\\) with the sample proportion \\(\\hat{p} = x/n\\), where \\(x\\) is the number of successes in a random sample of size \\(n\\).\nAs discussed in Chapter 8, if the sample is large and the number of successes and failures are both at least about 15, the sampling distribution of \\(\\hat{p}\\) is approximately normal with mean \\(p\\) and standard error \\(\\sqrt{p(1-p)/n}\\). That is, \\[\n\\begin{align*}\n\\hat{p} \\overset{\\cdot}{\\sim}N\\left(p, \\sqrt{\\frac{p(1-p)}{n}}\\right)\n\\end{align*}\n\\] for large \\(n\\).\nRecall from Chapter 7, that a normal random variable can be transformed to a standard normal random variable by taking the z-score. For \\(\\hat{p}\\), the z-score is \\[\nz = \\frac{\\hat p - p}{\\sqrt{\\frac{p(1-p)}{n}}}\n\\]\nUsing the normal distribution, we choose some area under the curve and find the values of \\(z_{\\alpha/2}\\) that satisfies \\[\nP(-z_{\\alpha/2}&lt;Z&lt;z_{\\alpha/2}) = 1-\\alpha\n\\] where\n\n\\(1-\\alpha\\) is the area in the middle you choose,\n\\(\\alpha\\) is the area in the tails\n\\(z_{\\alpha/2}\\) is the value on the standard normal distribution that has \\(\\alpha/2\\) area to the right.\n\nThe value \\(z_{\\alpha/2}\\) is often referred to as the critical value.\nFor example, if we chose 95% of the area under the curve, then we would have \\[\nP(-z_{0.025}&lt;Z&lt;z_{0.025}) = 0.95\n\\]\nUsing the Distribution Calculator in JMP: \nThus, we would have \\[\nz_{\\alpha/2} = 1.96\n\\] and \\[\nP(-1.96&lt;Z&lt;1.96) = 0.95\n\\] when we use 95%.\nLet’s keep everything general and look at the interval \\[\n-z_{\\alpha/2}&lt;Z&lt;z_{\\alpha/2}\n\\]\nSince we said that \\(\\hat p\\) can be converted into the standard normal, we can substitute the z-score for \\(\\hat p\\) into this interval: \\[\n-z_{\\alpha/2}&lt;\\frac{\\hat p - p}{\\sqrt{\\frac{p(1-p)}{n}}}&lt;z_{\\alpha/2}\n\\] Let’s now solve this for \\(p\\) in the numerator: \\[\n\\begin{align*}\n-z_{\\alpha/2}&lt;\\frac{\\hat p - p}{\\sqrt{\\frac{p(1-p)}{n}}}&lt;z_{\\alpha/2} \\Longrightarrow& -z_{\\alpha/2}\\sqrt{\\frac{p(1-p)}{n}}&lt;{\\hat p - p}&lt;z_{\\alpha/2}\\sqrt{\\frac{p(1-p)}{n}}\\\\\n\\Longrightarrow& -\\hat p-z_{\\alpha/2}\\sqrt{\\frac{p(1-p)}{n}}&lt;{ - p}&lt;-\\hat p+z_{\\alpha/2}\\sqrt{\\frac{p(1-p)}{n}}\\\\\n\\Longrightarrow& \\hat p+z_{\\alpha/2}\\sqrt{\\frac{p(1-p)}{n}}&gt;{  p}&gt;\\hat p-z_{\\alpha/2}\\sqrt{\\frac{p(1-p)}{n}}\\\\\n\\Longrightarrow& \\hat p-z_{\\alpha/2}\\sqrt{\\frac{p(1-p)}{n}}&lt;{  p}&lt;\\hat p+z_{\\alpha/2}\\sqrt{\\frac{p(1-p)}{n}}\\\\\n\\end{align*}\n\\]\nThus, \\[\n\\hat p\\pm z_{\\alpha/2}\\sqrt{\\frac{p(1-p)}{n}}\n\\] is a \\((1-\\alpha)100\\%\\) confidence interval for \\(p\\).\nNote that we have a problem with this interval. The goal of the above confidence interval is to give an interval estimate for the unknown parameter \\(p\\). How can we calculate this interval that has \\(p\\) in the standard deviation if we don’t know \\(p\\). When we have a situation like this, we estimate the standard deviation with statistics that we do know. In this case, we use the statistic \\(\\hat p\\) as an estimate of \\(p\\).\nWhen a standard deviation of a sampling distribution is estimated, we call it the standard error.\nUsing the standard error, we can construct a \\((1-\\alpha)100\\%\\) confidence interval for \\(p\\) with \\[\n\\hat p\\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}\n\\]\nwhere\n\n\\(\\hat p\\) is the point estimate\n\\(\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}\\) is the standard error\n\\(z_{\\alpha/2}\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}\\) is the margin of error.\n\nThis interval is valid if\n\n\\(n\\hat p \\ge 15\\)\n\\(n(1-\\hat p)\\ge 15\\)\n\n\nExample: estimating a gene mutation prevalence\nSuppose a geneticist screens \\(n=200\\) individuals for a particular mutation and finds \\(x=36\\) carriers. Then \\(\\hat{p} = 36/200 = 0.18\\).\nSuppose we want a 95% confidence interval for \\(p\\) the true proportion of the population that has the mutation. We have \\[\nn\\hat p = 200(36/200) = 36\\ge 15\n\\] and \\[\nn(1-\\hat p) = 200(164/200)=164\\ge 15\n\\] Since both conditions hold, we can construct the 95% confidence interval.\nFor a 95% confidence interval, \\(z_{0.025} = 1.96\\). The standard error is \\[\n\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} = \\sqrt{\\frac{0.18(1-0.18)}{200}} \\approx 0.027\n\\]\nThe margin of error is \\[\nm = 1.96\\times 0.027 \\approx 0.053\n\\] The 95% confidence interval is \\(0.18 \\pm 0.053\\), or \\((0.127,\\,0.233)\\). We are 95% confident that between 12.7% and 23.3% of the population carries the mutation.\n\n\nExample: repeat-purchase rate in online retail\nA business analyst wants to estimate the proportion of customers who make a repeat purchase within 30 days. In a simple random sample of 500 customers, 120 made a repeat purchase. Here \\(\\hat{p} = 120/500 = 0.24\\). We have \\[\nn\\hat p = 500(120/500)=120\\ge 15\n\\] and \\[\nn(1-\\hat p)=500(380/500)=380\\ge 15\n\\]\nA 95% confidence level uses \\(z_{0.025} = 1.96\\). The standard error is \\(\\sqrt{0.24\\times 0.76/500} \\approx 0.019\\). The margin of error is \\(1.96\\times 0.019 \\approx 0.037\\). Thus the 95% confidence interval is \\((0.203,\\,0.277)\\). We can report that the true repeat‑purchase rate is between 20.3% and 27.7% with 95% confidence.\n\n\nConditions and cautions\nThe normal approximation works well when both \\(n\\hat{p}\\) and \\(n(1-\\hat{p})\\) are at least about 15. When the sample size is small or the proportion is very close to 0 or 1, the distribution of \\(\\hat{p}\\) is skewed. In such situations, exact binomial methods or bootstrap confidence intervals may provide more accurate results. Always check that the sample is random and that the binary outcome is independent across individuals.\n\n\nWorking in JMP Pro 17\nTo compute a confidence interval for a proportion in JMP:\n\nOrganise your data so that the binary outcome is coded as 1 = success and 0 = failure.\nUse Analyze → Distribution and assign the binary variable to Y.\nUnder the red triangle menu, select Confidence Interval. JMP will report the sample proportion, the standard error and the confidence interval. You can specify the confidence level if you wish to use something other than 95%.\nTo explore the sampling distribution interactively, use Graph Builder and select Bootstrap from the red triangle. Choose the proportion statistic and set the number of resamples. JMP will display the bootstrap distribution, which approximates the sampling distribution.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nStandard error of \\(\\hat{p}\\)\nThe estimated standard deviation of the sampling distribution of \\(\\hat{p}\\), estimated by \\(\\sqrt{\\hat{p}(1-\\hat{p})/n}\\).\n\n\nCritical value\nA quantile of the standard normal distribution used to achieve a desired confidence level (e.g., \\(z_{0.025}=1.96\\) for 95%).\n\n\nConfidence interval for \\(p\\)\nThe interval \\(\\hat{p} \\pm z_{\\alpha/2}\\sqrt{\\hat{p}(1-\\hat{p})/n}\\) that contains \\(p\\) with a specified level of confidence.\n\n\n\n\n\nCheck your understanding\n\nIn a study of a new vaccine, 75 out of 250 randomly selected participants experienced mild side effects. Compute a 90% confidence interval for the proportion of all vaccine recipients who experience side effects.\nA marketing survey of \\(1{,}000\\) customers finds that 280 prefer a new product design. At the 99% confidence level, estimate the population proportion of customers who prefer the new design and interpret the result.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nVaccine side effects. \\(\\hat{p} = 75/250 = 0.30\\). For a 90% confidence level, \\(z_{0.05} = 1.645\\). The standard error is \\(\\sqrt{0.30\\times 0.70/250} \\approx 0.0289\\). The margin of error is \\(1.645\\times 0.0289 \\approx 0.0476\\). Thus the confidence interval is \\(0.30 \\pm 0.0476\\), or \\((0.2524,\\,0.3476)\\). We are 90% confident that between 25.2% and 34.8% of all vaccine recipients will experience mild side effects.\nPreference for new design. \\(\\hat{p} = 280/1000 = 0.28\\). For a 99% confidence level, \\(z_{0.005} = 2.576\\). The standard error is \\(\\sqrt{0.28\\times 0.72/1000} \\approx 0.0141\\). The margin of error is \\(2.576\\times 0.0141 \\approx 0.0363\\). The 99% confidence interval is \\((0.2437,\\,0.3163)\\). We can say with 99% confidence that between 24.4% and 31.6% of all customers prefer the new design.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimation and Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "09.html#sec-09_03",
    "href": "09.html#sec-09_03",
    "title": "9  Estimation and Confidence Intervals",
    "section": "9.3 Confidence Intervals for a Population Mean",
    "text": "9.3 Confidence Intervals for a Population Mean\n\n“Statistics cannot be any smarter than the people who use them. And in some cases, they can make smart people do dumb things.” – Charles Wheelan\n\n\nGuiding question: How do we build a confidence interval for a mean?\n\nWe’ve learned how to construct a confidence interval for a population proportion—a parameter that summarizes a categorical variable.\nNext we’ll learn how to construct a confidence interval for a population mean—a summary parameter for a quantitative variable.\nThe method resembles that for a proportion.\nThe confidence interval again has the form \\[\n\\text{point estimate} \\pm \\text{margin of error}\n\\]\nThe margin of error again equals a multiple of a standard error.\nThe sample mean \\(\\bar{x}\\) is the point estimate of the population mean \\(\\mu\\).\nIn Chapter 8, we learned that the standard deviation of the sample mean equals \\[\n\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\n\\]\nwhere \\(\\sigma\\) is the population standard deviation.\nLike the standard deviation of the sample proportion, the standard deviation of the sample mean depends on a parameter whose value is unknown, in this case \\(\\sigma\\).\nIn practice, we estimate \\(\\sigma\\) by the sample standard deviation \\(s\\).\nSo, the estimated standard deviation used in confidence intervals is the standard error, \\[\nse_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\n\\]\nAs with the proportion, the margin of error for a 95% confidence interval is roughly two standard errors.\nHowever, we need to introduce a new distribution similar to the normal distribution to give us a more precise margin of error.\nWe’ll find the margin of error by multiplying \\(se\\) by a score that is a bit larger than the z-score when \\(n\\) is small but very close to it when \\(n\\) is large.\n\nThe \\(t\\) Distribution and Its Properties\nSuppose we knew the standard deviation, \\(\\frac{\\sigma}{\\sqrt{n}}\\), of the sample mean. Then, with the additional assumption that the population is normal, with small \\(n\\) we could use the formula \\[\n\\bar{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\n\\]\nIn practice, we don’t know the population standard deviation \\(\\sigma\\). Substituting the sample standard deviation \\(s\\) for \\(\\sigma\\) then introduces extra error.\nThis error can be sizeable when \\(n\\) is small. To account for this increased error, we must replace the z-score by a slightly larger score, called a t-score.\nThe confidence interval is then a bit wider.\nThe \\(t\\)-score is like a \\(z\\)-score but it comes from a bell-shaped distribution that has slightly thicker tails than a normal distribution. This distribution is called the \\(t\\) distribution.\nThe \\(t\\) distribution resembles the standard normal distribution, being bell-shaped around a mean of 0.\nIts standard deviation is a bit larger than 1, the precise value depending on what is called the degrees of freedom, denoted by \\(df\\).\nFor inference about a population mean, the degrees of freedom equal \\[\ndf = n - 1\n\\]\n\n\nIllustration: the \\(t\\) distribution vs. the normal distribution\nThe \\(t\\) distribution resembles the standard normal distribution but has heavier tails, reflecting extra uncertainty from estimating \\(\\sigma\\) with \\(s\\). As the degrees of freedom increase, the \\(t\\) distribution converges to the normal. The plot below shows the \\(t\\) densities for \\(df=2\\), \\(df=5\\), \\(df=20\\) and the standard normal for comparison.\n\n\n\n\n\n\n\n\n\n\n\nUsing the \\(t\\) Distribution to Construct a Confidence Interval for a Mean\nThe confidence interval for a mean has a margin of error that equals a t-score times the standard error.\n\\((1-\\alpha)100%\\) Confidence Interval for a Population Mean: \\[\n\\bar{x} \\pm t_{\\alpha/2} \\frac{s}{\\sqrt{n}}\n\\]\nHere, \\(df = n - 1\\) for the t-score.\nTo use this method, you need:\n\nData obtained by randomization\nAn approximately normal population distribution\n\n\n\n\nIf the Population Is Not Normal, Is the Method Robust?\nA basic assumption of the confidence interval using the \\(t\\) distribution is that the population distribution is normal.\nThis is worrisome because many variables have distributions that are far from a bell shape. How problematic is it if we use the \\(t\\) confidence interval even if the population distribution is not normal?\nFor large random samples, it’s not problematic because of the central limit theorem. The sampling distribution is bell shaped even when the population distribution is not. But what about for small \\(n\\)?\nSuppose we had a dataset with \\(n = 11\\). For the confidence interval with this data to be valid, we must assume that the probability distribution of the population is normal.\nHow do we know if it is normal? A dot plot, histogram, or stem-and-leaf plot gives us some information about the population distribution, but it is not precise when \\(n\\) is small and it tells us little when \\(n = 11\\).\nFortunately, the confidence interval using the t distribution is a robust method in terms of the normality assumption.\nA statistical method is said to be robust with respect to a particular assumption if it performs adequately even when that assumption is modestly violated.\nEven if the population distribution is not normal, confidence intervals using t-scores usually work quite well.\nThe actual probability that the 95% confidence interval method provides a correct inference is close to 0.95 and gets closer as \\(n\\) increases.\nThe most important case when the \\(t\\) confidence interval method does not work well is when the data contain extreme outliers.\nPartly this is because of the effect on the method but also because the mean itself may not then be a representative summary of the center.\n\n\nIn Practice Assumptions Are Rarely Perfectly Satisfied\nKnowing that a statistical method is robust (that is, it still performs adequately) even when a particular assumption is violated is important because in practice assumptions are rarely perfectly satisfied.\nConfidence intervals for a mean using the \\(t\\) distribution are robust against most violations of the normal population assumption.\nHowever, you should check the data graphically to identify outliers that could affect the validity of the mean or its confidence interval.\nAlso, unless the data production used randomization, statistical inference may be inappropriate.\n\n\nExample: enzyme activity levels\nA biologist measures the activity (in units per milliliter) of a particular enzyme in a sample of \\(n=12\\) subjects. The sample mean is \\(\\bar{x} = 45\\) and the sample standard deviation is \\(s=10\\). With 11 degrees of freedom, the 95% \\(t\\)‑critical value is \\(t_{.025}=2.201\\). The standard error is \\[\ns/\\sqrt{n} = 10/\\sqrt{12} \\approx 2.89\n\\]\nThe margin of error is \\[\n2.201\\times 2.89 \\approx 6.35\n\\] Thus the 95% confidence interval for \\(\\mu\\) is \\(45 \\pm 6.35\\), or \\((38.65,\\,51.35)\\) units. We are 95% confident that the true mean enzyme activity lies within this interval.\n\n\nExample: customer satisfaction ratings\nA company surveys 40 customers about their satisfaction on a 1–5 scale and obtains a sample mean of \\(\\bar{x} = 4.2\\) with standard deviation \\(s = 0.6\\). With \\(n=40\\), the degrees of freedom are 39 and the 95% \\(t\\)‑critical value is \\(t_{0.025} \\approx 2.023\\). The standard error is \\[\n0.6/\\sqrt{40} \\approx 0.095\n\\] The margin of error is \\[\n2.023\\times 0.095 \\approx 0.19\n\\] Therefore, the 95% confidence interval for the mean satisfaction rating is \\((4.01,\\,4.39)\\). The company can report that the average satisfaction is between approximately 4.0 and 4.4 out of 5 at the 95% confidence level.\n\n\nWorking in JMP Pro 17\nTo obtain a confidence interval for a mean in JMP:\n\nUse Analyze → Distribution and assign your quantitative variable to Y.\nClick the red triangle next to the variable name and select Confidence Interval. JMP will display the interval using the \\(t\\) distribution by default when the standard deviation is estimated from the sample.\nIf you wish to specify a different confidence level, choose Confidence Interval again and adjust the level accordingly.\nFor small samples, visually examine your data using Graph Builder or Distribution to ensure the normality assumption is reasonable. If the data are heavily skewed, consider a bootstrap interval instead of relying on the \\(t\\) distribution.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nStandard error of \\(\\bar{x}\\)\nThe estimated standard deviation of the sampling distribution of \\(\\bar{x}\\), estimated by \\(s/\\sqrt{n}\\).\n\n\n\\(t\\) distribution\nA family of distributions with heavier tails than the normal, indexed by degrees of freedom; used when \\(\\sigma\\) is unknown.\n\n\nConfidence interval for \\(\\mu\\)\nThe interval \\(\\bar{x} \\pm t_{\\alpha/2}\\, s/\\sqrt{n}\\) that contains the population mean with a specified level of confidence.\n\n\n\n\n\nCheck your understanding\n\nA physician records the recovery time (in days) for 8 patients after a minor surgery: 5, 7, 6, 4, 6, 8, 5 and 9 days. Compute a 95% confidence interval for the mean recovery time.\nA manufacturing process produces bottles of juice with an unknown mean fill volume. In a sample of 25 bottles the mean fill volume is 499.2 mL with standard deviation 3.1 mL. Construct a 99% confidence interval for the true mean fill volume and interpret the result.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nRecovery time. First compute \\(\\bar{x}\\) and \\(s\\) for the eight observations (mean = 6.25 days, standard deviation = 1.667 days). With \\(n=8\\), the degrees of freedom are 7 and the 95% \\(t\\)‑critical value is \\(t_{0.025}=2.365\\). The standard error is \\(1.667/\\sqrt{8}\\approx0.589\\). The margin of error is \\(2.365\\times0.589\\approx1.39\\). The 95% confidence interval is \\(6.25\\pm1.39\\), or \\((4.86,\\,7.64)\\) days.\nBottle fill volumes. \\(\\bar{x} = 499.2\\), \\(s = 3.1\\), \\(n = 25\\) and degrees of freedom \\(\\nu=24\\). The 99% \\(t\\)‑critical value is \\(t_{0.005}=2.797\\). The standard error is \\(3.1/\\sqrt{25}=0.62\\). The margin of error is \\(2.797\\times0.62\\approx1.73\\). The confidence interval is \\(499.2\\pm1.73\\), or \\((497.47,\\,500.93)\\) mL. We can be 99% confident that the true mean fill volume lies between about 497.5 mL and 500.9 mL.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimation and Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "09.html#sec-09_04",
    "href": "09.html#sec-09_04",
    "title": "9  Estimation and Confidence Intervals",
    "section": "9.4 Choosing a Sample Size",
    "text": "9.4 Choosing a Sample Size\n\n“Quality is never an accident; it is always the result of intelligent effort.” – John Ruskin\n\n\nGuiding question: How do we choose a sample size for a desired accuracy?\n\nBefore collecting data, researchers often want to know how many observations they need to achieve a specified margin of error. Planning the sample size is crucial for budgeting time and resources. The margin of error \\(m\\) for a confidence interval depends on three quantities: the critical value (\\(z_{\\alpha/2}\\) or \\(t_{\\alpha/2}\\)), the variability of the data (standard deviation or proportion) and the sample size \\(n\\). Solving the margin‑of‑error formula for \\(n\\) yields the required sample size.\n\nSample size for estimating a mean\nIf the population standard deviation \\(\\sigma\\) is known or can be approximated from previous studies, the required sample size to estimate \\(\\mu\\) with margin of error \\(m\\) at confidence level \\((1-\\alpha)\\) is\n\\[\nn = \\left( \\frac{z_{\\alpha/2}\\,\\sigma}{m} \\right)^2.\n\\]\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nLet’s denote the margin of error \\(m\\). Then the margin of error is \\[\nm =  {z_{\\alpha/2}{\\frac{\\sigma}{\\sqrt{n}}}}\n\\]\nWe can solve this equation for \\(n\\): \\[\n\\begin{align*}\n    m =  z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} & \\Longrightarrow \\frac{m}{z_{\\alpha/2}} =  \\frac{\\sigma}{\\sqrt{n}}\\\\\\\\\n    &\\Longrightarrow \\frac{m}{z_{\\alpha/2}\\sigma} =  \\frac{1}{\\sqrt{n}}\\\\\\\\\n    & \\Longrightarrow \\left(\\frac{m}{z_{\\alpha/2}\\sigma}\\right)^2 =  \\frac{1}{n}\\\\\\\\\n    &\\Longrightarrow n = \\left( \\frac{z_{\\alpha/2}\\,\\sigma}{m} \\right)^2\\\\\\\\\n\\end{align*}\n\\]\n\n\n\nWhen \\(\\sigma\\) is unknown, you may use an estimate from a pilot study or a similar population. Always round up to the next whole number—surveying a fractional person is impossible and rounding down yields a margin of error larger than desired.\n\n\nSample size for estimating a proportion\nFor a desired margin of error \\(m\\) and confidence level \\((1-\\alpha)\\), the sample size needed to estimate a population proportion is \\[\nn = \\left( \\frac{z_{\\alpha/2}}{m} \\right)^2 p^*(1-p^*),\n\\]\nwhere \\(p^*\\) is a preliminary estimate of the true proportion.\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nLet’s denote the margin of error \\(m\\). Then the margin of error is \\[\nm =  {z_{\\alpha/2}\\sqrt{\\frac{p^*(1-p^*)}{n}}}\n\\]\nWe can solve this equation for \\(n\\): \\[\n\\begin{align*}\n    m =  z_{\\alpha/2}\\sqrt{\\frac{p^*(1-p^*)}{n}} & \\Longrightarrow{  \\frac{m}{z_{\\alpha/2}} =  \\sqrt{\\frac{p^*(1-p^*)}{n}}}\\\\\\\\\n    & {\\Longrightarrow  \\frac{m^2}{z^2_{\\alpha/2}} =  \\frac{p^*(1-p^*)}{n}}\\\\\\\\\n    & {\\Longrightarrow  \\frac{m^2}{z^2_{\\alpha/2}p^*(1-p^*)} =  \\frac{1}{n}}\\\\\\\\\n    &  {\\Longrightarrow n =  \\frac{z^2_{\\alpha/2}p^*(1-p^*)}{m^2} }\\\\\\\\\n\\end{align*}\n\\]\n\n\n\nIf no prior estimate is available, use \\(p^* = 0.5\\) to obtain the most conservative (largest) sample size. As with means, round up to the next whole number.\n\n\nExample: sample size for mean blood pressure reduction\nA medical researcher wants to estimate the mean reduction in systolic blood pressure produced by a new medication within \\(2\\) mmHg at 95% confidence. Previous studies suggest the standard deviation of reductions is about \\(8\\) mmHg. With \\(z_{\\alpha/2} = 1.96\\), the sample size is \\[\nn = \\left(\\frac{1.96 \\times 8}{2}\\right)^2 = (7.84)^2 \\approx 61.5.\n\\]\nRounding up, at least 62 participants are needed to achieve the desired accuracy.\n\n\nExample: sample size for a proportion of smokers\nPublic health officials want to estimate the proportion of adults in a city who smoke cigarettes with a margin of error of \\(\\pm0.04\\) at 95% confidence. A prior survey suggested a smoking rate of 30%. Using \\(p^* = 0.30\\) and \\(z_{\\alpha_2} = 1.96\\), \\[\nn = \\left( \\frac{1.96}{0.04} \\right)^2 0.30(1 - 0.30) = (49)^2 \\times 0.21 \\approx 504.0.\n\\]\nThus at least 505 individuals should be surveyed. If no prior estimate were available, substituting \\(p^* = 0.5\\) would yield a larger required sample size of 601.\n\n\nExample: planning a customer satisfaction survey\nSuppose a company wants to estimate the average satisfaction rating to within 0.1 points on a 5‑point scale at the 95% confidence level. Past surveys suggest the standard deviation of ratings is about 0.6. Using \\(z_{\\alpha/2} = 1.96\\), we compute \\[\nn = \\left( \\frac{1.96 \\times 0.6}{0.1} \\right)^2 = (11.76)^2 \\approx 138.3.\n\\]\nTherefore, the company should sample 139 customers to achieve the desired precision.\n\n\nExploring the relationship between sample size and margin of error\nThe margin of error for estimating a mean decreases at a rate proportional to \\(1/\\sqrt{n}\\). The plot below shows how the margin of error changes as \\(n\\) increases when \\(z_{\\alpha/2} = 1.96\\) and \\(\\sigma = 10\\). Notice that quadrupling the sample size halves the margin of error.\n\n\n\n\n\n\n\n\n\n\n\nWorking in JMP Pro 17\nJMP includes a Sample Size and Power platform that helps you plan your study:\n\nChoose Analyze → Specialized Modeling → Sample Size and Power and select the appropriate type of analysis (for means or proportions).\nFor a mean, specify the standard deviation and the desired margin of error. JMP computes the required sample size for various confidence levels and displays the results interactively.\nFor a proportion, enter the anticipated proportion \\(p^*\\) (if available) and the desired margin of error. JMP will calculate the needed sample size at different confidence levels.\nThe platform also allows you to explore how changes in confidence level or margin of error affect the sample size. Use these tools during the design phase to ensure your study is neither under‑ nor over‑powered.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nMargin of error\nHalf the width of a confidence interval; the maximum allowed difference between the point estimate and the true parameter.\n\n\nSample size for a mean\n\\(n = (z_{\\alpha/2}\\sigma/m)^2\\), the number of observations needed to estimate a mean within margin of error \\(m\\) at a given confidence level.\n\n\nSample size for a proportion\n\\(n = (z_{\\alpha/2}/m)^2 p^*(1-p^*)\\), the number of observations needed to estimate a proportion within margin of error \\(m\\).\n\n\nConservative estimate\nWhen no prior estimate for \\(p\\) is available, use \\(p^* = 0.5\\) to maximize \\(p^*(1-p^*)\\) and obtain a sample size that is large enough for any actual proportion.\n\n\nRounding up\nAlways round up the computed sample size to the next whole number to ensure the margin of error is not exceeded.\n\n\n\n\n\nCheck your understanding\n\nA dietician wants to estimate the average daily sodium intake of a population within ± 30 mg at 90% confidence. Previous studies suggest a standard deviation of 100 mg. How many individuals should she include in her sample?\nNo prior information is available about the proportion of employees who prefer working remotely. The human resources department wants to estimate this proportion within ± 0.05 at 95% confidence. What sample size is required? What if prior information suggests the true proportion is around 0.7?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nSodium intake. For a 90% confidence level, \\(z_{\\alpha/2} = 1.645\\). Plugging into the formula \\(n = (z_{\\alpha/2}\\sigma/m)^2\\) with \\(\\sigma = 100\\) and \\(m = 30\\) yields \\(n = (1.645\\times100/30)^2 \\approx (5.483)^2 = 30.07\\). Rounding up, she should sample at least 31 individuals.\nRemote work preference. Without prior information, use \\(p^* = 0.5\\). For a 95% confidence level \\(z_{\\alpha/2} = 1.96\\) and \\(m = 0.05\\). The sample size is \\(n = (1.96/0.05)^2 \\times 0.25 = (39.2)^2 \\times 0.25 = 1536.64 \\times 0.25 = 384.16\\). Rounding up, a sample of 385 employees is required. If a prior estimate is \\(p^* = 0.7\\), then \\(p^*(1-p^*) = 0.21\\) and \\(n = (1.96/0.05)^2 \\times 0.21 \\approx 323.0\\). A sample of 324 employees would suffice.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimation and Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "10.html",
    "href": "10.html",
    "title": "10  Hypothesis Testing: The Basics",
    "section": "",
    "text": "10.1 Null and Alternative Hypotheses\nGuiding question: What is a hypothesis, and how do we test it?\nAny statistical test begins with a claim about an unknown population parameter. That claim, called a hypothesis, describes what you believe might be true about a mean, a proportion, a difference, or some other quantity.\nHypotheses come in pairs: a null hypothesis and an alternative hypothesis.\nThe null hypothesis (denoted \\(H_0\\)) states that there is no effect, no difference, or no change in the population. It often represents the status quo or “nothing is happening.”\nThe alternative hypothesis (\\(H_a\\)) states that there is an effect or difference, typically what the researcher hopes to demonstrate. In a hypothesis test we start by assuming the null hypothesis is true and ask whether our sample provides enough evidence to reject it. Because samples are subject to chance, our decisions are based on probabilities rather than certainties.\nIn a legal analogy, the null hypothesis is like the presumption of innocence. A jury does not “prove” innocence; it either convicts (rejects the null) or fails to convict (fails to reject the null). Similarly, in statistics we never accept \\(H_0\\); we simply fail to reject it when the evidence is weak. This language reminds us that a test can be wrong either way (see the next section).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Hypothesis Testing: The Basics</span>"
    ]
  },
  {
    "objectID": "10.html#sec-10_01",
    "href": "10.html#sec-10_01",
    "title": "10  Hypothesis Testing: The Basics",
    "section": "",
    "text": "“If you do not know how to ask the right question, you discover nothing.” – W. E. Deming\n\n\n\n\n\n\n\n\nDefining the hypotheses\nTo construct a test, identify the parameter of interest and write down competing statements about it:\n\nPopulation parameter vs. sample statistic. A parameter is a fixed but unknown number that describes a population (e.g., the mean cholesterol level of all patients taking a drug). A statistic is a number computed from a sample and used to estimate the parameter (e.g., the mean cholesterol level of the 30 patients in a trial). Hypotheses are always statements about parameters, not statistics.\nNull hypothesis (\\(H_0\\)). The null typically asserts that the parameter equals a specific value (e.g., \\(H_0:\\mu=0\\) or \\(H_0:p=0.5\\)). It reflects the idea that nothing unusual is happening.\nAlternative hypothesis (\\(H_a\\)). The alternative expresses the research question (Sometimes, the alternative hypothesis is called the research hypothesis). It can be one-sided (greater than or less than the null value) or two-sided (simply “not equal”). Its direction determines the type of test (see Section 10.4).\n\n\n\nExamples\n\nResearchers develop a new cholesterol-lowering drug and want to know if it reduces LDL cholesterol more than the current standard. Let \\(\\mu\\) be the mean reduction in LDL. The hypotheses are\n\\[\n\\begin{align*}\n  H_0&: \\mu = 0 \\quad\\text{(no average reduction)}\\\\\\quad H_a&: \\mu &gt; 0 \\quad\\text{(mean reduction is positive)}\n\\end{align*}\n\\]\nHere the alternative is one-sided because the investigators care only about increases in reduction. They will look for evidence that the mean change is greater than zero.\nA geneticist compares the average expression level of a gene in treated cells versus control cells. Let \\(\\mu_T\\) and \\(\\mu_C\\) be the mean expression levels. If the goal is simply to detect any difference, the hypotheses are\n$$\nH_0: _T - _C = 0 \\ H_a: _T - _C $$\nThis is a two-sided alternative: the gene could be up-regulated or down-regulated, and both possibilities matter.\nA manufacturer packages coffee beans labelled “500 g.” Customers suspect the packages may weigh less than advertised. Let \\(\\mu\\) be the true mean weight. The hypotheses become\n\\[\n\\begin{align*}\nH_0&: \\mu = 500 \\\\\nH_a&: \\mu &lt; 500\n\\end{align*}\n\\]\nNotice that \\(H_a\\) is one-sided (\\(&lt;\\)). If sample data show strong evidence that the mean is lower than 500 g, the company may need to adjust its filling process.\n\nIn each example we clearly separate the null (no change) from the alternative (change in a specified direction). Remember that a test never proves the alternative; it only provides evidence against the null. The next sections discuss the risks associated with rejecting or failing to reject \\(H_0\\) and how we quantify that evidence.\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition\n\n\n\n\nHypothesis\nA claim about a population parameter that can be tested using sample data.\n\n\nNull hypothesis (\\(H_0\\))\nA statement of no effect or no difference; the hypothesis assumed true as the starting point of a test.\n\n\nAlternative hypothesis (\\(H_a\\))\nThe statement researchers hope to support; it suggests a parameter is different (greater, less, or not equal) from the null value.\n\n\n\n\n\n\nCheck your understanding\n\nA pharmaceutical company claims its new drug lowers systolic blood pressure by at least 5 mmHg compared with a placebo. Express the null and alternative hypotheses in symbols.\nA botanist believes two fertilizer treatments yield the same average plant height. What are \\(H_0\\) and \\(H_a\\) for testing this belief? Is the alternative one-sided or two-sided?\nIn your own words, explain why we never “accept” the null hypothesis in a statistical test.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nLet \\(\\mu_D\\) be the mean reduction in systolic blood pressure (drug minus placebo). The company’s claim is that the reduction is at least 5 mmHg, so we test\n\\[\nH_0: \\mu_D = 5 \\quad \\text{vs.} \\quad H_a: \\mu_D &gt; 5.\n\\]\nHere \\(H_a\\) is one-sided because we care only about reductions greater than 5 mmHg. If sample evidence shows the mean reduction exceeds 5 by more than can be attributed to chance, we will reject \\(H_0\\).\nLet \\(\\mu_1\\) and \\(\\mu_2\\) be the mean heights under fertilizers 1 and 2. The botanist’s belief is equality, so the null hypothesis is \\(H_0:\\mu_1 - \\mu_2 = 0\\). The alternative is that they differ: \\(H_a:\\mu_1 - \\mu_2 \\neq 0\\). This is a two-sided alternative because we do not know which treatment might produce taller plants.\nWe do not accept \\(H_0\\) because failing to reject it does not prove it is true. A small sample might lack power to detect a real difference, so we can only say the data do not provide strong evidence against \\(H_0\\). Using the language “fail to reject” reminds us that all conclusions are tentative and based on limited information.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Hypothesis Testing: The Basics</span>"
    ]
  },
  {
    "objectID": "10.html#sec-10_02",
    "href": "10.html#sec-10_02",
    "title": "10  Hypothesis Testing: The Basics",
    "section": "10.2 Type I and Type II Errors",
    "text": "10.2 Type I and Type II Errors\n\n“I can prove anything by statistics except the truth.” – George Canning\n\nGuiding question: What do Type I and Type II errors really mean?\nWhenever we base a decision on a sample, there is a chance of making an error. In hypothesis testing there are two kinds of error:\n\nA Type I error occurs when we reject the null hypothesis even though it is true. It’s a false positive: concluding that there is an effect when there isn’t.\nFor example, in a clinical trial we might incorrectly conclude that a new drug improves symptoms when in reality it does not.\nThe probability of a Type I error is denoted by \\(\\alpha\\), the significance level. By choosing \\(\\alpha\\) (often 0.05 or 0.01), we control how often we are willing to risk a false positive.\nA Type II error happens when we fail to reject the null hypothesis even though the alternative is true. This is a false negative: missing a real effect.\nFor instance, we might conclude that a drug doesn’t improve symptoms when in truth it does. The probability of a Type II error is \\(\\beta\\). The test’s power is \\(1-\\beta\\): the probability that the test correctly rejects \\(H_0\\) when \\(H_a\\) is true.\n\nControlling \\(\\alpha\\) makes Type I errors rare, but it increases the risk of Type II errors (and vice versa). A large sample size, clear alternative hypothesis, and good experimental design can increase power and reduce \\(\\beta\\).\n\nExamples and intuitive consequences\n\nMedical trial. Suppose we test a new vaccine. A Type I error would mean approving an ineffective vaccine, potentially exposing people to harm without benefit. A Type II error would mean dismissing a vaccine that actually works, delaying protection.\nBusiness quality control. In monitoring the mean fill weight of coffee packages, a Type I error might lead us to shut down a production line that is actually meeting specifications (costly downtime), whereas a Type II error might allow underfilled packages to reach customers (leading to complaints and regulatory problems).\nBiology experiment. When testing whether a gene is differentially expressed, a Type I error corresponds to claiming the gene is regulated when it is not (leading to wasted follow-up experiments), and a Type II error misses a truly regulated gene.\n\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition\n\n\n\n\nType I error\nRejecting the null hypothesis when it is actually true; a false positive. Its probability is the significance level \\(\\alpha\\).\n\n\nType II error\nFailing to reject the null hypothesis when the alternative is true; a false negative. Its probability is denoted \\(\\beta\\).\n\n\nSignificance level (\\(\\alpha\\))\nThe probability of committing a Type I error. It is chosen by the researcher before data are collected (common values are 0.05 or 0.01).\n\n\nPower (\\(1-\\beta\\))\nThe probability of correctly rejecting the null hypothesis when the alternative is true. Power increases with sample size, effect size, and higher significance level.\n\n\n\n\n\n\nCheck your understanding\n\nIn a courtroom analogy, a Type I error corresponds to convicting an innocent defendant, and a Type II error corresponds to letting a guilty defendant go free. Explain why society might choose a very small \\(\\alpha\\) (e.g., 0.01) in this context.\nA quality control engineer tests whether the mean diameter of machine-produced screws equals 5 mm. Describe what a Type I error and a Type II error mean in this setting. Which error would be more serious for the company?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nIn criminal justice we usually prefer to err on the side of not convicting an innocent person. A Type I error (convicting the innocent) is considered worse than a Type II error (acquitting the guilty). Choosing a very small \\(\\alpha\\) makes false convictions rare, though it increases the chance that some guilty people go free.\nA Type I error means concluding that the mean diameter differs from 5 mm when it actually equals 5 mm. This might lead to unnecessary machine adjustments or wasted product. A Type II error means concluding that the mean is 5 mm when it is not; defective screws may reach customers. Depending on regulatory requirements, the latter error could be more costly, so the engineer may accept a slightly higher \\(\\alpha\\) to reduce \\(\\beta\\).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Hypothesis Testing: The Basics</span>"
    ]
  },
  {
    "objectID": "10.html#sec-10_03",
    "href": "10.html#sec-10_03",
    "title": "10  Hypothesis Testing: The Basics",
    "section": "10.3 The p-Value and Significance Level",
    "text": "10.3 The p-Value and Significance Level\n\n“A common misconception is that an effect exists only if it is statistically significant and that it does not exist if it is not.” – Jonas Ranstam\n\nGuiding question: How do p-values guide our decisions?\nOnce the hypotheses are set, we quantify the evidence against the null hypothesis using a test statistic. The test statistic measures how far the sample data (in the form of sample statistics) are from the hypothesized value assuming \\(H_0\\) is true (e.g., the difference between the sample mean and the hypothesized mean, standardized by variability).\nFrom this statistic we compute a p-value, which is the probability of obtaining a result at least as extreme as the one observed given the null hypothesis is true. The smaller the p-value, the stronger the evidence against \\(H_0\\).\n\nInterpreting the p-value\nSuppose we test \\(H_0:\\mu=0\\) versus \\(H_a:\\mu&gt;0\\) and obtain a p-value of 0.03. This p-value means that if the true mean were zero, there is a 3% chance of obtaining a sample mean as large (or larger) than what we observed. Because such data are unlikely under \\(H_0\\), we consider rejecting \\(H_0\\). Importantly, a p-value is not the probability that the null is true; it is conditional on \\(H_0\\) being true. Small p-values provide evidence against \\(H_0\\), but they do not measure the size or importance of an effect.\nTo make a decision, we compare the p-value to the significance level \\(\\alpha\\). If the p-value is less than \\(\\alpha\\), we reject \\(H_0\\); otherwise we fail to reject it. Common significance levels are 0.05, 0.01, and 0.10. The choice should reflect the consequences of Type I and Type II errors in the context of the problem.\n\n\nExample: computing a p-value\nImagine a pharmaceutical company claims its headache medication provides immediate relief. Historically the mean time to relief is 30 minutes. In a sample of 25 patients using the new formulation, the sample mean time is 26 minutes with a standard deviation of 8 minutes. We want to test\n\\[\nH_0:\\mu = 30\\quad\\text{versus}\\quad H_a:\\mu &lt; 30\n\\]\nat \\(\\alpha=0.05\\). The null hypothesis asserts no improvement; the alternative suggests the new drug works faster. A one-sample t-test provides the test statistic and p-value. The JMP output below shows the results.\n\nThe resulting p-value will be compared to 0.05. If it is less than 0.05, we conclude that the data provide significant evidence that the new drug shortens the time to relief. Otherwise we fail to reject \\(H_0\\). For this example, the p-value is 0.0098. Thus we reject \\(H_0\\) and conclude that there is enough evidence to conclude the mean time to relief is less than 30 minutes.\n\n\nPractical vs. statistical significance\nStatistical significance tells us whether the data provide enough evidence to reject a null hypothesis. Practical significance, sometimes called clinical or substantive significance, asks whether the size of the effect is large enough to matter in the real world.\nA p-value alone cannot answer this second question. In particular, large samples can produce very small p-values for effects that are trivial in practice, while small samples may yield non-significant p-values despite substantial differences. To assess practical significance you need subject-matter expertise and, often, a pre-defined smallest effect size of interest.\nFor example, if an educational program needs to improve test scores by at least 5 points to justify its cost, a statistically significant 3-point increase would not be practically significant. Similarly, in medicine a reduction of 2 mmHg in blood pressure might achieve a tiny p-value when measured in thousands of patients, but clinicians may deem it too small to change practice.\nOne way to gauge the magnitude and precision of an effect is to report its confidence interval. A confidence interval provides a range of plausible values for the population effect size. Because it incorporates sampling variability, a CI tells us not only whether an effect exists but also how large it might be. When the entire interval lies above (or below) the threshold for practical importance, we can be more confident that the effect is both statistically and practically meaningful. If the interval straddles this threshold, the study’s estimate is imprecise: the true effect may or may not be large enough to matter. In contrast, if the CI includes zero, the data are compatible with no effect at all, and we should not claim practical significance.\nTo illustrate, consider two studies evaluating the same training program. Both report a mean improvement of 9 points in test scores and both are statistically significant. Study A has a 95% confidence interval from 3 to 15 points. Since 3 points is below the 5-point threshold, we cannot rule out the possibility that the true effect is too small to be meaningful.\nStudy B has a 95% confidence interval from 7 to 11 points. Because the entire interval exceeds 5 points, we conclude that the improvement is both statistically and practically significant.\nConfidence intervals focus attention on effect sizes and their uncertainty rather than just whether the p-value crosses a cutoff.\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition\n\n\n\n\nTest statistic\nA numerical summary of the sample that measures how far the data deviate from what is expected under the null hypothesis.\n\n\np-value\nThe probability, under the null hypothesis, of obtaining a test statistic as extreme or more extreme than the one observed.\n\n\nSignificance level (\\(\\alpha\\))\nA threshold chosen before the test that determines when to reject \\(H_0\\). If p-value &lt; \\(\\alpha\\), we reject \\(H_0\\). Common levels are 0.05, 0.01, and 0.10.\n\n\nStatistical vs. practical significance\nStatistical significance refers to small p-values; practical significance refers to whether the effect size is large enough to matter in context.\n\n\n\n\n\n\nCheck your understanding\n\nA study compares the mean yield of two corn varieties. The p-value from a test is 0.08 at \\(\\alpha=0.05\\). What conclusion would you draw? Could the difference still be important in practice?\nDescribe in words what a p-value of 0.001 means in the context of testing whether the average commute time is greater than 20 minutes.\nExplain why a p-value is not the probability that the null hypothesis is true.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nBecause 0.08 &gt; 0.05, we fail to reject the null hypothesis at the 5 % level. The data do not provide strong evidence of a difference in mean yield. However, the observed difference might still be practically important—for example, if one variety yields 10% more—but the study may lack power. Researchers could collect more data or consider the cost/benefit of switching varieties.\nA p-value of 0.001 means that if the true mean commute time were 20 minutes (the null), we would see an average as large (or larger) as the one observed only about 0.1% of the time. Such an extreme result is very unlikely under \\(H_0\\), so we have strong evidence that the average commute exceeds 20 minutes.\nThe p-value is computed assuming \\(H_0\\) is true; it measures the compatibility of the data with that assumption. It does not give the probability that \\(H_0\\) itself is true because hypotheses are statements about fixed parameters, not random events.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Hypothesis Testing: The Basics</span>"
    ]
  },
  {
    "objectID": "10.html#sec-10_04",
    "href": "10.html#sec-10_04",
    "title": "10  Hypothesis Testing: The Basics",
    "section": "10.4 One-Tailed and Two-Tailed Tests",
    "text": "10.4 One-Tailed and Two-Tailed Tests\n\n“As Confucius might have said, if the difference isn’t different enough to make a difference, what’s the difference?” – V. Chew\n\nGuiding question: How do one-tailed and two-tailed tests differ?\nWhen you write the alternative hypothesis you must decide whether it specifies a direction. If \\(H_a\\) asserts that a parameter is greater than or less than the null value, the test is one-tailed (or one-sided). If \\(H_a\\) simply asserts that the parameter is not equal to the null value, the test is two-tailed (or two-sided). This decision affects both the p-value and the interpretation of results.\n\nOne-tailed tests\nA one-tailed test asks whether a parameter is significantly higher or lower than the null value. For example, a company might test whether a new marketing campaign increases average sales. The hypotheses are \\(H_0:\\mu=\\mu_0\\) versus \\(H_a:\\mu&gt;\\mu_0\\). Only values of the test statistic in one direction count as evidence against \\(H_0\\).\nBecause we focus on a single tail of the distribution, the p-value for a given statistic is half that of a two-tailed test for the same data. However, one-tailed tests ignore large deviations in the opposite direction, so you should only use them when increases (or decreases) in the other direction are irrelevant or impossible. You must decide this before looking at the data.\n\n\nTwo-tailed tests\nA two-tailed test asks whether a parameter is different from the null value, regardless of direction. For example, researchers might test whether a new fertilizer yields a different average plant height, without specifying whether it is higher or lower. The hypotheses are \\(H_0:\\mu=\\mu_0\\) versus \\(H_a:\\mu\\neq\\mu_0\\).\nEvidence on both sides of the sampling distribution counts against \\(H_0\\). Because the tails share the significance level, the p-value is roughly twice that of a one-tailed test for the same statistic.\n\n\nExample: comparing one- and two-tailed p-values\nSuppose a manufacturer packages orange juice with a nominal volume of 250 mL. In a sample of 40 cartons, the mean volume is 248.5 mL with a standard deviation of 4 mL. We test whether the mean volume is less than 250 mL (one-tailed) and whether it differs from 250 mL (two-tailed).\n\nBecause the sample mean is lower than 250 mL, the one-tailed p-value is smaller than the two-tailed p-value. If the sample mean had been above 250 mL, the one-tailed p-value would be large (close to 1) because the deviation in the “wrong” direction provides no evidence for \\(H_a\\). Always choose the alternative before analyzing data; otherwise you risk biasing your results.\n\n\nChoosing between one- and two-tailed tests\n\nUse a two-tailed test when you care about differences in either direction or when you want a more conservative test (it requires more evidence to reject \\(H_0\\)). Many scientific studies default to two-tailed alternatives to avoid the risk of missing an effect in the unexpected direction.\nUse a one-tailed test only when values in the opposite direction are irrelevant, impossible, or would not change your conclusion. For instance, if a regulatory standard specifies that a contaminant must not exceed 50 ppm, you may test \\(H_0:\\mu=50\\) versus \\(H_a:\\mu&gt;50\\).\n\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition\n\n\n\n\nOne-tailed test\nA hypothesis test with a directional alternative (greater than or less than). Evidence is evaluated in one tail of the sampling distribution.\n\n\nTwo-tailed test\nA test with a non-directional alternative (not equal). Evidence in both tails counts against the null hypothesis. The p-value is roughly twice the one-tailed p-value for the same statistic.\n\n\nDirectional vs. non-directional\nDirectional alternatives specify “greater than” or “less than”; non-directional alternatives specify “not equal.” Choosing the direction must occur before seeing the data.\n\n\n\n\n\n\nCheck your understanding\n\nA dietician wants to know if a new dietary program changes average blood glucose levels compared to the current standard. Write the null and alternative hypotheses and state whether the test should be one- or two-tailed.\nA safety inspector measures the decibel level of a machine that is regulated not to exceed 90 dB. Should the test of \\(H_0:\\mu=90\\) be one- or two-tailed? Explain why.\nIf a one-tailed t-test yields a p-value of 0.04, what would be the p-value of the corresponding two-tailed test (approximately) for the same data? Why is it different?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nLet \\(\\mu\\) be the mean blood glucose level under the new program. To detect any change (increase or decrease) the hypotheses are \\(H_0:\\mu = \\mu_0\\) versus \\(H_a:\\mu \\neq \\mu_0\\). This is a two-tailed test because both higher and lower glucose levels matter. If the dietician is only concerned about decreases (for example, reducing high glucose levels), a one-tailed test could be used, but the decision must be made before looking at the data.\nBecause the regulation is about exceeding a maximum, the relevant test is \\(H_0:\\mu = 90\\) versus \\(H_a:\\mu &gt; 90\\). This is a one-tailed alternative: we care only about the machine being too loud. If the measured average is below 90 dB, we will not shut down the machine even if it is lower than 90 by a large margin.\nFor symmetric distributions, the two-tailed p-value is approximately twice the one-tailed p-value. If the one-tailed p-value is 0.04, then the two-tailed p-value is about 0.08. The two-tailed test divides the total significance level between the two tails, making it harder to reject \\(H_0\\) for a given statistic.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Hypothesis Testing: The Basics</span>"
    ]
  },
  {
    "objectID": "10.html#sec-10_05",
    "href": "10.html#sec-10_05",
    "title": "10  Hypothesis Testing: The Basics",
    "section": "10.5 The Steps of Hypothesis Testing",
    "text": "10.5 The Steps of Hypothesis Testing\n\n“My philosophy is basically this, and this is something that I live by, and I always have, and I always will: Don’t ever, for any reason, do anything, to anyone, for any reason, ever, no matter what, no matter where, or who, or who you are with, or where you are going, or where you’ve been, ever, for any reason whatsoever.” – Michael Scott\n\nGuiding question: How do we structure the steps of a test?\nConducting a hypothesis test involves a systematic sequence of decisions. There are four main steps: state the hypotheses, formulate an analysis plan, analyze the data, and interpret the results. Here we break down those steps and illustrate them in practice.\n\nStep 1: State the hypotheses\nIdentify the population parameter of interest and formulate the null and alternative hypotheses. Specify whether the alternative is one-sided or two-sided. The hypotheses should reflect the research question.\n\n\nStep 2: Formulate the analysis plan\nChoose the significance level \\(\\alpha\\) (the probability of a Type I error) and select an appropriate test statistic. The statistic depends on the type of data and the assumptions you can make. Also decide whether your test will be one-tailed or two-tailed and check conditions (such as normality or independence).\n\n\nStep 3: Analyze the data\nCompute the test statistic from your sample and determine the p-value. Modern software (including JMP, R, or Excel) readily calculates p-values. Underlying calculations often involve standardizing the statistic by the standard error and referring to a distribution (t, normal, chi-square, etc.).\n\n\nStep 4: Interpret the results in context\nCompare the p-value to the chosen significance level. If \\(p &lt; \\alpha\\), reject the null hypothesis; otherwise fail to reject it. Then translate this conclusion back to the practical context of the problem. Remember to discuss both statistical and practical significance and consider potential errors. Summarize what the result means for stakeholders.\n\n\nExample: testing patient wait times\nSuppose administrators of a hospital claim that the average wait time in the emergency department is 30 minutes. A patient advocacy group believes the true mean is longer. They collect a random sample of 40 patients and find an average wait time of 35 minutes with a standard deviation of 12 minutes. We will test\n\\[\nH_0:\\mu = 30 \\quad\\text{versus}\\quad H_a:\\mu &gt; 30\n\\]\nat \\(\\alpha=0.05\\). Here’s how we carry out the test in JMP:\n\nThe t-test computes a test statistic of approximately 2.6352 and a p-value of approximately 0.006. Because the p-value is less than 0.05, we reject \\(H_0\\) and conclude that the average wait time is significantly longer than 30 minutes. From a practical standpoint, a five-minute difference (35 minutes instead of 30) may be important if it reflects overcrowding and patient frustration. We construct a 95% confidence interval as:\n\nNot only do we see that there is enough evidence to conclude the mean wait time is greater than 30 minutes, we can also be 95% confident that the true mean wait time is in the interval \\((31.16, 38.83)\\). Thus, we do not have enough evidence to say the mean wait time is greater than 35 minutes.\n\n\nPutting it all together\nWhen you conduct hypothesis tests in practice, follow this template:\n\nState the hypotheses in terms of the population parameter and decide if the alternative is one- or two-sided.\nChoose the significance level \\(\\alpha\\) and the appropriate test statistic, verifying assumptions.\nCompute the test statistic and p-value using software or formulas.\nDraw a conclusion: compare the p-value to \\(\\alpha\\), decide whether to reject \\(H_0\\), and interpret the result in context.\n\nThis structured approach ensures that your analysis is transparent and reproducible.\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition\n\n\n\n\nHypothesis test steps\nThe sequence of state hypotheses, choose significance level and test statistic, analyze the data (compute test statistic and p-value), and interpret results.\n\n\n\n\n\n\nCheck your understanding\n\nOutline the four steps of hypothesis testing in your own words and explain why each step is important.\nA company advertises that the average life of its lightbulbs is 2,000 hours. You take a sample of 50 bulbs and find a mean life of 1930 hours with a standard deviation of 250 hours. At \\(\\alpha=0.05\\), test whether the mean life is less than advertised. Write out the steps and provide the conclusion.\nIn Step 2 you must choose the significance level. What factors should you consider when deciding between \\(\\alpha=0.05\\) and \\(\\alpha=0.01\\)?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nStep 1: State \\(H_0\\) and \\(H_a\\) clearly in terms of the population parameter. This ensures the test addresses the research question. Step 2: Choose \\(\\alpha\\) and the test statistic appropriate for your data, and verify assumptions. This controls the probability of Type I errors and ensures validity. Step 3: Compute the statistic and p-value using your sample. This translates data into evidence. Step 4: Compare the p-value to \\(\\alpha\\) and interpret the result in context, considering both statistical and practical significance. Communicating the conclusion completes the analysis.\nLet \\(\\mu\\) be the mean life of the bulbs. Step 1: \\(H_0:\\mu = 2000\\) versus \\(H_a:\\mu &lt; 2000\\) (one-tailed). Step 2: Choose \\(\\alpha=0.05\\) and a one-sample t-test (population sd unknown). Step 3: Compute the test statistic and p-value:\n\n\n         t \n-0.9116943 \n\n\n[1] 0.1831966\n\n\nSuppose the p-value is around 0.03 (the actual value depends on random simulation). Step 4: Since p &lt; 0.05, we reject \\(H_0\\) and conclude that the average bulb life is significantly less than 2 000 hours. The company may need to revise its claim or improve manufacturing.\nThe choice of \\(\\alpha\\) reflects how serious a Type I error would be. A smaller \\(\\alpha\\) (e.g., 0.01) reduces the chance of falsely claiming a discovery but increases the chance of missing a real effect (Type II error). Consider the consequences: in a medical trial for a life-saving drug, you may tolerate a higher \\(\\alpha\\) to avoid missing an effective treatment; in a court case, you might choose a very small \\(\\alpha\\) to minimize false convictions. Practical and ethical considerations should guide the decision.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Hypothesis Testing: The Basics</span>"
    ]
  },
  {
    "objectID": "11.html",
    "href": "11.html",
    "title": "11  Hypothesis Testing with Categorical Response",
    "section": "",
    "text": "11.1 Response and Explanatory Variables\nEvery statistical study involves variables that play different roles. Two of the most important are the response variable and the explanatory variable.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing with Categorical Response</span>"
    ]
  },
  {
    "objectID": "11.html#sec-response-explanatory",
    "href": "11.html#sec-response-explanatory",
    "title": "11  Hypothesis Testing with Categorical Response",
    "section": "",
    "text": "“Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write.” - H.G. Wells\n\n\nGuiding question: How do we distinguish between the variable we are trying to understand and the variable(s) that might explain it?\n\n\n\nResponse Variable\nThe response variable (sometimes called the dependent variable or outcome variable) measures the result or outcome we want to understand, predict, or explain.\nExamples include:\n\nIn a medical study, the response variable might be whether a patient recovers (yes/no) or the reduction in blood pressure (in mmHg).\nIn a business context, it could be sales revenue, customer satisfaction score, or whether a customer makes a repeat purchase.\nIn biology, it could be plant height, growth rate, or presence of a mutation.\n\nThe response variable answers the question: “What outcome are we interested in?”\n\n\nExplanatory Variable\nThe explanatory variable (also called the independent variable, predictor, or factor) is the variable that we think might help explain or predict changes in the response.\nExamples include:\n\nIn a clinical trial, the explanatory variable could be treatment type (new drug vs. placebo).\nIn a marketing experiment, it might be advertising strategy or price level.\nIn an ecological study, it could be amount of sunlight or soil type affecting plant growth.\n\nSometimes there are multiple explanatory variables. For instance, a researcher studying cholesterol levels might consider both diet and exercise level.\n\n\nRelationship Between the Two\n\nWe use explanatory variables to describe or predict changes in the response variable.\nThe direction of explanation goes from explanatory → response, not the other way around.\n\n\n\n\n\n\n\n\n\nType of Study\nResponse Variable\nExplanatory Variable(s)\n\n\n\n\nMedical Trial\nWhether patient’s blood pressure decreases\nDrug type (placebo vs. new drug)\n\n\nBusiness Survey\nCustomer satisfaction (1–10 scale)\nWait time, price, or service quality\n\n\nBiology Experiment\nGrowth of seedlings (cm)\nAmount of fertilizer or sunlight\n\n\n\nIn experiments, researchers manipulate the explanatory variable to study its effect on the response. In observational studies, the explanatory variable is merely recorded, not controlled—so we must be cautious about claiming causation.\n\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition\n\n\n\n\nResponse variable\nThe outcome or result measured in a study; the variable of primary interest.\n\n\nExplanatory variable\nThe variable thought to explain or predict changes in the response.\n\n\nDependent vs. independent\n“Dependent” refers to the response variable; “independent” refers to the explanatory variable.\n\n\n\n\n\n\nCheck your understanding\n\nIdentify the response and explanatory variables in each scenario:\n\n\nA company studies whether providing flexible work hours increases employee productivity.\n\n\nA biologist examines whether soil pH affects seed germination rate.\n\n\nAn economist studies whether interest rates influence consumer spending.\n\n\nIn which of the above examples would it be appropriate to claim a cause-and-effect relationship? Why?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n\n\n\n\nResponse: employee productivity; Explanatory: work-schedule type (flexible vs. fixed).\n\n\nResponse: germination rate; Explanatory: soil pH.\n\n\nResponse: consumer spending; Explanatory: interest rate.\n\n\n\nCause-and-effect can be claimed only in a true experiment—typically case (a) or (b) if the explanatory variable is controlled by the researcher. In (c), interest rates are not manipulated by the researcher but merely observed, so the study is observational and cannot establish causation.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing with Categorical Response</span>"
    ]
  },
  {
    "objectID": "11.html#sec-11_01",
    "href": "11.html#sec-11_01",
    "title": "11  Hypothesis Testing with Categorical Response",
    "section": "11.2 One Sample Test for a Proportion",
    "text": "11.2 One Sample Test for a Proportion\n\n“60% of the time, it works every time.” - Brian Fantana\n\n\nGuiding question: How do we test whether a single proportion equals a hypothesized value?\n\nTo decide on a statistical method, we should first start with asking what type of response variable we have (or will have). There are two main types: quantitative and categorical.\nAfter we determine the type of the response variable, we should then determine the explanatory variables (if any). We can think of this in terms of a flowchart:\n\nFor the rest of this chapter, we will focus on a categorical response variable (the blue side on the flowchart). We will then cover different methods depending on the type and number of explanatory variables.\n\nHypotheses and assumptions\nOur first method is the scenario where the response is categorical but there are no explanatory variables. In particular, the response variable is binary and we are interested in making an inference of the proportion of the population for one of those categories.\n\nLet \\(p\\) denote the true proportion of interest and let \\(p_0\\) be the hypothesized value.\nThe null hypothesis states that the population proportion equals the hypothesized value. The alternative hypothesis may be two‑sided if we simply want to detect any difference, or one‑sided if we are interested in increases or decreases.\n\\[\n\\begin{align*}\nH_0&: p = p_0\\\\\nH_a&: p \\ne p_0\\quad\\text{ or }\\quad p&gt;p_0\\quad\\text{ or }\\quad p&lt;p_o\n\\end{align*}\n\\]\nLike all inference procedures, this test requires certain conditions. We assume a simple random sample from a population of independent Bernoulli trials. Each trial has two categories (success/failure) and a constant probability of success. We must also have at least 15 successes and 15 failures (\\(np\\geq15\\) and \\(n(1-p)\\geq15\\)) so that the normal approximation is reasonable. These “success–failure” conditions ensure the sampling distribution of the sample proportion is approximately normal.\n\n\nTest statistic and p‑value\nGiven a sample of size \\(n\\) with \\(x\\) successes, the sample proportion is \\(\\hat{p}=x/n\\). Under \\(H_0\\) the test statistic\n\\[\nz = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}}\n\\]\nmeasures how many standard errors the sample proportion deviates from the hypothesized proportion. For a two‑sided alternative the p‑value is \\(2P(Z &gt; |z|)\\); for a one‑sided alternative we compute \\(P(Z &gt; z)\\) or \\(P(Z &lt; z)\\) depending on the direction. If the p‑value is less than the significance level \\(\\alpha\\), we reject \\(H_0\\) in favor of \\(H_a\\).\n\n\nPost‑hoc confidence interval\nWhen the test is significant, a confidence interval for \\(p\\) provides context. A \\((1-\\alpha)\\times100\\%\\) interval is\n\\[\n\\hat{p} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}},\n\\]\nwhere \\(z_{\\alpha/2}\\) is the critical value from the standard normal distribution. This interval uses the sample proportion in place of the hypothesized value in the standard error. It gives a range of plausible values for \\(p\\) and helps assess practical importance.\n\n\nExample – Side effects of a vaccine\nSuppose a pharmaceutical company wishes to evaluate whether fewer than a quarter of patients experience a mild rash after receiving a vaccine. In a random sample of \\(n=200\\) vaccinated patients, \\(x=38\\) report a rash. We test\n\\[\nH_0: p = 0.25 \\quad\\text{versus}\\quad H_a: p &lt; 0.25\n\\]\nusing \\(\\alpha=0.05\\). The sample proportion is \\(\\hat{p} = 38/200 = 0.19\\). Plugging into the test statistic formula gives\n\\[\nz = \\frac{0.19 - 0.25}{\\sqrt{\\frac{0.25\\times 0.75}{200}}} \\approx -2.06.\n\\]\nFor a one‑sided test the p‑value is \\(P(Z &lt; -2.06) \\approx 0.0198\\), which is below 0.05. We reject \\(H_0\\) and conclude that the rash rate is lower than 25%.\nA 95 % confidence interval for \\(p\\) is 5 \\[\n0.19 \\pm 1.96\\sqrt{\\frac{0.19\\times0.81}{200}} = (0.13, 0.244).\n\\]\nThis interval suggests the true proportion lies somewhere between 13.5% and 24.4%, consistent with our conclusion.\n\nPerforming the one‑sample proportion test in JMP Pro 18\n\nEnter or import the data. Create a column for the binary response (e.g., “Rash”) coded with two categories (Yes/No). If you have counts rather than individual observations, include a Freq column containing the counts.\nLaunch the Distribution platform. Choose Analyze → Distribution, assign the response to Y and the Freq column (if present) to Freq, and click OK. JMP will display counts, proportions and a bar chart.\nTest the proportion. Click the red triangle next to the variable’s name and choose Test Probabilities. Enter the hypothesized proportion \\(p_0\\); JMP reports the Pearson \\(\\chi^2\\) statistic and associated p‑value. The Pearson statistic is \\(z^2\\) for a two‑sided test, so you can recover \\(z\\) by taking its square root and assigning the sign based on \\(\\hat{p}-p_0\\).\nConfidence interval. Under the same menu choose Confidence Interval. Specify the confidence level (e.g., 0.95). JMP calculates the interval using the standard normal approximation as above.\n\n\n\n\nExample – Color morph frequency\nA biologist studying a species of lizard notes that 40 out of 150 captured individuals have a rare blue color morph. Is there evidence that the morph frequency differs from 20%? We set up\n\\[\nH_0: p = 0.20 \\quad\\text{vs.}\\quad H_a: p \\neq 0.20.\n\\]\nWe could perform this test by manually doing the calculations like the last example. Instead, we will use the Calculators available in JMP 18 Student Edition.\n\nThe p‑value is \\(P(Z &gt; 2.0412) =0.0412\\). At \\(\\alpha=0.05\\) we reject \\(H_0\\); the data provides enough evidence to conclude that the blue morph frequency differs from 20%.\n\nA 95% confidence interval for \\(p\\) is \\((0.1959, 0.3374)\\). Note that the confidence interval just passes 0.20 on the lower limit. This is due to the confidence interval uses \\(\\hat p\\) in the standard error where as the test statistic uses the hypothesized value \\(p_0\\). A larger study might yield a more definitive conclusion.\n\nExample – Customer satisfaction\nA retailer advertises that 85% of its customers are satisfied with the online shopping experience. An independent survey of 250 recent customers finds that 192 report being satisfied. We test\n\\[\nH_0: p = 0.85 \\quad\\text{vs.}\\quad H_a: p &lt; 0.85.\n\\]\nHere \\(\\hat{p} = 192/250 = 0.768\\). The z‑statistic is \\((0.768-0.85)/\\sqrt{0.85\\times 0.15/250} \\approx -3.34\\) with p‑value \\(&lt;0.001\\). The data provide strong evidence that the true satisfaction rate is below the advertised 85%. A 9 % confidence interval for \\(p\\) is \\((0.72, 0.81)\\), well below 0.85. Companies often report high satisfaction percentages in advertisements, but independent surveys can reveal gaps between perception and reality.\n\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition/Formula\n\n\n\n\nNull hypothesis \\(H_0: p=p_0\\)\nAssumes the population proportion equals the hypothesized value \\(p_0\\).\n\n\nAlternative hypothesis\nSpecifies whether \\(p\\) differs from \\(p_0\\) (two‑sided) or is greater/less than \\(p_0\\) (one‑sided).\n\n\nZ‑statistic\n\\(z = (\\hat{p}-p_0)/\\sqrt{p_0(1-p_0)/n}\\).\n\n\nConfidence interval for \\(p\\)\n\\(\\hat{p}\\pm z_{\\alpha/2}\\sqrt{\\hat{p}(1-\\hat{p})/n}\\).\n\n\n\n\n\n\nCheck your understanding\n\nA survey asks whether citizens of a town support building a new park. Out of 400 respondents, 112 say yes. Test at the 0.05 level whether the support rate differs from 25%. Find and interpret a 95% confidence interval.\nA biologist believes that 30% of a wildflower species carry a recessive allele. In a sample of 120 plants, 46 carry the allele. Set up and perform a one‑sided test at \\(\\alpha=0.10\\) to determine whether the allele frequency is greater than 30%.\nExplain why the z‑statistic uses the hypothesized proportion \\(p_0\\) in the denominator when calculating the test statistic, but the confidence interval uses \\(\\hat{p}\\).\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nThe sample proportion is \\(\\hat{p}=112/400=0.28\\). Test \\(H_0:p=0.25\\) vs. \\(H_a:p\\neq0.25\\) at \\(\\alpha=0.05\\). The z‑statistic is \\((0.28-0.25)/\\sqrt{0.25\\times0.75/400}\\approx1.39\\) with two‑sided p‑value 0.164. We fail to reject \\(H_0\\); the support rate is not significantly different from 25 %. A 95 % confidence interval is \\(0.28\\pm1.96\\sqrt{0.28\\times0.72/400}= (0.23,0.33)\\), which includes 0.25.\nHere \\(\\hat{p}=46/120=0.383\\). We test \\(H_0:p=0.30\\) vs. \\(H_a:p&gt;0.30\\) at \\(\\alpha=0.10\\). The z‑statistic is \\((0.383-0.30)/\\sqrt{0.30\\times0.70/120}\\approx2.07\\) with one‑sided p‑value 0.019. Since 0.019 &lt; 0.10, we reject \\(H_0\\) and conclude the allele frequency exceeds 30 %. A 90 % one‑sided confidence bound would start at \\(\\hat{p}-1.28\\sqrt{\\hat{p}(1-\\hat{p})/n}=0.383-1.28\\times0.042=0.33\\); we are 90 % confident that the allele frequency is at least 33 %.\nUnder the null hypothesis the sampling distribution of \\(\\hat{p}\\) is centered at \\(p_0\\) with variance \\(p_0(1-p_0)/n\\), so the z‑statistic uses \\(p_0\\) to measure how far the sample proportion deviates from what we expect when \\(H_0\\) is true. For a confidence interval we treat \\(\\hat{p}\\) as our best estimate of \\(p\\) and use its estimated standard error \\(\\sqrt{\\hat{p}(1-\\hat{p})/n}\\) to account for sampling variability.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing with Categorical Response</span>"
    ]
  },
  {
    "objectID": "11.html#sec-11_02",
    "href": "11.html#sec-11_02",
    "title": "11  Hypothesis Testing with Categorical Response",
    "section": "11.3 Test for Difference in Proportions",
    "text": "11.3 Test for Difference in Proportions\n\n“The essence of mathematics is not to make simple things complicated, but to make complicated things simple.” - S. Gudder\n\n\nGuiding question: When comparing two independent groups with a binary outcome, how can we test whether their proportions differ?\n\nIn clinical trials, marketing experiments and ecological studies we often compare two treatments or populations. Each unit yields a binary response (success/failure), and the explanatory variable indicates group membership. To decide whether a difference observed in the sample reflects a true difference in the populations we use a two‑sample proportion test.\n\n\nHypotheses and assumptions\nLet \\(p_1\\) and \\(p_2\\) denote the proportions in groups 1 and 2, and let \\(\\hat{p}_1\\) and \\(\\hat{p}_2\\) be the sample proportions from independent random samples of sizes \\(n_1\\) and \\(n_2\\). The null hypothesis is \\(H_0: p_1 = p_2\\), implying no group effect. The alternative can be two‑sided (\\(H_a: p_1 \\neq p_2\\)) or one‑sided (e.g., \\(H_a: p_1 &gt; p_2\\)).\nThe success–failure conditions must hold for each group separately: \\(n_1\\hat{p}_1\\geq 15\\), \\(n_1(1-\\hat{p}_1)\\geq 15\\), \\(n_2\\hat{p}_2\\geq 15\\) and \\(n_2(1-\\hat{p}_2)\\geq 15\\). We also require that the samples are independent of each other and that observations within each sample are independent. Meeting these conditions allows us to approximate the sampling distribution of \\(\\hat{p}_1 - \\hat{p}_2\\) by a normal distribution.\n\n\nTest statistic\nUnder \\(H_0\\) we assume the common population proportion is \\(p\\) and estimate it by the pooled proportion\n\\[\np = \\frac{x_1 + x_2}{n_1 + n_2},\n\\]\nwhere \\(x_1\\) and \\(x_2\\) are the numbers of successes in groups 1 and 2. The test statistic is\n\\[\nz = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{p(1-p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}},\n\\]\nFor a two‑sided alternative the p‑value is \\(2P(Z &gt; |z|)\\); for one‑sided tests we compute \\(P(Z &gt; z)\\) or \\(P(Z &lt; z)\\) depending on the direction. A small p‑value indicates evidence against \\(H_0\\).\n\n\nPost‑hoc confidence interval and multiple comparisons\nWhen we reject \\(H_0\\) (or even if we do not), it is informative to estimate the difference in proportions with a confidence interval. Using the sample proportions (not pooled), the standard error is\n\\[\nSE = \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}},\n\\]\nand a \\((1-\\alpha)\\times100\\%\\) confidence interval for \\(p_1 - p_2\\) is\n\\[\n(\\hat{p}_1 - \\hat{p}_2) \\pm z_{\\alpha/2} SE.\n\\]\nUnlike the test statistic, the confidence interval does not use the pooled proportion because it aims to estimate the true difference rather than test a null hypothesis.\n\n\nExample– Treatment success rates\nA clinical study compares recovery rates between two therapies for treating a viral infection. Therapy A is given to 150 patients, 105 of whom recover within a week; therapy B is given to 130 patients, with 72 recoveries. We test\n\\[\nH_0: p_A = p_B \\quad\\text{vs.}\\quad H_a: p_A &gt; p_B\n\\]\nat \\(\\alpha=0.05\\). The sample proportions are \\(\\hat{p}_A = 105/150 = 0.70\\) and \\(\\hat{p}_B = 72/130 \\approx 0.554\\). The pooled proportion is\n\\[\np = \\frac{105 + 72}{150 + 130} = \\frac{177}{280} = 0.632.\n\\]\nThe test statistic is\n\\[\nz = \\frac{0.70 - 0.554}{\\sqrt{0.632\\times(1-0.632)\\left(\\frac{1}{150} + \\frac{1}{130}\\right)}} \\approx 2.68.\n\\]\nThe one‑sided p‑value is \\(P(Z &gt; 2.68) \\approx 0.0037\\). Since 0.0037 &lt; 0.05, we reject \\(H_0\\) and conclude therapy A has a higher recovery rate. A 95% confidence interval for \\(p_A - p_B\\) uses the standard error with unpooled proportions:\n\\[\nSE = \\sqrt{\\frac{0.70\\times0.30}{150} + \\frac{0.554\\times0.446}{130}} \\approx 0.062,\n\\]\nso the interval is \\((0.70-0.554) \\pm 1.96 \\times 0.062 = (0.031, 0.238)\\). We are 95% confident that therapy A’s recovery rate exceeds therapy B’s by between 3.1% and 23.8%.\n\n\nExample – Genotype frequencies\nResearchers investigate whether the frequency of a deleterious allele differs between male and female mice. Among 80 males, 24 carry the allele; among 90 females, 12 do. Testing \\(H_0:p_M = p_F\\) vs. \\(H_a:p_M \\neq p_F\\), we have \\(\\hat{p}_M=0.30\\) and \\(\\hat{p}_F=0.133\\). The pooled proportion is \\((24+12)/(80+90)=36/170=0.212\\). The z‑statistic is\n\\[\nz=\\frac{0.30 - 0.133}{\\sqrt{0.212\\times0.788\\left(\\frac{1}{80}+\\frac{1}{90}\\right)}} \\approx 3.24,\n\\]\nyielding a two‑sided p‑value of about 0.0012. We reject \\(H_0\\). At a 5% significance level, there is enough evidence to conclude the frequency of a deleterious allele differs between male and female mice. A 95% confidence interval is \\((0.30-0.133) \\pm 1.96\\sqrt{0.30\\times0.70/80 + 0.133\\times0.867/90} = (0.064, 0.262)\\).\n\n\nExample – A/B testing\nAn online retailer runs an A/B experiment to compare two website layouts. Group A (old layout) has 1,800 visitors with 144 purchases; Group B (new layout) has 1,900 visitors with 190 purchases. We test\n\\[H_0:p_A = p_B \\quad\\text{vs.}\\quad H_a:p_A \\neq p_B.\\]\nLet’s do the test with JMP 18 Student Edition.\n\nThe two‑sided p‑value is about 0.0339, indicating a significant difference. At the 5% significance level, there is enough evidence to conclude the the proportion of visitors to the old layout who made a purchase differs from the proportion who made a purchase to the new layout.\n\nA 95% confidence interval for \\(p_A - p_B\\) is \\((-0.0384, -0.00016)\\), so the new layout increases purchase rates by roughly 0.02% to 3.8%.\n\nPerforming the two‑sample proportion test in JMP Pro 18\n\nPrepare your data. Each row should correspond to an individual with a column for the binary response (e.g., “Purchased”) and a column for group membership (“Layout”). If you have aggregated counts, include a Freq column with the counts.\nUse the Contingency platform. Choose Analyze → Fit Y by X, assign the response variable to Y, the group to X, and (if present) the counts column to Freq. Click OK to produce a contingency table.\nRun the two‑sample test. Click the red triangle next to “Contingency Analysis” and choose Two Sample Test for Proportions. JMP reports the difference in sample proportions, the test statistic and p‑value, and a confidence interval for the difference. Use the “Cell Chi Square” option to view contributions to the chi‑square statistic.\nMultiple comparisons. For pairwise comparisons, rerun the two‑sample test on each pair and adjust \\(\\alpha\\) using the Bonferroni correction to maintain the overall error rate.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition/Formula\n\n\n\n\nTwo‑sample test for proportions\nTests \\(H_0: p_1 = p_2\\) using a pooled estimate of the common proportion and a z‑statistic.\n\n\nPooled proportion \\(p\\)\n\\((x_1+x_2)/(n_1+n_2)\\), used in the denominator of the test statistic under \\(H_0\\).\n\n\nStandard error for CI\n\\(\\sqrt{\\hat{p}_1(1-\\hat{p}_1)/n_1 + \\hat{p}_2(1-\\hat{p}_2)/n_2}\\).\n\n\n\n\n\n\nCheck your understanding\n\nA vaccine trial compares adverse event rates between men and women. Out of 250 men, 38 report an adverse event; out of 300 women, 22 do. At \\(\\alpha=0.05\\), test whether the rates differ and construct a 95% confidence interval for the difference.\nWhy is the pooled proportion used in the test statistic but not in the confidence interval for \\(p_1 - p_2\\)?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n\\(\\hat{p}_{\\text{men}}=38/250=0.152\\) and \\(\\hat{p}_{\\text{women}}=22/300=0.073\\). The pooled proportion is \\((38+22)/(250+300)=0.1067\\). The z‑statistic is \\((0.152-0.073)/\\sqrt{0.1067\\times0.8933\\,(1/250+1/300)}\\approx2.98\\) with two‑sided p‑value 0.003. We reject \\(H_0\\); men experience adverse events more often. A 95 % CI uses the unpooled SE \\(\\sqrt{0.152\\times0.848/250+0.073\\times0.927/300}\\approx0.025\\); the interval is \\((0.079\\pm1.96\\times0.025)=(0.029,0.129)\\), so the difference ranges from 2.9 % to 12.9 %.\nUnder the null hypothesis \\(p_1 = p_2\\), the sampling distribution of \\(\\hat{p}_1-\\hat{p}_2\\) is centered at zero with variance \\(p(1-p)(1/n_1+1/n_2)\\), where \\(p\\) is the common value. The pooled proportion estimates this common value. For confidence intervals we no longer assume \\(p_1=p_2\\); instead we estimate each population proportion separately to capture the true difference.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing with Categorical Response</span>"
    ]
  },
  {
    "objectID": "11.html#sec-11_03",
    "href": "11.html#sec-11_03",
    "title": "11  Hypothesis Testing with Categorical Response",
    "section": "11.4 Chi‑Square Goodness of Fit",
    "text": "11.4 Chi‑Square Goodness of Fit\n\n“As far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality.” - Albert Einstein\n\n\nGuiding question: How can we test whether observed category frequencies match a theoretical distribution?\n\nThe chi‑square goodness‑of‑fit test assesses whether a single categorical variable follows a specified distribution. Examples include testing whether the distribution of blood types in a sample matches known population proportions, whether Mendelian inheritance ratios (9 : 3 : 3 : 1) hold in a genetics experiment, or whether market shares of product categories agree with company projections.\n\n\nHypotheses and assumptions\nSuppose there are \\(k\\) categories with expected proportions \\(p_{1,0},\\dots,p_{k,0}\\) under the null hypothesis. We observe counts \\(O_i\\) for category \\(i\\) in a random sample of size \\(n\\). The null hypothesis asserts that the population follows the specified distribution: \\(H_0: p_i = p_{i,0}\\) for all \\(i\\). The alternative hypothesis is that at least one category proportion differs.\nFor the chi‑square goodness‑of‑fit test to be valid the data must be counts of a categorical variable, collected from a simple random sample, and that expected counts \\(E_i = n\\,p_{i,0}\\) should be at least 5 in each category. These conditions ensure the chi‑square approximation is accurate.\n\n\nTest statistic\nFor each category compute the expected count \\(E_i = n p_{i,0}\\). The chi‑square statistic\n\\[\n\\chi^2 = \\sum_{i=1}^k \\frac{(O_i - E_i)^2}{E_i}\n\\]\nmeasures the discrepancy between the observed and expected frequencies. Under \\(H_0\\) the statistic has a chi‑square distribution with \\(k-1\\) degrees of freedom. Because large deviations in either direction contribute to the sum of squared standardized differences, the test is always one‑sided: large \\(\\chi^2\\) values yield small p‑values. If the p‑value is less than \\(\\alpha\\), we reject \\(H_0\\) and conclude the observed distribution does not fit the specified proportions.\n\n\nPost‑hoc analysis:Bonferroni confidence intervals\nA significant chi‑square statistic tells us that the observed distribution differs from the expected one, but it does not identify which categories contribute most. To diagnose the differences, construct separate confidence intervals for each category proportion: \\[\n\\hat{p}_i \\pm z_{\\alpha^*/2}\\sqrt{\\hat{p}_i(1-\\hat{p}_i)/n}\n\\]\nComparing these intervals to the hypothesized proportions reveals which categories differ from expectation.\n\nThe Bonferroni Confidence Intervals\nWhen we construct confidence intervals for multiple categories at once, each interval has its own chance of error. If we build \\(k\\) intervals each at the same confidence level (\\(1-\\alpha\\)), the probability that at least one of them fails to capture the true parameter can exceed \\(\\alpha\\). This inflation of the overall (family-wise) error rate is known as the multiple comparisons problem.\nThe Bonferroni correction provides a simple and conservative way to control this overall error rate. Instead of using the full significance level \\(\\alpha\\) for each interval, we divide it equally among the \\(k\\) intervals. That is, we use a per-comparison error rate of\n\\[\n\\alpha^* = \\frac{\\alpha}{k}\n\\]\nand construct each interval with confidence level\n\\[\n1 - \\alpha^* = 1 - \\frac{\\alpha}{k}.\n\\]\nFor example, if we want an overall 95% family confidence level across five categories (\\(k=5\\)), we set \\(\\alpha^* = 0.05/5 = 0.01\\). Each individual interval then has confidence level \\(1 - 0.01 = 0.99\\). The resulting family of intervals jointly maintains an overall confidence of about 95%.\n\n\nWhy It Works\nThe justification comes from a basic probability inequality known as Bonferroni’s inequality, which states that for any collection of events \\(A_1, A_2, \\ldots, A_k\\),\n\\[\nP(A_1 \\cup A_2 \\cup \\ldots \\cup A_k) \\leq P(A_1) + P(A_2) + \\ldots + P(A_k).\n\\]\nIf we interpret each \\(A_i\\) as the event that the \\(i\\)th confidence interval fails to contain its true proportion, then\n\\[\nP(\\text{at least one interval fails}) \\leq k \\times \\alpha^* = \\alpha.\n\\]\nThus, the probability that all intervals simultaneously contain their true values is at least \\(1 - \\alpha\\). In other words, the Bonferroni method guarantees that the family-wise confidence level is no smaller than the desired value.\nBonferroni intervals are conservative—they make it slightly harder to declare significance—but they ensure that the overall Type I error rate does not exceed the nominal level. This conservatism is often desirable in confirmatory analyses, such as identifying which categories deviate from a hypothesized distribution after a significant chi-square test.\n\n\n\nExample – Mendelian inheritance\nIn a classic genetics experiment, pea plants are self‑fertilized to examine the distribution of phenotypes produced by a dihybrid cross. Mendel’s theory predicts a 9 : 3 : 3 : 1 ratio of phenotypes. Suppose a biologist observes the following counts in 1,600 offspring: 900 have phenotype A, 320 have phenotype B, 300 have phenotype C and 80 have phenotype D. We test\n\\[\n\\begin{align*}\n&H_0:p_1 = 0.5625, p_2=0.1875, p_3 = 0.1875, p_4 = 0.0625\\\\\n&H_a:\\text{at least one differs}\n\\end{align*}\n\\]\nThe expected counts based on the 9 : 3 : 3 : 1 proportions are \\[\n(0.5625, 0.1875, 0.1875, 0.0625)\\times1600 = (900, 300, 300, 100)\n\\]\nThe chi‑square statistic is\n\\[\n\\begin{align*}\n\\chi^2 &= \\frac{(900-900)^2}{900} + \\frac{(320-300)^2}{300} + \\frac{(300-300)^2}{300} + \\frac{(80-100)^2}{100}\\\\\n&\\approx 5.33\n\\end{align*}\n\\]\nWith 3 degrees of freedom the p‑value is about 0.1490, above 0.05. We fail to reject \\(H_0\\) and cannot conclude the observed distribution differs from the predicted Mendelian ratio.\nWe can perform the analysis in JMP to get the\n\nWe can find the Bonferroni confidence intervals by first calculating \\[\n\\begin{align*}\n1 - \\alpha^* &= 1 - \\frac{\\alpha}{k}\\\\\n& = 1-\\frac{0.05}{4}\\\\\n& = 1-0.0125\\\\\n& = 0.9875\n\\end{align*}\n\\] Thus, we are finding 98.75% confidence intervals:\n\n\nPerforming the goodness‑of‑fit test in JMP Pro 18\n\nEnter the counts. Create a column for the categorical variable and another for the observed counts (if the data are aggregated). Alternatively, enter one row per observation.\nAnalyze the distribution. Choose Analyze → Distribution, assign the categorical variable to Y and, if using aggregated counts, the counts column to Freq. Click OK to see the observed counts and bar chart.\nSpecify expected proportions. Click the red triangle next to the variable name and choose Test Probabilities. In the dialog, set the expected probabilities (e.g., 0.45, 0.40, 0.11, 0.04) or equal proportions. JMP displays the Pearson chi‑square statistic, degrees of freedom and p‑value.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition/Formula\n\n\n\n\nGoodness‑of‑fit test\nEvaluates whether observed categorical data follow a specified distribution.\n\n\nExpected count\n\\(E_i = n p_{i,0}\\); should be at least 5 in each category.\n\n\nChi‑square statistic\n\\(\\sum (O_i-E_i)^2/E_i\\); follows a chi‑square distribution with \\(k-1\\) degrees of freedom.\n\n\nBonferroni adjustment\nDivide \\(\\alpha\\) by the number of categories when examining individual residuals to control the family‑wise error rate.\n\n\n\n\n\n\nCheck your understanding\n\nA genetics lab predicts a 3 : 1 ratio of dominant to recessive phenotypes in a monohybrid cross. In a sample of 500 offspring there are 375 dominant and 125 recessive. Perform a goodness‑of‑fit test at \\(\\alpha=0.05\\) and interpret the standardized residuals.\nA restaurant anticipates that dinner orders will be evenly split among four meal types (fish, chicken, beef, vegetarian). One night the observed counts are (80, 60, 100, 60) out of 300 orders. Test whether the distribution differs from expectation and identify which meals are unusually popular or unpopular. Use a Bonferroni correction.\nWhy is the chi‑square goodness‑of‑fit test always one‑sided? How does this affect the calculation of the p‑value?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nExpected counts under a 3 : 1 ratio are \\((375,125)\\). The chi‑square statistic is \\((375-375)^2/375+(125-125)^2/125=0\\) with p‑value 1. We fail to reject \\(H_0\\); the observed distribution perfectly matches the prediction.\nExpected counts are (75, 75, 75, 75). The chi‑square statistic is \\(((80-75)^2+(60-75)^2+(100-75)^2+(60-75)^2)/75 = 20\\). With 3 degrees of freedom the p‑value is about 0.00016, so the distribution differs significantly. Standardized residuals are \\((0.58,-1.73,2.89,-1.73)\\). With Bonferroni adjustment (\\(\\alpha/4=0.0125\\)) the critical value is \\(\\approx 2.49\\); only the beef category residual of 2.89 exceeds this threshold. Beef orders are significantly more frequent than expected.\nThe chi‑square statistic sums squared standardized deviations, so any deviation from expected counts (whether above or below) increases the statistic. Thus a large chi‑square statistic indicates lack of fit regardless of the direction of deviations. The p‑value is the upper‑tail probability \\(P(\\chi^2_{k-1} \\geq \\chi^2)\\); there is no “lower tail” of interest because small values indicate good fit, not evidence against the null.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing with Categorical Response</span>"
    ]
  },
  {
    "objectID": "11.html#sec-11_04",
    "href": "11.html#sec-11_04",
    "title": "11  Hypothesis Testing with Categorical Response",
    "section": "11.5 Chi‑Square Test for Association",
    "text": "11.5 Chi‑Square Test for Association\n\n“Although we often hear that data speak for themselves, their voices can be soft and sly.” - Frederick Mosteller\n\n\nGuiding question: Are two categorical variables associated? If so, how can we identify the pattern of association?\n\nThe chi‑square test for association (also called the chi‑square test of independence) examines whether two categorical variables are related. It generalizes the two‑sample proportion test when the explanatory variable has more than two categories or when the response variable has more than two categories. Examples include assessing the association between smoking status and lung disease, genotype and phenotype, or customer gender and product preference.\n\n\nHypotheses and assumptions\nSuppose a contingency table has \\(r\\) rows (categories of the response variable) and \\(c\\) columns (categories of the explanatory variable). Let \\(O_{ij}\\) be the observed count in cell \\((i,j)\\) with row total \\(R_i\\), column total \\(C_j\\) and grand total \\(N\\). The null hypothesis posits that the two variables are independent; equivalently, \\[\nP(\\text{row }i \\text{ and column }j) = P(\\text{row }i)×P(\\text{column }j)\n\\]\nThe alternative states that at least one cell’s probability differs from the product of its marginal probabilities. Independence implies no association; rejecting it suggests an association exists.\nThe data must be counts from a simple random sample, both variables must be categorical, and all expected counts should be at least 5. These conditions parallel those of the goodness‑of‑fit test and ensure the chi‑square approximation is valid.\n\n\nTest statistic\nFor each cell compute the expected count under independence:\n\\[\nE_{ij} = \\frac{R_i C_j}{N}.\n\\]\nThe chi‑square statistic is\n\\[\n\\chi^2 = \\sum_{i=1}^r \\sum_{j=1}^c \\frac{(O_{ij}-E_{ij})^2}{E_{ij}},\n\\]\nwhich follows a chi‑square distribution with \\((r-1)(c-1)\\) degrees of freedom under \\(H_0\\). As with the goodness‑of‑fit test, the p‑value is the upper‑tail probability: large values indicate evidence of association.\n\n\nPost‑hoc analysis\nA significant result signals that the variables are associated but does not reveal the nature of the association. Two common post‑hoc approaches are:\n\nStandardized and adjusted residuals. Compute cell residuals \\(r_{ij} = (O_{ij}-E_{ij})/\\sqrt{E_{ij}}\\). Residuals with absolute value greater than about 2 highlight cells contributing to the chi‑square statistic. Adjusted Pearson residuals account for the degrees of freedom and have approximately a standard normal distribution. Apply a Bonferroni correction by dividing \\(\\alpha\\) by the number of cells examined when identifying significant residuals.\nPairwise proportion tests. For a table with more than two columns, perform two‑sample proportion tests on pairs of columns. For example, if you have 3 columns there are 3 pairwise comparisons; use \\(\\alpha^*=\\alpha/3\\) when computing p‑values or confidence intervals.\n\nYou can also compute measures of association such as Cramér’s V, which rescales the chi‑square statistic to lie between 0 (no association) and 1 (perfect association).\n\n\nExample 11.10 – Smoking status and lung disease (medicine)\nA health researcher investigates the relationship between smoking status (non‑smoker, past smoker, current smoker) and diagnosis of a chronic lung disease (present or absent). The observed counts for 400 participants are:\n\n\n\n\nDisease present\nDisease absent\nTotal\n\n\n\n\nNon‑smoker\n20\n180\n200\n\n\nPast smoker\n25\n75\n100\n\n\nCurrent smoker\n30\n70\n100\n\n\nTotal\n75\n325\n400\n\n\n\nUnder independence the expected count for the first cell is \\((200\\times75)/400=37.5\\), and similarly for the other cells. The chi‑square statistic is\n\\[\n\\chi^2 = \\sum_{i,j}\\frac{(O_{ij}-E_{ij})^2}{E_{ij}} \\approx 20.923,\n\\]\nwith \\((3-1)(2-1)=2\\) degrees of freedom. The p‑value is &lt;0.0001. We reject the null hypothesis; smoking status and lung disease are associated.\nIn JMP, the output looks like\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerforming the chi‑square test for association in JMP Pro 18\n\nOrganize your data. Each row should represent an individual with two categorical variables. If you have aggregated counts, include a Freq column.\nUse the Contingency platform. Select Analyze → Fit Y by X. Assign one variable to Y, the other to X, and the counts to Freq if needed. Click OK to obtain the contingency table.\nRun the chi‑square test. JMP displays the chi‑square statistic, degrees of freedom and p‑value. Verify that all expected counts exceed 5.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition/Formula\n\n\n\n\nChi‑square test for association\nTests independence between two categorical variables; the statistic sums \\((O_{ij}-E_{ij})^2/E_{ij}\\) over all cells.\n\n\nExpected count\n\\(E_{ij} = (R_i C_j)/N\\); must be at least 5 in each cell.\n\n\nDegrees of freedom\n\\((r-1)(c-1)\\) for an \\(r\\times c\\) contingency table.\n\n\n\n\n\n\nCheck your understanding\n\nA study records the preferred news source (TV, radio, internet, newspaper) and political affiliation (Democrat, Republican, Independent) for 900 voters. Describe how to test for independence between news source and political affiliation. If the overall test is significant, how would you identify which cells contribute most to the association?\nThe table below shows the relationship between genotype (AA, Aa, aa) and survival (survived, died) in a sample of 450 organisms. Perform a chi‑square test for association and interpret the results. Then compute standardized residuals and determine which genotype categories differ most from expectation.\n\n\n\n\n\nSurvived\nDied\nTotal\n\n\n\n\nAA\n130\n20\n150\n\n\nAa\n120\n30\n150\n\n\naa\n80\n70\n150\n\n\nTotal\n330\n120\n450\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nEnter counts in a contingency table with rows for political affiliation and columns for news source. Compute expected counts \\(E_{ij}=R_i C_j/N\\) and the chi‑square statistic \\(\\chi^2 = \\sum (O_{ij}-E_{ij})^2/E_{ij}\\). With \\((r-1)(c-1)\\) degrees of freedom, find the p‑value and decide whether to reject independence. If significant, calculate standardized residuals for each cell and compare them to a Bonferroni‑adjusted critical value (e.g., \\(\\alpha/(rc)\\)). Cells with large positive residuals occur more often than expected; large negative residuals occur less often.\nExpected counts: row totals × column totals divided by 450. For AA survived: \\((150×330)/450=110\\); for AA died: 40; for Aa survived: 110; Aa died: 40; aa survived: 110; aa died: 40. The chi‑square statistic is \\(((130-110)^2/110 + (20-40)^2/40 + (120-110)^2/110 + (30-40)^2/40 + (80-110)^2/110 + (70-40)^2/40)=\\ldots\\) which simplifies to 39.09. With \\((3-1)(2-1)=2\\) degrees of freedom the p‑value is essentially zero, so genotype and survival are associated. Standardized residuals: AA survived \\((130-110)/\\sqrt{110}=1.91\\), AA died \\((20-40)/\\sqrt{40}=-3.16\\), Aa survived \\((120-110)/\\sqrt{110}=0.95\\), Aa died \\((30-40)/\\sqrt{40}=-1.58\\), aa survived \\((80-110)/\\sqrt{110}=-2.86\\), aa died \\((70-40)/\\sqrt{40}=4.74\\). After Bonferroni adjustment (\\(\\alpha/6\\)), the critical value is about 2.73. The aa died cell (4.74) and AA died cell (–3.16) show significant deviations: aa individuals die more often than expected while AA individuals die less often.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing with Categorical Response</span>"
    ]
  },
  {
    "objectID": "12.html",
    "href": "12.html",
    "title": "12  Methods for Quantitative Response Variables – One and Two Groups",
    "section": "",
    "text": "12.1 One‑Sample Tests for the Mean and Variance\nIn the previous chapter we focused on categorical outcomes. Here our response variable is quantitative, meaning it is measured on a numeric scale such as weight, blood pressure, cholesterol level, enzyme activity, revenue or time. To decide on an appropriate analysis we must consider how many explanatory variables we have and whether the data are paired or independent. The table below summarizes the situations covered in this chapter:\nWhen there is no explanatory variable, we have a single quantitative sample. Two natural questions arise:\nWe address these with the one‑sample t-test and the chi‑square test for the variance.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Methods for Quantitative Response Variables – One and Two Groups</span>"
    ]
  },
  {
    "objectID": "12.html#onesample-tests-for-the-mean-and-variance",
    "href": "12.html#onesample-tests-for-the-mean-and-variance",
    "title": "12  Methods for Quantitative Response Variables – One and Two Groups",
    "section": "",
    "text": "Is the population mean equal to a specified value?\nIs the population variance equal to a specified value?\n\n\n\n\nOne‑sample t‑test for a mean\nSuppose we have a random sample \\(x_1, x_2,\\dots,x_n\\) from a population with unknown mean \\(\\mu\\) and standard deviation \\(\\sigma\\). We want to test\n\\[\n\\begin{align*}\n&H_0: \\mu = \\mu_0 \\\\\n&H_a: \\mu \\neq \\mu_0\\quad (\\text{or } \\mu &gt; \\mu_0,\\ \\mu &lt; \\mu_0)\n\\end{align*}\n\\]\n\nAssumptions\nLike all inference methods, the t‑test rests on assumptions. In practice we require:\n\nA simple random sample from the population of interest; observations are independent of one another.\nThe population is normally distributed, or the sample size is large (typically \\(n \\ge 30\\)) so that the Central Limit Theorem ensures approximate normality.\n\n\n\nTest statistic and p‑value\nUnder \\(H_0\\), the test statistic\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}\n\\]\nfollows a Student’s \\(t\\) distribution with \\(n-1\\) degrees of freedom. We obtain the p‑value by comparing the observed \\(|t|\\) to the \\(t\\) distribution. For a two‑sided alternative the p‑value is \\(2P(T_{n-1} &gt; |t|)\\); for a one‑sided alternative we compute \\(P(T_{n-1} &gt; t)\\) or \\(P(T_{n-1} &lt; t)\\) depending on the direction.\n\n\nConfidence interval\nWhen the test is significant (or even when it is not), a \\((1-\\alpha)\\times100\\%\\) confidence interval for \\(\\mu\\) gives a range of plausible values. The interval is\n\\[\n\\bar{x} \\pm t_{\\alpha/2}\\,\\frac{s}{\\sqrt{n}},\n\\]\nwhere \\(t_{\\alpha/2}\\) is the critical value from the \\(t\\) distribution.\n\n\nExample – Leaf nitrogen content\nIn a nutrient study a botanist measures the nitrogen content (in % dry weight) of 15 leaves from a wildflower species. The sample mean is 3.4% with a standard deviation of 0.6%. Test whether the mean differs from 3.0% at the 5% significance level.\nSolution. The hypotheses are \\(H_0: \\mu = 3.0\\) vs. \\(H_a: \\mu \\neq 3.0\\). The test statistic is\n\\[\nt = \\frac{3.4 - 3.0}{0.6/\\sqrt{15}} \\approx 2.58.\n\\]\nWith \\(n-1 = 14\\) degrees of freedom the two‑sided p‑value is \\(2P(T_{14} &gt; 2.58) \\approx 0.021\\). Since 0.021 &lt; 0.05, we reject \\(H_0\\) and conclude that mean nitrogen content differs from 3.0%. A 95% confidence interval is \\(3.4 \\pm t_{0.025,14}\\times0.6/\\sqrt{15} = (3.05, 3.75)\\).\n\n\nPerforming the one‑sample mean test in JMP 18\nTo carry out this test in JMP 18 Student Edition:\n\nEnter the data. Create a column for the quantitative variable (e.g., “HeartRate”).\nLaunch the Distribution platform. Select Analyze → Distribution, assign the variable to Y, and click OK. JMP displays summary statistics and a histogram.\nTest the mean. Click the red triangle next to the variable name and choose Test Mean. Enter the hypothesized value \\(\\mu_0\\) and specify the alternative (two‑sided or one‑sided). JMP reports the \\(t\\) statistic, degrees of freedom, and p‑value. The same dialog box allows you to request a confidence interval.\n\n\n\n\nChi‑square test for a variance\nSometimes we are interested in assessing whether the population variance equals a specified value \\(\\sigma_0^2\\). For example, an engineer may want to know whether the variability of machine parts exceeds a tolerance level. Let \\(s^2\\) be the sample variance computed from a random sample of size \\(n\\). To test\n\\[\n\\begin{align*}\n&H_0: \\sigma^2 = \\sigma_0^2\\\\\n&H_a: \\sigma^2 \\neq \\sigma_0^2 \\quad (\\text{or } &gt;, &lt;)\n\\end{align*}\n\\]\nwe use the test statistic\n\\[\n\\chi^2 = \\frac{(n-1)s^2}{\\sigma_0^2},\n\\]\nwhich has a chi‑square distribution with \\(n-1\\) degrees of freedom under \\(H_0\\). Large values of \\(\\chi^2\\) indicate that \\(s^2\\) is bigger than expected under \\(H_0\\); small values indicate it is too small. For a two‑sided alternative the p‑value is \\(P(\\chi^2_{n-1} &gt; \\chi^2_\\text{obs})\\) for one tail plus \\(P(\\chi^2_{n-1} &lt; \\chi^2_\\text{obs})\\) for the other.\n\nConfidence interval\nA \\((1-\\alpha)\\times100\\%\\) confidence interval for the population variance is\n\\[\n\\left(\\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2}},\\ \\frac{(n-1)s^2}{\\chi^2_{\\alpha/2}}\\right),\n\\]\nwhere \\(\\chi^2_{\\alpha/2, n-1}\\) and \\(\\chi^2_{1-\\alpha/2, n-1}\\) are critical values from the chi‑square distribution. Taking square roots yields a confidence interval for the standard deviation.\n\n\nExample – Manufacturing variability\nA company manufactures springs with a target variance in spring length of \\(\\sigma_0^2 = 0.04\\ \\mathrm{cm}^2\\). A quality‑control engineer measures 25 springs and computes a sample variance of \\(s^2 = 0.065\\ \\mathrm{cm}^2\\). Test whether the variance differs from the target at the 5% level.\nSolution. With \\(n-1 = 24\\) degrees of freedom, the test statistic is \\[\n\\begin{align*}\n\\chi^2 &= \\frac{(24)(0.065)}{0.04}\\\\\n&= 39.0\n\\end{align*}\n\\]\nFor a two‑sided test, the p‑value is about \\(2P(\\chi^2&gt;39.0)=0.055\\). We fail to reject \\(H_0\\). A 95% confidence interval for \\(\\sigma^2\\) is\n\\[\n\\left(\\frac{24\\times0.065}{39.4},\\ \\frac{24\\times0.065}{12.4}\\right) = (0.0396, 0.1256),\n\\]\nso the variance may range from 0.04 to 0.13 cm².\n\n\nPerforming the variance test in JMP 18\nThere is no dedicated “chi‑square test for variance” command in JMP. However, you can assess variability through graphical tools and by constructing the confidence interval manually. To do this:\n\nCompute the sample variance \\(s^2\\) and degrees of freedom \\(n-1\\).\nFind the critical chi‑square values using a table or the distribution calculator in JMP (Analyze → Distribution → Continuous Distribution).\nPlug into the confidence interval formula above.\n\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition/Formula\n\n\n\n\nOne‑sample t‑test\nTests \\(H_0: \\mu=\\mu_0\\) using \\(t = (\\bar{x}-\\mu_0)/(s/\\sqrt{n})\\) with \\(n-1\\) df.\n\n\nConfidence interval for mean\n\\(\\bar{x} \\pm t_{\\alpha/2,n-1} \\times s/\\sqrt{n}\\).\n\n\nChi‑square test for variance\nTests \\(H_0: \\sigma^2 = \\sigma_0^2\\) using \\(\\chi^2 = (n-1)s^2/\\sigma_0^2\\) with \\(n-1\\) df.\n\n\nConfidence interval for variance\n\\(\\left(\\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2,n-1}},\\ \\frac{(n-1)s^2}{\\chi^2_{\\alpha/2,n-1}}\\right)\\).\n\n\n\n\n\n\nCheck your understanding\n\nThe caffeine content of a new energy drink is claimed to be 80 mg per can. In a lab study 12 cans have a mean caffeine level of 77 mg and a standard deviation of 5 mg. At \\(\\alpha=0.05\\) test whether the average caffeine content differs from 80 mg and compute a 95% confidence interval.\nA plant geneticist measures the variance in seed weight for a hybrid strain and obtains \\(s^2=0.25\\,\\text{g}^2\\) from \\(n=16\\) seeds. Test whether the true variance differs from 0.20 g² at \\(\\alpha=0.10\\).\nWhy does the t‑test for the mean use the sample standard deviation \\(s\\) in the denominator instead of the known standard deviation \\(\\sigma\\)?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nTest of the mean: \\(H_0: \\mu=80\\) vs. \\(H_a: \\mu\\neq80\\). Here \\(\\bar{x}=77\\), \\(s=5\\) and \\(n=12\\), so \\(t=(77-80)/(5/\\sqrt{12})\\approx-2.08\\). With \\(n-1=11\\) df the two‑sided p‑value is \\(2P(T_{11}&gt;2.08)\\approx0.062\\). At \\(\\alpha=0.05\\) we fail to reject \\(H_0\\). A 95% CI is \\(77 \\pm t_{0.025,11}\\times5/\\sqrt{12}=(73.0,81.0)\\); this interval contains 80 mg.\nWith \\(n-1=15\\) df the test statistic is \\(\\chi^2=(15\\times0.25)/0.20=18.75\\). The critical values at \\(\\alpha=0.10\\) are \\(\\chi^2_{0.95,15}\\approx7.26\\) and \\(\\chi^2_{0.05,15}\\approx24.99\\). Since 18.75 lies between these, we fail to reject \\(H_0\\). The data do not provide enough evidence that the variance differs from 0.20 g².\nIn practice we rarely know the population standard deviation \\(\\sigma\\). The t‑test treats \\(s\\) as an estimate of \\(\\sigma\\). Using \\(s\\) introduces additional uncertainty, so the sampling distribution becomes a t distribution rather than a normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Methods for Quantitative Response Variables – One and Two Groups</span>"
    ]
  },
  {
    "objectID": "12.html#two-independent-samples-students-ttest-and-welchs-ttest",
    "href": "12.html#two-independent-samples-students-ttest-and-welchs-ttest",
    "title": "12  Methods for Quantitative Response Variables – One and Two Groups",
    "section": "12.2 Two Independent Samples: Student’s t‑Test and Welch’s t‑Test",
    "text": "12.2 Two Independent Samples: Student’s t‑Test and Welch’s t‑Test\nWhen comparing means between two independent groups, the appropriate procedure depends on whether the population variances are equal. We consider two methods: the pooled (Student’s) t‑test and Welch’s t‑test.\n\n\nStudent’s (pooled) t‑test\nLet \\(x_{1i}\\) and \\(x_{2j}\\) be independent random samples from populations with means \\(\\mu_1\\) and \\(\\mu_2\\) and common variance \\(\\sigma^2\\). To test \\[\n\\begin{align*}\n&H_0: \\mu_1 = \\mu_2\\\\\n&H_a: \\mu_1 \\neq \\mu_2\n\\qquad\\text{(or one‑sided)}\n\\end{align*}\n\\]\nwe compute\n\nPooled estimate of variance:\n\n\\[\ns_p^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}\n\\]\n\nTest statistic:\n\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p\\sqrt{1/n_1 + 1/n_2}}\n\\]\nThe statistic follows a \\(t\\) distribution with \\(n_1 + n_2 - 2\\) degrees of freedom. Assumptions include:\n\nTwo independent random samples.\nEach population is approximately normally distributed.\nThe population variances are equal (\\(\\sigma_1^2 = \\sigma_2^2\\)).\n\nA \\((1-\\alpha)\\times100\\%\\) confidence interval for \\(\\mu_1 - \\mu_2\\) is\n\\[\n(\\bar{x}_1 - \\bar{x}_2) \\pm t_{\\alpha/2,}\\,s_p \\sqrt{1/n_1 + 1/n_2}.\n\\]\n\n\nWelch’s (nonpooled) t‑test (unequal variances)\nWhen the assumption of equal variances is violated, we use Welch’s t‑test (also called the nonpooled t-test). This test modifies the standard error and the degrees of freedom. The test statistic is\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{s_1^2/n_1 + s_2^2/n_2}},\n\\]\nand the degrees of freedom are approximated by\n\\[\n\\nu = \\frac{\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right)^2}{\\frac{(s_1^2/n_1)^2}{n_1 - 1} + \\frac{(s_2^2/n_2)^2}{n_2 - 1}}.\n\\]\nThis formula yields a non‑integer value; software uses it to compute the p‑value. The confidence interval for \\(\\mu_1 - \\mu_2\\) uses the same standard error and the \\(t_{\\nu}\\) distribution.\n\n\nAssumptions and choosing the test\nThe assumptions for two‑sample t‑tests: the groups must be independent, each sample should come from a normal distribution, and the variances should be equal for the pooled test. If variances are not equal, the Welch test is preferred. In practice, Welch’s t-test should be used by default unless it is known the variances are equal.\n\n\nExample – Ad campaign effectiveness\nA marketing analyst compares sales increases after two different advertising campaigns. Sales increases for 20 regions under campaign A have mean 1.8% and SD = 1.0%; for 18 regions under campaign B the mean is 1.2% with SD = 0.6%. We do not know if the population variances are equal so we use Welch’s t-test:\n\\[\nt = \\frac{1.8 - 1.2}{\\sqrt{1.0^2/20 + 0.6^2/18}} = 2.17.\n\\]\nThe Welch degrees of freedom are\n\\[\n\\nu = \\frac{\\left(\\frac{1.0^2}{20} + \\frac{0.6^2}{18}\\right)^2}{\\frac{(1.0^2/20)^2}{19} + \\frac{(0.6^2/18)^2}{17}} \\approx 31.\n\\]\nWith \\(\\nu\\approx31\\) df, the two‑sided p‑value is 0.036. We reject \\(H_0\\); campaign A yields a greater increase in sales. A 95% CI for the difference is \\((0.037, 1.163)\\) percentage points.\n\n\nPerforming the two‑sample tests in JMP 18\n\nOrganize the data. Each row corresponds to an observation; include a response column and a group column.\nFit Y by X. Choose Analyze → Fit Y by X, assign the response variable to Y and the group to X, and click OK.\nPooled t‑test (equal variances). From the red triangle select Means/ANOVA/Pooled t. JMP reports the pooled t statistic, p‑value and confidence interval.\nUnequal variances t‑test (Welch). From the red triangle select Unequal Variances. At the bottom of the report JMP presents the Welch t‑test results; use these when variances differ or by default for robustness.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition/Formula\n\n\n\n\nPooled t‑test\nTests \\(H_0: \\mu_1=\\mu_2\\) assuming equal variances; statistic \\(t=(\\bar{x}_1-\\bar{x}_2)/(s_p\\sqrt{1/n_1+1/n_2})\\) with df \\(n_1+n_2-2\\).\n\n\nWelch’s (nonpooled) t‑test\nTests \\(H_0: \\mu_1=\\mu_2\\) without assuming equal variances; statistic \\((\\bar{x}_1-\\bar{x}_2)/\\sqrt{s_1^2/n_1+s_2^2/n_2}\\) with approximate df given by Welch’s formula.\n\n\n\n\n\n\nCheck your understanding\n\nResearchers compare the mean glucose levels of two independent groups of mice (n = 15 per group). Group 1 has \\(\\bar{x}_1=110\\,\\text{mg/dL}\\) and \\(s_1=12\\); Group 2 has \\(\\bar{x}_2=103\\,\\text{mg/dL}\\) and \\(s_2=15\\). Assume equal variances in the populations. Test whether the mean glucose levels differ at the 5% level and compute a 95% confidence interval for the difference.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nSince the F‑test is not significant, we use the pooled t‑test. The pooled standard deviation is \\(s_p=\\sqrt{(14\\times12^2+14\\times15^2)/28}=13.54\\). The test statistic is \\(t=(110-103)/(13.54\\sqrt{1/15+1/15})=1.52\\) with df = 28; the two‑sided p‑value is 0.14. We fail to reject \\(H_0\\); the mean glucose levels do not differ significantly. A 95% CI is \\((110-103) \\pm t_{0.025,28}\\,13.54\\sqrt{2/15} = (-2.56, 16.56)\\) mg/dL.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Methods for Quantitative Response Variables – One and Two Groups</span>"
    ]
  },
  {
    "objectID": "12.html#equality-of-variances-ftest-and-alternatives",
    "href": "12.html#equality-of-variances-ftest-and-alternatives",
    "title": "12  Methods for Quantitative Response Variables – One and Two Groups",
    "section": "12.3 Equality of Variances: F‑Test and Alternatives",
    "text": "12.3 Equality of Variances: F‑Test and Alternatives\n\nThe F‑test compares two sample variances to evaluate\n\\[\nH_0: \\sigma_1^2 = \\sigma_2^2 \\quad\\text{vs.}\\quad H_a: \\sigma_1^2 \\neq \\sigma_2^2.\n\\]\n\nTest statistic and assumptions\nLet \\(s_1^2\\) and \\(s_2^2\\) be the sample variances from independent random samples of sizes \\(n_1\\) and \\(n_2\\) drawn from normally distributed populations. Without loss of generality assume \\(s_1^2 \\ge s_2^2\\). The test statistic\n\\[\nF = \\frac{s_1^2}{s_2^2}\n\\]\nhas an F distribution with \\((n_1-1, n_2-1)\\) degrees of freedom under \\(H_0\\). Large values of \\(F\\) indicate that \\(s_1^2\\) is much larger than \\(s_2^2\\); very small values (which correspond to \\(s_2^2\\) being larger) are captured by the symmetry of the F distribution.\nAssumptions include:\n\nTwo independent random samples.\nEach population is normally distributed. The F‑test is sensitive to departures from normality, so a normality check is essential.\n\n\n\nConfidence interval for the ratio of variances\nWe can estimate the ratio of population variances \\(\\sigma_1^2/\\sigma_2^2\\) using the sample ratio \\(F\\) and obtain a \\((1-\\alpha)\\times100\\%\\) confidence interval:\n\\[\n\\left(\\frac{s_1^2}{s_2^2}\\times\\frac{1}{F_{\\alpha/2,\\,n_1-1,\\,n_2-1}},\\ \\frac{s_1^2}{s_2^2}\\times\\frac{1}{F_{1-\\alpha/2,\\,n_1-1,\\,n_2-1}}\\right).\n\\]\n\n\nExample – Enzyme activity variability (biology)\nTwo species of bacteria are tested for variability in an enzyme’s activity. For species A (\\(n_1=10\\)) the sample variance is \\(s_1^2=0.08\\); for species B (\\(n_2=12\\)) \\(s_2^2=0.03\\). With \\(s_1^2 &gt; s_2^2\\), the test statistic is \\(F=0.08/0.03=2.67\\). Degrees of freedom are \\((9,11)\\). Using software, we find \\(P(F_{9,11}&gt;2.67)\\approx0.08\\). The two‑sided p‑value is \\(2\\times0.08=0.16\\). We fail to reject \\(H_0\\); the variances are not significantly different. A 95% CI for \\(\\sigma_1^2/\\sigma_2^2\\) would be computed using the formula above.\n\n\nPerforming the F‑test in JMP 18\n\nPrepare data. Create a column for the quantitative response and a column for the group indicator (e.g., “Species”).\nFit Y by X. Choose Analyze → Fit Y by X, assign the response to Y and the group to X, and click OK.\nUnequal Variances. Click the red triangle next to “Fit Y by X” and select Unequal Variances. JMP displays tests such as Levene’s and Bartlett’s along with the F‑ratio.\n\nIf the normality assumption is doubtful, use a robust alternative such as Levene’s test (also available in the Unequal Variances output).\n\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition/Formula\n\n\n\n\nF‑test\nTests \\(H_0: \\sigma_1^2=\\sigma_2^2\\) with statistic \\(F=s_1^2/s_2^2\\).\n\n\nConfidence interval for variance ratio\n\\(\\big(\\frac{s_1^2}{s_2^2}/F_{\\alpha/2},\\ \\frac{s_1^2}{s_2^2}/F_{1-\\alpha/2}\\big)\\) for df \\((n_1-1,n_2-1)\\).\n\n\n\n\n\n\nCheck your understanding\n\nTwo laboratories measure the variability of protein concentrations in a sample. Lab 1 obtains \\(s_1^2=0.20\\) (n = 8) and Lab 2 obtains \\(s_2^2=0.11\\) (n = 10). Test at \\(\\alpha=0.05\\) whether the variances differ.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nWith degrees of freedom \\((7,9)\\) the F statistic is \\(F=0.20/0.11=1.82\\). The two‑sided p‑value is \\(2\\min\\{P(F_{7,9}&gt;1.82),\\ P(F_{7,9}&lt;1/1.82)\\}=2\\times0.20=0.40\\). We fail to reject \\(H_0\\); there is no evidence of unequal variances.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Methods for Quantitative Response Variables – One and Two Groups</span>"
    ]
  },
  {
    "objectID": "12.html#paired-samples-matchedpairs-ttest",
    "href": "12.html#paired-samples-matchedpairs-ttest",
    "title": "12  Methods for Quantitative Response Variables – One and Two Groups",
    "section": "12.4 Paired Samples: Matched‑Pairs t‑Test",
    "text": "12.4 Paired Samples: Matched‑Pairs t‑Test\nSuppose we want to compare two methods of teaching reading to “slow learners”. Suppose it is possible to measure the “reading IQs” of the “slow learners” before they are subjected to a teaching method.\nEight pairs of “slow learners” with similar reading IQs are found, and one member of each pair is randomly assigned to the standard teaching method while the other is assigned to the new method.\nThe data are below and can be found in PAIREDSCORES.jmp.\n\n\n\nPair\nNew Method (2)\nStandard Method (1)\n\n\n\n\n1\n77\n72\n\n\n2\n74\n68\n\n\n3\n82\n76\n\n\n4\n73\n68\n\n\n5\n87\n84\n\n\n6\n69\n68\n\n\n7\n66\n61\n\n\n8\n80\n76\n\n\n\nDo the data support the hypothesis that the population mean reading test score for “slow learners” taught by the new method is greater than the mean reading test score for those taught by the standard method?\nUsing what you know so far, the hypotheses are \\[\n\\begin{align*}\n    {H_0:\\mu_1 = \\mu_2}\\\\\n    {H_a:\\mu_1&lt;\\mu_2}\n\\end{align*}\n\\]\nSuppose we use the nonpooled t statistic for two independent samples to conduct this test and suppose both random samples come from approximately normal populations.\nJMP output:\n\nThe p-value is \\[\n\\begin{align*}\n   {P(T&lt;-1.256)=0.1149}\n\\end{align*}\n\\]\nAt \\(\\alpha=.1\\), we would fail to reject \\(H_0\\) and conclude that there is insufficient evidence to infer a difference in the mean test scores for the two methods.\nLook at the data again in the table. You will see that the test score of the new method is larger than the corresponding test score for the standard method for every one of the eight pairs of “slow learners .”\nThis, in itself, seems to provide strong evidence to indicate that \\(\\mu_1&lt;\\mu_2\\).\nWhy, then, did the t -test fail to detect the difference?\nThe answer is, the independent samples t-test is not a valid procedure to use with this set of data.\nWe have randomly chosen pairs of test scores; thus, once we have chosen the sample for the new method, we have not independently chosen the sample for the standard method.\nThe dependence between observations within pairs can be seen by examining the pairs of test scores, which tend to rise and fall together as we go from pair to pair.\nWe now consider a valid method of analyzing the data. We can add a column for the differences between the test scores of the pairs of “slow learners.”\n\n\n\n\n\n\n\n\n\nPair\nNew Method (2)\nStandard Method (1)\nDifference (new - standard)\n\n\n\n\n1\n77\n72\n5\n\n\n2\n74\n68\n6\n\n\n3\n82\n76\n6\n\n\n4\n73\n68\n5\n\n\n5\n87\n84\n3\n\n\n6\n69\n68\n1\n\n\n7\n66\n61\n5\n\n\n8\n80\n76\n4\n\n\n\nWhen each subject or experimental unit provides two related measurements, such as before and after treatment or left‑ and right‑handed performance, we cannot treat the two samples as independent. Instead, we analyze the differences within pairs. The matched‑pairs t‑test focuses on the mean difference.\n\n\nSetting up the problem\nSuppose we observe pairs \\((x_{1i}, x_{2i})\\) for \\(i=1,\\dots,n\\). Let \\(d_i = x_{1i} - x_{2i}\\) denote the difference for each pair and let \\(\\bar{x}_x\\) and \\(s_d\\) be the mean and standard deviation of the differences. To test whether the average difference \\(\\mu_d\\) equals zero, we formulate\n\\[\n\\begin{align*}\n&H_0: \\mu_d = 0\\\\\n&H_a: \\mu_d \\neq 0\\quad (\\text{or } &gt; 0,&lt;0)\n\\end{align*}\n\\]\n\n\nMatched‑pairs t‑test\nThe test statistic is\n\\[\nt = \\frac{\\bar{x}_d}{s_d/\\sqrt{n}}\n\\]\nwhich follows a \\(t\\) distribution with \\(n-1\\) degrees of freedom under \\(H_0\\). We compute the p‑value as before. A \\((1-\\alpha)\\times100\\%\\) confidence interval for \\(\\mu_d\\) is \\(\\bar{d} \\pm t_{\\alpha/2,n-1}\\, s_d/\\sqrt{n}\\).\nFor our example, we have the following JMP output:\n\nThe p-value for this paired}t-test is \\[\n\\begin{align*}\n    {P(T&lt;-7.344)&lt;0.0001}\n\\end{align*}\n\\]\n\nAssumptions\n\nObservations are paired and differences \\(d_i\\) are independent of one another.\nThe distribution of differences is approximately normal. If the number of pairs is small (\\(n &lt; 30\\)) and the differences are skewed, the t‑test may not be reliable.\n\n\n\n\nPaired t-test in JMP\nJMP users can perform the matched‑pairs t‑test by creating a column for the differences and following the one‑sample mean procedure. Alternatively, JMP provides a dedicated Matched Pairs command in the Fit Y by X platform: choose Analyze → Fit Y by X, assign the after measurement as Y, the before measurement as X, then click the red triangle and select Matched Pairs. JMP computes the difference column, reports the t‑statistic, and provides a confidence interval.\n\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition/Formula\n\n\n\n\nPaired t‑test\nTests \\(H_0: \\mu_d=0\\) using \\(t=\\bar{d}/(s_d/\\sqrt{n})\\) with \\(n-1\\) df.\n\n\n\n\n\n\nCheck your understanding\n\nA pharmaceutical company tests a new drug designed to lower systolic blood pressure. Ten volunteers have their blood pressure measured before and after taking the drug for a month. The differences (before – after) are: 8, 5, 10, 6, 12, 3, 9, 7, 11, 4 mmHg. Use the paired t‑test to determine whether the drug reduces blood pressure on average. Provide a 95% CI for the mean reduction.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nCompute \\(\\bar{d}=7.5\\), \\(s_d=2.87\\) and \\(n=10\\). The t‑statistic is \\(t=7.5/(2.87/\\sqrt{10})\\approx8.25\\) with 9 df. The two‑sided p‑value is \\(&lt;0.0001\\); we reject \\(H_0\\) and conclude the drug lowers blood pressure. A 95% CI is \\(7.5 \\pm t_{0.025,9}\\times2.87/\\sqrt{10}=(6.0,9.0)\\) mmHg.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Methods for Quantitative Response Variables – One and Two Groups</span>"
    ]
  },
  {
    "objectID": "13.html",
    "href": "13.html",
    "title": "13  Analysis of Variance",
    "section": "",
    "text": "13.1 Why compare more than two means?\nImagine a physician comparing the average reduction in systolic blood pressure from three different antihypertensive medications. We could do a t-test for all possible pairs of the groups. So we would have the following sets of hypotheses: \\[\n\\begin{align*}\n   &{H_0: \\mu_1=\\mu_2}\\qquad\\qquad &{H_0: \\mu_1=\\mu_3}\\qquad\\qquad &{H_0: \\mu_2=\\mu_3}\\\\\n   &{H_a: \\mu_1\\ne \\mu_2}\\qquad\\qquad&{H_a: \\mu_1\\ne\\mu_3}\\qquad\\qquad&{H_a: \\mu_2\\ne \\mu_3}\n\\end{align*}\n\\]\nIn general, for \\(g\\) groups, there would be \\[\n\\begin{align*}\n  { {}_g C_2 = {g \\choose 2} = \\frac{g!}{2!(g-2)!}}\n\\end{align*}\n\\]\nSuppose you have 5 groups. We would have to do \\[\n\\begin{align*}\n  { {}_5 C_2} &{= {5 \\choose 2} = \\frac{5!}{2!(5-2)!}}\\\\\\\\\n   &{=\\frac{120}{2(6)}}\\\\\\\\\n   &{=10}\n\\end{align*}\n\\]\ntests to see if there were any differences between the means of the five groups.\nWhat is the danger in doing this many hypothesis tests?\nRecall the significance level, \\(\\alpha\\), is chosen by the researcher before doing the test. Usually, \\(\\alpha=0.05\\).\nThe significance level is also the probability of making a Type I Error (Rejecting \\(H_0\\) when \\(H_0\\) is actually true). This would be saying the means are different, when they actually are not different.\nThe complement of Type I error is failing to reject \\(H_0\\) when it is actually true. This would be saying the means are not different, when they actually are not different.\nThe probability of concluding the means are not different when they actually are not different is \\[\n    \\begin{align*}\n  { 1-\\alpha = 0.95}\n\\end{align*}\n\\]\nIf we were to conduct two hypothesis test for comparing the means, then the probability of saying that none of the means are different, when the actually are not will be \\[\n        \\begin{align*}\n  { 0.95\\times 0.95=0.9025}\n\\end{align*}\n\\] The complement of this would be the probability of making a Type I error in either test: \\[\n            \\begin{align*}\n  { 1-0.9025=0.0975}\n\\end{align*}\n\\] So even though the researcher picks \\(\\alpha=0.05\\) for each test, the probability of making a Type I error in either of the tests would be 0.0975.\nWhat if we do 10 tests (number of comparisons when there are five groups)? \\[\n            \\begin{align*}\n  { 1-(1-0.05)^{10}=0.4013}\n\\end{align*}    \n\\] So the probability of making a Type I error in any of the tests is 0.4013. As the number of comparisons increases, the probability of at least one erroneous conclusion skyrockets. Analysis of variance (ANOVA) was designed to answer a single overarching question—“Are there any differences among the group means?”—while controlling the overall Type I error rate.\nA one‑way ANOVA compares the means of three or more independent groups. The groups correspond to different levels of a single explanatory factor (for example, different diets, treatments or business strategies).\nUnder the null hypothesis all group means are equal, while the alternative is that at least one mean differs.\n\\[\n\\begin{align*}\n&H_0: {\\mu_1=\\mu_2=\\cdots=\\mu_m}\\\\\n&H_a: \\text{At least one mean differs}\n\\end{align*}\n\\]\nThe test statistic is an F ratio—the ratio of variation between groups to variation within groups. If \\(H_0\\) is false, perhaps all the population means differ, but perhaps only one mean differs from the others.\nThe test analyzes whether the differences observed among the sample means could have reasonably occurred by chance, if the null hypothesis of equal population means was true.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "13.html#sec-13_01",
    "href": "13.html#sec-13_01",
    "title": "13  Analysis of Variance",
    "section": "",
    "text": "“The analysis of variance is the Hubble telescope of the experimenter; it lets you see structures you would miss if you only looked one piece at a time.” – George Box\n\n\nGuiding question: When and why do we use ANOVA instead of doing several two‑sample t‑tests?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBetween‑ and within‑group variation\nConsider \\(m\\) groups with \\(n_i\\) observations in group \\(i\\). Let \\(\\bar{y}_{i\\cdot}\\) be the sample mean of group \\(i\\) and \\(\\bar{y}_{\\cdot\\cdot}\\) be the grand mean. ANOVA partitions the total sum of squares,\n\\[\n\\mathrm{SS}_{\\mathrm{Total}} = \\sum_{i=1}^m\\sum_{j=1}^{n_i} (y_{ij} - \\bar{y}_{\\cdot\\cdot})^2\n\\]\ninto between‑group variability\n\\[\n\\mathrm{SS}_{\\mathrm{Between}} = \\sum_{i=1}^m n_i\\,(\\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot\\cdot})^2\n\\]\nand within‑group (error) variability\n\\[\n\\mathrm{SS}_{\\mathrm{Error}} = \\sum_{i=1}^m\\sum_{j=1}^{n_i} (y_{ij} - \\bar{y}_{i\\cdot})^2\n\\]\nThe total variability satisfies \\[\n\\mathrm{SS}_{\\mathrm{Total}}=\\mathrm{SS}_{\\mathrm{Between}}+\\mathrm{SS}_{\\mathrm{Error}}\n\\]\nDividing each sum of squares by its degrees of freedom gives the mean squares:\n\\[\n\\mathrm{MS}_{\\mathrm{Between}} = \\frac{\\mathrm{SS}_{\\mathrm{Between}}}{m-1},\\quad \\mathrm{MS}_{\\mathrm{Error}} = \\frac{\\mathrm{SS}_{\\mathrm{Error}}}{N - m}\n\\]\nwhere \\(N=\\sum_{i=1}^m n_i\\) is the total sample size. The F statistic is the ratio of these mean squares:\n\\[\nF = \\frac{\\mathrm{MS}_{\\mathrm{Between}}}{\\mathrm{MS}_{\\mathrm{Error}}}\n\\]\nUnder \\(H_0\\) the F statistic follows an \\(F\\) distribution with \\((m-1,\\,N-m)\\) degrees of freedom. A large F value indicates that the variability between group means is large relative to the random variability within groups, suggesting that at least one mean differs from the others.\nThe results of an ANOVA F-test are usually presented in a table. The ANOVA table usually takes the form\n\n\n\n\n\n\n\n\n\n\n\nSource\nSum of Squares\ndf\nMean Square\nF\np-value\n\n\n\n\nBetween\n\\(\\text{SS}_\\text{Between}\\)\n\\(m-1\\)\n\\(\\text{MS}_\\text{Between}\\)\n\\(\\frac{\\text{MS}_\\text{Between}}{\\text{MS}_\\text{Error}}\\)\n\\(P(F&gt;\\frac{\\text{MS}_\\text{Between}}{\\text{MS}_\\text{Error}})\\)\n\n\nError\n\\(\\text{SS}_\\text{Error}\\)\n\\(N-m\\)\n\\(\\text{MS}_\\text{Error}\\)\n\n\n\n\nTotal\n\\(\\text{SS}_\\text{Total}\\)\n\\(N-1\\)\n\n\n\n\n\n\n\n\nAssumptions\nANOVA relies on a few conditions. Each observation should be independent of the others, the response variable should be continuous and approximately normally distributed within each group, and the population variances in all groups should be equal. In practice the F test is fairly robust to mild departures from normality, especially when the sample sizes are similar. If sample variances are markedly different, alternatives such as Welch’s ANOVA may be more appropriate.\n\n\nExample: Cholesterol‑lowering treatments\nSuppose four cholesterol‑lowering drugs—A, B, C and D—are given to randomly selected patients. After 6 weeks the reduction in LDL cholesterol (mg/dL) is measured. We want to know if the average reduction differs among the four drugs. Rather than perform six t‑tests, we compute the ANOVA F statistic. An F ratio much larger than 1 would suggest that at least one mean reduction differs from the others.\nWe can visualize the data using boxplots.\n\n\n\n\n\n\n\n\n\nThe boxplot hints at possible differences—drug D seems to reduce cholesterol more than the others. The F test will confirm whether such differences are statistically significant. B\nBelow is the JMP output of the ANOVA table for this data.\n\nUsing \\(\\alpha=0.05\\), we reject the null hypothesis. At the 5% significance level, there is enough evidence to conclude that at least one of the drugs differs. We will discuss how to determine which drugs differ with confidence intervals in the next section.\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition\n\n\n\n\nOne‑way ANOVA\nA procedure for testing whether the means of three or more independent groups are equal.\n\n\nBetween‑group variability\nVariability due to differences among group means.\n\n\nWithin‑group (error) variability\nVariability of observations around their group means.\n\n\nF statistic\nThe ratio \\(\\mathrm{MS}_{\\mathrm{Between}}/\\mathrm{MS}_{\\mathrm{Error}}\\) used to test \\(H_0\\) that all group means are equal.\n\n\nAssumptions\nIndependence, normality of each group and equality of variances across groups.\n\n\n\n\n\n\nCheck your understanding\n\nWhy is it not advisable to use multiple t‑tests to compare the means of four or more groups?\nIn your own words, explain what it means if the ANOVA F statistic is close to 1.\nList the key assumptions of a one‑way ANOVA.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nPerforming several independent t‑tests inflates the probability of making at least one Type I error (false positive). ANOVA tests all the means simultaneously and controls the overall error rate.\nAn F ratio near 1 means that the variability between group means is similar to the variability within groups. Under such circumstances the data are consistent with all groups having the same mean, and we would likely fail to reject \\(H_0\\).\nThe response variable should be continuous and approximately normally distributed in each group, observations should be independent, and the population variances should be equal.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "13.html#sec-13_02",
    "href": "13.html#sec-13_02",
    "title": "13  Analysis of Variance",
    "section": "13.2 Post‑hoc comparisons",
    "text": "13.2 Post‑hoc comparisons\n\n“Statistics is a science in my opinion, and it is no more a branch of mathematics than are physics, chemistry, and economics; for if its methods fail the test of experience–not the test of logic–they will be discarded.” - John Tukey\n\n\nGuiding question: After finding a significant ANOVA result, how do we determine which group means differ?\n\nThe global F test in a one‑way ANOVA tells us that not all group means are equal, but it does not identify which pairs of means differ. Performing multiple two‑sample t‑tests on the same data inflates the family‑wise error rate. Post‑hoc procedures are designed to compare all pairs of group means while controlling the overall probability of making any false positives. The basic idea is to construct confidence intervals for each pair of means and adjust the critical values so that the chance of incorrectly claiming a difference remains at or below the chosen \\(\\alpha\\) level.\n\nCommon post‑hoc methods\nSeveral procedures exist for comparing means after an ANOVA. Here is a brief overview:\n\nFisher’s least significant difference (LSD) – performs unadjusted two‑sample t‑tests for each pair of means but only after the omnibus F test is significant. Simple to compute but does not control the family‑wise error rate when there are many comparisons.\nBonferroni – divides the desired significance level by the number of comparisons (or multiplies p‑values by that number). Valid for unequal sample sizes, but conservative when many comparisons are made.\nScheffé’s method – constructs simultaneous confidence intervals for all possible contrasts among means. Highly conservative but useful when testing complex hypotheses involving multiple groups.\n\nOther specialized procedures include Holm’s sequential Bonferroni and Dunnett’s test (for comparing several treatments against a control), which we will not cover in detail here.\n\n\nTukey’s honestly significant difference (HSD)\nBecause it balances power and control of the family‑wise error rate, tukey’s hsd is the most widely used post‑hoc procedure in one‑way ANOVA. It relies on the studentized range distribution, which models the range of sample means standardized by the within‑group variability. After computing the ANOVA F test, the critical difference for comparing means of groups \\(i\\) and \\(j\\) is\n\\[\n\\mathrm{HSD} = \\mathrm{q}_{1-\\alpha, m, N - m}\\,\\sqrt{\\frac{\\mathrm{MS}_{\\mathrm{Error}}}{2}\\left(\\frac{1}{n_i} + \\frac{1}{n_j}\\right)}\n\\]\nwhere \\(\\mathrm{MS}_{\\mathrm{Error}}\\) is the within‑group mean square from the ANOVA table and \\(\\mathrm{q}_{1-\\alpha, m, N - m}\\) is the critical value from the studentized range distribution. For equal group sizes \\(n_i = n_j = n\\), this simplifies to \\[\n\\mathrm{HSD} = \\mathrm{q}\\,\\sqrt{\\mathrm{MS}_{\\mathrm{Error}}/n}\n\\]\nA pair of sample means \\(\\bar{y}_{i\\cdot}\\) and \\(\\bar{y}_{j\\cdot}\\) is declared significantly different if \\(|\\bar{y}_{i\\cdot} - \\bar{y}_{j\\cdot}| &gt; \\mathrm{HSD}\\). Equivalently, the \\(100(1-\\alpha)\\%\\) confidence interval for the difference \\(\\bar{y}_{i\\cdot} - \\bar{y}_{j\\cdot}\\) is\n\\[\n\\bar{y}_{i\\cdot} - \\bar{y}_{j\\cdot} \\pm \\mathrm{HSD}\n\\]\nBecause Tukey’s method is based on the largest range among means, it controls the family‑wise error rate exactly when all group sizes are equal and remains robust for slight imbalances.\n\n\n\nPerforming Tukey’s HSD in JMP 18 Student Edition\n\nUse Analyze &gt; Fit Y by X to fit the one‑way ANOVA. Assign the continuous response variable to Y and the categorical factor to X.\nClick the red triangle next to the Oneway analysis report and choose Means/Anova to display the ANOVA table.\nFrom the red triangle menu in the Oneway window, select Compare Means &gt; All Pairwise, Tukey HSD. A dialog box will prompt you to specify the significance level (default is 0.05); you can adjust this if needed.\nJMP adds a table labeled Tukey HSD listing each pair of groups, the difference in means, standard error, confidence interval and adjusted p‑value. Pairs with intervals that do not contain zero are flagged as significantly different.\nYou can visualize the results by selecting Plot in the same menu, which adds confidence interval plots for each pairwise comparison. Always ensure that the global ANOVA F test is significant before interpreting post‑hoc comparisons.\n\n\n\n\nExample: Plant growth under different fertilizers\nSuppose an agronomist measures the dry weight (grams) of plants grown under three fertilizers—Nitro, SuperGrow and GreenUp—with twenty plants in each group. Below are boxplots of the data.\n\n\n\n\n\n\n\n\n\nThe one‑way ANOVA yields a significant F statistic, indicating that not all mean weights are equal.\n\nAt the 5% significance level, there is enough evidence to conclude that at least one of the fertilizers has a mean different than the others.\nBelow is the JMP output for Tukey’s HSD:\n For the difference between SuperGrow and GreenUp, we see the confidence interval does not include zero. Thus, there is a significant difference between these two fertilizers. In addition, the interval indicates that SuperGrow is between 3.09 and 11.94 grams more than GreenUp, on average.\nLikewise, we see that SuperGrow is between 1.24 and 10.09 grams more thant Nitro, on Average.\nFor the difference between Nitro and GreenUp, we see the confidence intervals does include zero. Therefore, there is not enough evidence that these two fertilizers are different.\nIn all three of these results, we are 95% confident that all three of these intervals contain the true differences.\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition\n\n\n\n\nPost‑hoc test \nA procedure for comparing pairs of group means after an ANOVA indicates that not all means are equal, while controlling the family‑wise error rate.\n\n\nTukey’s HSD \nUses the studentized range distribution to construct simultaneous confidence intervals for all pairwise differences; exact for equal group sizes and less conservative than Bonferroni.\n\n\n\n\n\n\nCheck your understanding\n\nIn your own words, describe how Tukey’s HSD controls the family‑wise error rate.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nTukey’s HSD uses the studentized range distribution to adjust the critical value. It accounts for the probability of the largest difference among group means exceeding a threshold, ensuring that the chance of falsely declaring any difference among the means is at most \\(\\alpha\\).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "13.html#sec-13_03",
    "href": "13.html#sec-13_03",
    "title": "13  Analysis of Variance",
    "section": "13.3 Two‑way ANOVA and randomized blocks",
    "text": "13.3 Two‑way ANOVA and randomized blocks\n\n“Statistics is the art of stating in precise terms that which one does not know.” - William Kruskal\n\n\nGuiding question: How do we assess the effects of two explanatory variables on a single quantitative response?\n\nWhen experiments involve two categorical factors—say, treatment and gender, or fertilizer type and irrigation level—we can use a two‑way ANOVA. This model allows us to test for main effects of each factor and to examine whether the effect of one factor depends on the level of the other (the interaction). In certain designs where a second factor is used only to reduce variability and is not of substantive interest, we call it a randomized block design.\n\nModel and hypotheses\nSuppose we have two factors, \\(A\\) with \\(a\\) levels and \\(B\\) with \\(b\\) levels. Each treatment combination has \\(n_{ij}\\) observations. Let \\(\\mu_{ij}\\) be the mean for combination \\((i,j)\\). We write the model as\n\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk},\\]\nwhere \\(\\mu\\) is the grand mean, \\(\\alpha_i\\) is the effect of level \\(i\\) of factor \\(A\\), \\(\\beta_j\\) is the effect of level \\(j\\) of factor \\(B\\), \\((\\alpha\\beta)_{ij}\\) is the interaction effect, and \\(\\varepsilon_{ijk}\\) are independent normal errors with common variance. The hypotheses for each effect are:\n\nMain effect of \\(A\\): \\[\nH_0: \\alpha_1 = \\alpha_2 = \\cdots = \\alpha_a = 0\\quad \\text{vs.}\\quad H_a: \\text{At least one } \\alpha_i \\neq 0\n\\] This tests whether the mean response differs across levels of factor \\(A\\). This is equivalent to testing \\[\nH_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_a\\quad \\text{vs.}\\quad H_a: \\text{At least one differs}\n\\] For all levels of factor \\(B\\).\nMain effect of \\(B\\): \\[\nH_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_b = 0\\quad \\text{vs.}\\quad H_a: \\text{At least one } \\beta_j \\neq 0\n\\] This is equivalent to testing \\[\nH_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_b\\quad \\text{vs.}\\quad H_a: \\text{At least one differs}\n\\] for all levels of factor \\(A\\).\nInteraction: \\[\nH_0: (\\alpha\\beta)_{ij} = 0\\ \\text{for all } i,j\\quad \\text{vs.}\\quad H_a: \\text{At least one } (\\alpha\\beta)_{ij} \\neq 0\n\\] We can write this hypothesis without the math symbols as \\[\n\\begin{align*}\n&H_0: \\text{There is no interaction between the two factors}\\\\\n&H_a: \\text{There is interaction between the two factors}\n\\end{align*}\n\\]\n\nFor each hypothesis, an F statistic is computed by dividing the mean square for that factor or interaction by the mean square error. Large F values suggest significant effects.\n\n\nPartitioning variation\nThe two‑way ANOVA partitions the total variability into contributions from factor \\(A\\), factor \\(B\\), their interaction, and random error. For balanced designs with equal sample sizes \\(n\\) per combination, the sums of squares are\n\\[\n\\mathrm{SS}_A = bn\\sum_{i=1}^a(\\bar{y}_{i\\cdot\\cdot} - \\bar{y}_{\\cdot\\cdot\\cdot})^2,\\quad \\mathrm{SS}_B = an\\sum_{j=1}^b(\\bar{y}_{\\cdot j\\cdot} - \\bar{y}_{\\cdot\\cdot\\cdot})^2\n\\]\n\\[\n\\mathrm{SS}_{AB} = n\\sum_{i=1}^a\\sum_{j=1}^b (\\bar{y}_{ij\\cdot} - \\bar{y}_{i\\cdot\\cdot} - \\bar{y}_{\\cdot j\\cdot} + \\bar{y}_{\\cdot\\cdot\\cdot})^2,\n\\]\nand the error sum of squares captures the variability of observations around their cell means. Dividing each sum of squares by its degrees of freedom yields mean squares for computing F statistics.\n\n\nRandomized block designs\nIn a randomized block design, one factor is a blocking factor used to group similar experimental units and reduce variability. For example, in a drug trial, patients might be blocked by age group; in agricultural experiments, plots might be blocked by soil type. The model is the same as the two‑way ANOVA model, but only the treatment factor is of interest; the block factor is included solely to account for variation among blocks.\nWe typically test the treatment effect and ignore the block effect. When the blocking factor is actually a repeated measurement on the same subject, we instead use a repeated measures ANOVA (see next section).\n\n\nExample: Antidepressants and Age\nA psychiatrist wants to study the effects of three antidepressants on subjects in three different age groups. There were 12 subjects in each age group who were then randomly assigned one of the three antidepressants. Each subject was rated on a scale of 0 to 100, with higher numbers indicating greater relief from depression. The data is presented in the following table (and can be found in the file antidepressantrating.jmp):\n\n\n\n\n\n\n\n\n\n\n18–30\n31–50\n51–80\n\n\n\n\nDrug A\n43, 43, 41, 42\n53, 51, 53, 52\n57, 55, 58, 56\n\n\nDrug B\n52, 51, 51, 50\n62, 61, 60, 61\n65, 66, 66, 67\n\n\nDrug C\n71, 72, 70, 71\n81, 83, 82, 82\n86, 84, 87, 86\n\n\n\nIn two-way ANOVA, a null hypothesis states that the population means are the same in each category of one factor, at each fixed level of the other factor.\nFor example, we could test \\[\n\\begin{align*}\nH_0: \\text{Mean rating is equal for all three drugs for each age group}\n\\end{align*}\n\\] We could also test \\[\n\\begin{align*}\nH_0: \\text{Mean rating is equal for all age groups for each drug }\n\\end{align*}\n\\]\nSuppose the population means for each of the cells were displayed in table (a) below. Since the means are the same for all three drugs in each age group, our first null hypothesis above would be true. If the means were as displayed in table (b), then the means would be the same for all three age groups in each drug group. So, our second null hypothesis above would be correct.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\nDrug\n18–30\n31–50\n51–80\n\n(b)\nDrug\n18–30\n31–50\n51–80\n\n\n\n\n\nDrug A\n60\n70\n80\n\n\nDrug A\n50\n50\n50\n\n\n\nDrug B\n60\n70\n80\n\n\nDrug B\n60\n60\n60\n\n\n\nDrug C\n60\n70\n80\n\n\nDrug C\n70\n70\n70\n\n\n\n\nExploring Interaction Between Factors in Two-Way ANOVA\nInvestigating whether interaction occurs is important whenever we analyze multivariate relationships.\nNo interaction between two factors means that the effect of either factor on the response variable is the same at each category of the other factor.\nThe ANOVA tests of main effects assume there is no interaction between the factors. What does this mean in this context?\nLet’s plot the means for the three drugs, within each age group.\nThe figure below shows a plot in which the y-axis gives estimated mean ratings, and points are shown for the nine drug / age group combinations.\n\nThe horizontal axis is not a numerical scale but merely lists the three drugs. The drawn lines connect the means for the three drugs, for a given age group. The absence of interaction is indicated by the (approximate) parallel lines.\nThe parallel lines occur because the difference in the estimated mean rating between the three drugs is the same for each age group.\nBy contrast, the figure below shows a set of means for which there is interaction. \nHere, the difference in means depends on the age group: According to these means, drug A is better for the 51-80 age group, drug B is better for the 31-50 age group, and drug C is better for the 18-30 age group.\nThe lines in the previous figure are not parallel.\nIt is not meaningful to test the main effects hypotheses when there is interaction.\nBelow is the JMP output for the two-way ANOVA for this example.\n\nAt the 5% significance level, there is not enough evidence to conclude that there is interaction between Drug and Age Group. Because there is no significant interaction, we can test the main effects.\nAt the 5% significance level there is enough evidence to conclude that at least one of the drugs differs in mean for each Age Group.\nAt the 5% significance level there is enough evidence to conclude that at least one of the Age Group differs in mean for each Drug.\nWe can also do Tukey’s HSD method to determine which of the groups are different for each main effect.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerforming two‑way ANOVA in JMP 18 Student Edition\nTo carry out a two‑way ANOVA or a randomized block design in JMP, choose Analyze &gt; Fit Model. Select your response variable for Y and add both factors to the Construct Model Effects box. If you have a complete factorial design, click Macros &gt; Full Factorial to include the main effects and interaction. Click Run to produce the ANOVA table. JMP will display an Effect Tests table with F statistics for each effect. Use the prediction profiler to visualize interactions, and check residual plots to assess model assumptions.\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition\n\n\n\n\nTwo‑way ANOVA\nA model with two categorical explanatory variables (factors) that tests for main effects and interactions.\n\n\nMain effect\nThe effect of a factor averaged over the levels of the other factor; tested by comparing \\(\\mathrm{MS}_{\\text{factor}}\\) to \\(\\mathrm{MS}_{\\mathrm{Error}}\\).\n\n\nInteraction\nOccurs when the effect of one factor depends on the level of the other; tested via an F ratio using \\(\\mathrm{MS}_{AB}\\).\n\n\nRandomized block design\nA special two‑way ANOVA where one factor (the block) is used to control variability; only the treatment factor is of interest and the block factor accounts for nuisance variation.\n\n\n\n\n\n\nCheck your understanding\n\nDescribe the difference between a main effect and an interaction effect in a two‑way ANOVA.\nIn a randomized block design with five treatments and four blocks, how many degrees of freedom are associated with the treatment, block and error sums of squares?\nWhy is it important to include the blocking factor in the model when analyzing a randomized block design?\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nA main effect measures the average difference in the response across the levels of a factor, ignoring (averaging over) the other factor. An interaction effect tests whether the effect of one factor varies depending on the level of the other factor; if significant, the factors do not act independently.\nThe treatment factor has \\(a-1\\) degrees of freedom and the block factor has \\(b-1\\), where \\(a\\) is the number of treatments and \\(b\\) the number of blocks. The error degrees of freedom are \\((a-1)(b-1)\\) for a balanced design with one observation per cell. In this case, \\(a=5\\) and \\(b=4\\), so the treatment, block and error degrees of freedom are \\(4\\), \\(3\\) and \\(12\\), respectively.\nThe block factor captures variation among blocks. Omitting it would inflate the error term, reducing power to detect treatment differences. Including the blocking factor removes block‑to‑block variability from the error and improves precision.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "13.html#sec-13_04",
    "href": "13.html#sec-13_04",
    "title": "13  Analysis of Variance",
    "section": "13.4 Repeated measures ANOVA",
    "text": "13.4 Repeated measures ANOVA\n\n“Beware of the problem of testing too many hypotheses; the more you torture the data, the more likely they are to confess, but confessions obtained under duress may not be admissible in the court of scientific opinion.” - Stephen Stigler\n\n\nGuiding question: How do we compare means when the same subjects are measured repeatedly over time or under several conditions?\n\nIn many experiments the same experimental units are observed under multiple conditions—for example, measuring patients’ blood pressure at baseline, 1 month and 3 months after starting medication. Because measurements on the same subject are correlated, we cannot treat them as independent. A repeated measures ANOVA compares the means of three or more related measurements on the same subjects, taking into account this correlation.\n\nModel and partitioning variance\nFor a one‑factor repeated measures design with \\(m\\) time points or conditions, we have \\(n\\) subjects each observed under all \\(m\\) conditions. Let \\(y_{ij}\\) denote the response of subject \\(j\\) at time \\(i\\). The model can be written as\n\\[\ny_{ij} = \\mu + \\tau_i + s_j + \\varepsilon_{ij}\n\\]\nwhere \\(\\mu\\) is the grand mean, \\(\\tau_i\\) is the effect of condition \\(i\\), \\(s_j\\) is the effect of subject \\(j\\) (a random subject effect), and \\(\\varepsilon_{ij}\\) are error terms.\nThe ANOVA partitions the total variation into between‑treatments (conditions), between‑subjects, and error (within‑subjects) components. The F statistic for testing \\(H_0: \\tau_1=\\cdots=\\tau_m=0\\) uses the mean square for treatments divided by the mean square error:\n\\[\nF = \\frac{\\mathrm{MS}_{\\text{Treatments}}}{\\mathrm{MS}_{\\text{Error}}}\n\\]\nThe error term is derived by subtracting the between‑subjects variation from the within‑subjects variation, leading to a smaller denominator and increased power compared with ordinary one‑way ANOVA.\n\n\nAssumptions and sphericity\nRepeated measures ANOVA requires that the differences between all pairs of conditions have equal variances—a property known as sphericity. If sphericity is violated (common when measurements are taken at closely spaced time points), the F statistics may be too liberal. Solutions include applying a Huynh–Feldt or Greenhouse–Geisser correction to the degrees of freedom or using a multivariate approach such as MANOVA. Another assumption is that the subjects are independent; repeated measures should not be confused with randomized block designs in which treatments are assigned to different units within a block.\n\n\nExample: Pain scores over time\nSuppose researchers collect pain scores on 12 patients after a surgical procedure at baseline, 2 hours, 6 hours, and 24 hours post‑surgery. The goal is to determine whether mean pain changes over time. The repeated measures ANOVA compares mean pain at the four time points.\nJMP output for the repeated measures ANOVA is shown below:\n\nAt the 5% significance level, there is not enough evidence to conclude the mean pain levels differs over time. We can also construct Tukey HSD confidence intervals as we have done before. Since we did not find the mean pain levels differ over any of the time points, we expect these intervals to all include zero.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConducting repeated measures ANOVA in JMP 18 Student Edition\nStack the repeated measurements into a single column with a factor column indicating the measurement occasion. Then use Analyze &gt; Fit Model with the response variable as Y and include the repeated factor, the subject identifier, and their interaction. Select Random Effects for the subject term to specify the correlation structure. This approach yields the usual ANOVA table and F tests with appropriate degrees of freedom.\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition\n\n\n\n\nRepeated measures ANOVA\nA method for comparing means of three or more measurements on the same subjects, accounting for correlation among repeated observations.\n\n\nSphericity\nThe assumption that the variances of differences between all pairs of conditions are equal.\n\n\nBetween‑subjects variation\nVariability due to differences among subjects; removed from the error term in repeated measures ANOVA to increase sensitivity.\n\n\n\n\n\n\nCheck your understanding\n\nWhy can’t we treat repeated measurements on the same subject as independent observations?\nWhat is the purpose of the sphericity assumption, and what adjustments can be made when it is violated?\nDescribe one advantage of the repeated measures design over a completely randomized design when the same subjects are measured multiple times.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nMeasurements taken on the same subject are correlated; they share subject‑specific variability that violates the independence assumption of one‑way ANOVA. Repeated measures ANOVA models this correlation by separating between‑subjects and within‑subjects variation.\nSphericity ensures equal variances of pairwise differences between conditions. When sphericity is violated, F statistics can be adjusted using the Greenhouse–Geisser or Huynh–Feldt correction, or one can use a multivariate approach such as MANOVA.\nBecause each subject serves as their own control, repeated measures designs reduce the impact of between‑subject variability. This typically increases statistical power and requires fewer subjects to detect a given effect.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  }
]