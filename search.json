[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 2381 Introductory Statistical Methods",
    "section": "",
    "text": "Introduction\nThese are the lecture notes for STA 2381 - Introductory Statistical Methods.\nPrerequisites: None\n\nCourse Description:\nParametric statistical methods. Topics range from descriptive statistics through regression and one-way analysis of variance. Applications are typically from biology and medicine. Computer data analysis is required.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01.html",
    "href": "01.html",
    "title": "1  Introduction to Statistics",
    "section": "",
    "text": "1.1 What is Statistics?\nSuppose someone shows you a coin. It is a typical coin like the one you see below.\nAlso suppose this individual claims that this coin will land on heads 90% of the time when it is flipped.\nThis claim seems unbelievable. A typical coin lands on heads bout 50% of time if the coin is fair.\nWhat would you say to this individual who is making this claim?\nYou would likely say Show me some evidence.\nSo this individual flips the coin and it lands on heads.\nThis result is not surprising if the coin is fair or if the claim of 90% is true. If the coin landed on tails then you would likely have some doubt of the claim. Since we have heads, we really don’t have enough information to confirm the 90% claim. So we ask for the coin to be flipped again.\nSuppose it flipped to a heads again. So now we have two heads in two flips of this coin.\nIs this enough evidence to convince you of the 90% claim?\nTo answer that question, you should consider the chances of getting this result under both scenarios:\nAlthough the 90% claim has the higher chance of getting two heads in two flips, we cannot rule out the fair coin scenario. A chance of 25% is not high, but it is not impossible. If fact, we would not consider a result with chances of 25% to be rare or unusual if that result happens.\nWe need more information. Suppose the coin if flipped 20 times. Here are the results:\nIn these 20 flips, we have 11 heads and 9 tails.\nWith this information, we do you think about the 90% claim? Let’s compare what we have seen in this result compared to what we expected to see under the two scenarios.\nThese results appear to show the 90% claim is not true. But let’s look at another set of 20 flips.\nIn this set of 20 flips, we have 17 heads and 3 tails including 14 heads in a row.\nWithout even considering the chances under the two scenarios, you are likely more convinced of the 90% claim given these results. Let’s look at the chances anyways:\nThe evidence is clearly in favor of the 90% claim.\nHere’s the catch: that second set of 20 flips was not made up. They are actually the results of the coin toss of Super Bowls 31-50. If we let heads represent the NFC team winning the coin toss and tails represent the AFC team winning the toss, then the second set of flips would be the results.\nDoes this seem impossible? Do we think the Super Bowl coin was somehow weighted to give the NFC team an advantage? Certainly not. In fact, each Super Bowl uses a different commemorative coin for the coin toss.\nWhat this illustrates is rare things happen. Even when a process is completely fair and governed only by chance, long streaks and surprising outcomes can still occur. That means that we cannot judge claims based on a few eye-catching outcomes or coincidences alone. Instead, we must step back and ask deeper questions:\nIn other words, real-world questions rarely come with certainty. Instead, they come with uncertainty, variability, chance, and incomplete information. We cannot rely on intuition alone. We need a disciplined way to use data to evaluate claims, measure evidence, and make informed decisions even when we cannot know the truth with absolute certainty.\nThat disciplined way of thinking is what statistics is about.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "01.html#what-is-statistics",
    "href": "01.html#what-is-statistics",
    "title": "1  Introduction to Statistics",
    "section": "",
    "text": "“The non-scientist in the street probably has a clearer notion of physics, chemistry and biology than of statistics, regarding statisticians as numerical philatelists, mere collector of numbers.” - Stephen Senn, Dicing with Death: Chance, Risk and Health\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nIf the coin if fair, the chances of getting two heads in two flips is 25%.\nIf the 90% claim is true, the chances of getting two heads in two flips is 81%1.\n\n\n\n\n\n\n\nIf the coin if fair, we expect to see around 10 heads. We saw 11 heads which is close.\nIf the 90% claim is true, we expect to see around 18 heads. We saw 11 heads which is far off. In fact, the chances of seeing at most 11 heads under this scenario is less than 0.01%.\n\n\n\n\n\n\nIf the coin if fair, the chances of getting at least 17 heads out of 20 flips is around 0.13%.\nIf the 90% claim is true, the chances of getting at least 17 heads out of 20 flips is around 87%.\n\n\n\n\n\nAFC\nNFC\nNFC\nNFC\n\nNFC\nNFC\nNFC\nNFC\n\nNFC\nNFC\nNFC\nNFC\n\nNFC\nNFC\nNFC\nAFC\n\nAFC\nNFC\nNFC\nNFC\n\n\n\n\n\nWhat outcomes would we expect to see if a claim were true?\nWhat outcomes would we expect to see if the claim were not true?\nAre the results we observed consistent with ordinary chance variation, or are they so unusual that they suggest something else is going on?\n\n\n\n\nWhat is “statistics,” exactly?\nStatistics is the science of collecting, organizing, analyzing, and interpreting data to make decisions or draw conclusions. It’s not just about numbers-it’s about what those numbers tell us.\nIf Statistics concerns data, then we should define “data.” First, note that “data” is a plural word. “Datum” is singular, although it is common to hear someone refer to “data” in the singular. A “datum” is a piece of information or fact. So “data” is a collection of facts or information we collect. That could mean\n\nthe amount of profit a company makes,\nthe growth of plants under some conditions,\nor how many people voted in an election.\n\nA helpful way to organize the subject of Statistics is to distinguish two complementary activities:\n\nDescriptive statistics help us summarize and visualize what we observed-think graphs, tables, and numerical summaries. The goal is clarity.\nInferential statistics help us generalize from a sample to a broader group (or process) and quantify our uncertainty about that generalization. The goal is justified conclusions.\n\nEven in a short conversation about data, you’ll hear a few recurring ideas:\n\nA population is the full set of people, items, or occasions we care about (all Baylor first-years this fall, all batteries produced this week).\nA sample is the subset we actually observe.\nA parameter is a (usually unknown) number that describes a population (the true average battery life, for example).\nA statistic is a number we compute from a sample (the sample’s average battery life) that we use to learn about the parameter.\n\nWe’ll study these terms in more detail soon; for now, hold on to the big idea: we summarize what we see (description) and we reason beyond what we see (inference).\n\n\nWhy decisions need both description and inference\nSuppose a clinic tests a new flu-prevention program among 200 volunteer patients. A month later, 18% of the “usual care” group got the flu, compared to 12% of the “new program” group.\nDescriptively, the new program looks better. Inferentially, we ask: could this gap be due to chance? If we ran the study again with different patients, might the difference shrink or flip? Statistics gives us a way to quantify that uncertainty and decide what to do next.\nA similar story plays out in business A/B tests, manufacturing quality checks, and sports analytics. The descriptive picture tells us what happened in the data; inference tells us how strongly that evidence supports a decision.\n\n\n\n\n\n\n\nHow we’ll work in this course\nOur general workflow:\n\nStart with a clear question and name the observational units (what a single case is) and variables (what we record about each case).\nDecide how the data were or will be collected (survey, experiment, database pull).\nUse descriptive statistics and graphics to get oriented.\nBuild an inferential argument when you need to generalize or compare.\nCommunicate a conclusion in context-what it means, what it doesn’t, and what to do next.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nStatistics\nThe discipline of learning from data to describe patterns and make decisions under uncertainty.\n\n\nData\nRecorded information about cases (people/items/occasions) used as evidence for questions of interest.\n\n\nDescriptive statistics\nMethods for summarizing and visualizing what was observed (tables, graphs, numerical summaries).\n\n\nInferential statistics\nMethods for generalizing from a sample to a population and quantifying uncertainty.\n\n\n\n\n\n\n\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nIn your own words, how is describing data different from inferring from data? Give a short example for each.\n\n\n\nA streaming service tests two home-page designs on 5,000 visitors each. Version B produces a 0.6 percentage-point higher click-through rate than Version A. What questions would you ask before recommending the company switch to Version B?\n\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nDescription vs. inference. Description summarizes what happened in the observed data (e.g., “The median wait time yesterday was 11 minutes.”). Inference uses the sample to say something about a broader group or process, with uncertainty (e.g., “We estimate the typical wait time for all days like yesterday is 11 minutes, with margin of error ±2 minutes.”).\n\n\n\nHow were visitors assigned to versions (randomly)? Were there differences in traffic sources or device types? Is the effect stable over time? What outcome do we ultimately care about (sign-ups, retention), and does the change affect it?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "01.html#populations-and-samples",
    "href": "01.html#populations-and-samples",
    "title": "1  Introduction to Statistics",
    "section": "1.2 Populations and Samples",
    "text": "1.2 Populations and Samples\n\n“To understand God’s thoughts, we must study statistics, for these are the measures of his purpose.” - Florence Nightingale\n\nWhen you start any statistical investigation, the first two things to name are the population and the sample. The population is the full group or process you want to understand; the sample is the smaller set of cases you actually observe. Most of the time we can’t measure everyone or everything, so we sample wisely and then use statistics to bridge from the sample back to the population.\n\nDefining the population\nAt the heart of every statistical study is a population-the full set of individuals, objects, or events about which we want to learn. Sometimes this population is very concrete and well defined, such as “all tires produced by Line A during the third shift this week.” Other times it is more abstract or conceptual, such as “all patients with this condition who would receive this treatment under similar circumstances.” In both cases, the population exists independently of the data you happen to collect.\nTwo important refinements help clarify what we really mean by “the population” in practice:\n\nThe target population is the group you ultimately care about and want to draw conclusions about. This is the population referenced in your research question or claim. For example, a university might care about all incoming first-year students, or a manufacturer might care about all tires it produces under normal operating conditions.\nThe accessible population is the portion of the target population that you can realistically reach or observe. This group is often represented by a sampling frame-a list, database, or mechanism that allows you to identify and select individuals for the study.\n\nIn an ideal world, the accessible population would perfectly match the target population. In reality, they often differ. Practical constraints-time, cost, geography, incomplete records, or nonresponse-mean that some members of the target population are harder (or impossible) to include.\nClarity about this distinction matters. Suppose your target population is “all Baylor first-year students this fall.” If your sampling frame consists of “students who attend welcome week,” you may systematically exclude commuters, students with work or family obligations, or those who arrive late to campus. If these excluded students differ in meaningful ways-academically, socially, or financially-then your sample may not represent the target population well. This mismatch introduces bias, meaning your results may consistently overestimate or underestimate the quantity you are trying to measure.\nBeing explicit about the target population, the accessible population, and the sampling frame forces you to confront these limitations up front. It also helps you interpret results honestly: conclusions drawn from the accessible population should not automatically be generalized beyond it without careful justification. In statistics, knowing who your data represent is just as important as knowing what the data say.\n\n\nWhat counts as a sample?\nA sample is the subset of the population from which data are actually collected. It consists of the specific individuals, objects, or observations you measure in your study. While the population defines what you care about, the sample defines what you observe. Everything that follows in a statistical analysis-summary statistics, graphs, models, and conclusions-is based on the sample.\nWe will study how to select samples carefully in Chapter 2. For now, the key idea is what a good sample accomplishes. A good sample represents the population well enough that quantities computed from the sample-such as means, proportions, or regression coefficients-provide useful information about the corresponding population quantities, called parameters. When this happens, the sample serves as a stand-in for the population.\nImportantly, a sample does not need to “look like” the population in every detail. Rather, it needs to capture the relevant sources of variability in the population without systematically favoring certain groups or outcomes over others. Poor samples can lead to misleading conclusions, not because the calculations are wrong, but because the data themselves fail to reflect the population of interest.\nA special case is a census, in which you attempt to measure every unit in the population. In principle, a census eliminates sampling variability, since there is no subset involved. In practice, however, censuses are rare. They are often expensive, time-consuming, and logistically complex. Even large-scale censuses can suffer from missing data, nonresponse, or measurement error, which means they are not immune to bias.\nFor these reasons, sampling is the workhorse of statistics. Carefully designed samples allow us to learn about large or even infinite populations using a manageable amount of data. The central challenge of statistical inference is to quantify how much uncertainty remains when we use a sample to speak about a population-and that challenge begins with understanding exactly what our sample represents.\n\n\nParameters and statistics\nA parameter is a numerical summary that describes a characteristic of the entire population. Examples include the true mean income of all households in a city, the proportion of all voters who support a particular candidate, or the standard deviation of tire lifetimes produced by a factory. Parameters are fixed values-they do not change-but they are usually unknown, because measuring an entire population is rarely feasible.\nA statistic, by contrast, is a numerical summary computed from a sample. Sample means, sample proportions, and sample standard deviations are all statistics. Unlike parameters, statistics depend on which individuals happen to be included in the sample, and therefore they can change from sample to sample. In practice, statistics are the quantities we can actually calculate, and we use them as estimates of the corresponding population parameters.\nThis distinction is central to statistical thinking. When you report a sample mean, you are not claiming it is the population mean. Instead, you are using it as your best guess, based on the available data. Different samples from the same population would produce different statistics, even if the sampling method were perfectly fair.\nThat inevitable variation from sample to sample is called sampling error. Sampling error does not mean that a mistake was made or that the data were collected incorrectly. Rather, it reflects the natural randomness involved in observing only a subset of the population. Even well-designed samples are subject to sampling error.\nA major goal of statistics is to measure and communicate this uncertainty. Later chapters will introduce tools such as margins of error, confidence intervals, and hypothesis tests, which allow us to quantify how close a statistic is likely to be to the true parameter. These tools do not eliminate uncertainty, but they help us reason carefully about what our data can-and cannot-tell us about the population.\n\n\n\n\n\n\n\n\n\nWhy sampling works (and when it doesn’t)\nSampling works when your sample is representative of the population and your measurement is trustworthy. It struggles when parts of the population are systematically excluded (coverage problems), when participation differs by outcome (nonresponse), or when the way you select units is related to the outcome (selection effects). We’ll study these threats-and how to reduce them-next chapter.\n\n\nA quick illustration: samples differ, the goal doesn’t\nThe illustration below shows a fundamental idea in statistics: different samples from the same population give different answers, but they are all trying to estimate the same underlying truth.\nIn this simulation, we imagine a population where the true mean is known to be 10 and the standard deviation is 2. From this population, we repeatedly take random samples and compute the sample mean each time. We do this for several different sample sizes, and then look at how the sample means behave.\nEach histogram shows the distribution of sample means from 1,000 different samples of the same size. The dashed vertical line marks the true population mean of 10.\n\n\n\n\n\n\n\n\n\nWhen the sample size is small (for example, (n = 20)), the sample means vary noticeably from one sample to another. Some are below 10, some are above, and a few are fairly far away. This variability is not a flaw in the method-it is the unavoidable consequence of working with limited data.\nAs the sample size increases (to (n = 50) and then (n = 200)), two things become apparent:\n\nThe sample means continue to center around the true mean of 10.\nThe spread of the sample means shrinks. Larger samples produce more stable, less “wiggly” estimates.\n\nThis shrinking spread reflects a reduction in sampling error. With more data, random fluctuations tend to cancel out, and the statistic becomes a more precise estimate of the parameter. Importantly, the target never changes: all of these samples-large or small-are aiming to learn about the same population mean.\nThis example captures the intuition behind statistical inference. We accept that individual samples differ, but we rely on probability and repeated-sampling logic to understand how much they differ and how close our estimates are likely to be to the truth. Later chapters will formalize this intuition using sampling distributions, standard errors, and confidence intervals, but the core idea is already visible here: uncertainty decreases with sample size, even when the goal stays the same.\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nPopulation\nThe full group or process you want to understand.\n\n\nTarget population\nThe group you truly care about answering a question for.\n\n\nAccessible population\nThe portion of the target population you can practically reach.\n\n\nSampling frame\nThe list or mechanism from which the sample is drawn.\n\n\nSample\nThe subset of units you actually observe and measure.\n\n\nCensus\nAn attempt to measure every unit in the population.\n\n\nParameter\nA numerical characteristic of a population (e.g., \\(\\mu, p\\)).\n\n\nStatistic\nA numerical summary from a sample (e.g., \\(\\bar{x}, \\hat{p}\\)) used to estimate a parameter.\n\n\nSampling error\nNatural variation in a statistic from sample to sample.\n\n\nBias\nSystematic deviation caused by design, coverage, or selection issues.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nA battery plant tests 80 batteries from today’s production line and finds an average life of 7.8 hours. Identify the population, sample, parameter, and statistic.\nYour target is “all Baylor first-year students this fall.” You gather data from an email list of students who signed up for an early-interest program. What is the sampling frame? Name a potential source of bias.\nA marketing team uses comments on their Instagram post to judge product satisfaction among all customers. Explain why this may not represent the population.\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nPopulation: all batteries produced today. Sample: the 80 tested batteries. Parameter: the true mean life of all batteries produced today. Statistic: the sample mean of 7.8 hours.\nSampling frame: students on the early-interest email list. Potential bias: students who didn’t sign up (e.g., commuters, late enrollees) may be under-represented, creating coverage/selection bias.\nInstagram commenters are a self-selected subset; satisfied or dissatisfied users may be more likely to comment, and customers not on Instagram are excluded-both threaten representativeness.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "01.html#variables-and-types-of-data",
    "href": "01.html#variables-and-types-of-data",
    "title": "1  Introduction to Statistics",
    "section": "1.3 Variables and Types of Data",
    "text": "1.3 Variables and Types of Data\n\n“Not everything that can be counted counts, and not everything that counts can be counted.” - Albert Einstein\n\nEvery data table is built from two essential ingredients: cases and variables. The cases form the rows of the table and represent the individual observational units-people, objects, events, or time points-on which data are collected. The variables form the columns and describe characteristics recorded for each case. Thinking clearly about both is crucial: a dataset only makes sense when you know what each row represents and what information each column contains.\n\nWhat is a variable?\nA variable is any characteristic that can take different values across cases. For example, resting heart rate may vary from person to person, major differs across students, state of residence varies geographically, and did the patient improve? can differ from one patient to the next. If a characteristic does not vary-if it takes the same value for every case-it does not function as a variable and typically provides no information for analysis.\nVariables always live in context. For any dataset, we should be able to answer two basic questions:\n\nWhat does one row represent? This is the observational unit (for example, one student, one hospital visit, or one manufactured item).\nWhat does each column represent, and how was it measured or recorded? For numerical variables, this includes the unit of measurement (beats per minute, dollars, hours, miles, etc.).\n\nContext matters because the same-looking numbers can mean very different things depending on how they were collected. A column of values like 72, 68, 75 might represent heart rates, exam scores, or daily temperatures, and each interpretation leads to different conclusions and appropriate analyses. Even non-numeric variables require clear definitions: knowing whether “major” refers to a declared major at enrollment or at graduation can change how the data should be interpreted.\nBeing explicit about variables and their meaning lays the groundwork for everything that follows. The choice of summary statistics, graphs, and statistical models depends not just on the values in a column, but on what kind of variable it is and what it represents.\n\n\nThe two big families\nMost variables encountered in practice fall into two broad families. Different textbooks use slightly different names-qualitative vs. quantitative, categorical vs. numerical-but the underlying ideas are the same. The key question is whether the values represent group membership or measured quantities.\n\nCategorical variables\nA categorical variable assigns each case to a group, category, or label. The values describe what kind of thing each case is, not how much of something it has. Because the categories are labels, performing arithmetic operations on them does not make sense-even if the categories are coded using numbers behind the scenes.\nFor example, coding blood types as 1, 2, 3, and 4 does not imply that type 4 is “twice” type 2 or that averaging blood types is meaningful. The numbers are merely labels.\nCategorical variables commonly appear in three forms:\n\nNominal variables consist of categories with no natural ordering. One category is not inherently “higher” or “lower” than another. Examples include blood type, home state, eye color, device brand, or political party. For nominal variables, we typically summarize data using counts, proportions, or percentages and visualize them with bar charts or pie charts.\nOrdinal variables have categories with a meaningful order, but the spacing between categories is uneven or unknown. Likert-scale responses such as “Strongly disagree,” “Disagree,” “Neutral,” “Agree,” and “Strongly agree” fall into this category, as do rankings like race finish places (1st, 2nd, 3rd) or course grades (A, B, C, D, F). While the order carries information, it is generally unsafe to treat the differences between categories as numerically equal.\nBinary variables are categorical variables with exactly two categories. Common examples include yes/no, success/failure, pass/fail, or disease/no disease. Binary variables are especially important in statistics because they often encode outcomes of interest and are closely connected to proportions and probabilities.\n\nRecognizing a variable as categorical-and identifying which subtype it belongs to-guides nearly every analytical choice, from the graphs you make to the summary measures you report and the statistical methods you apply.\n\n\n\n\n\n\nLikert Scale\n\n\n\n\n\nA Likert Scale is a common psychometric scale used in questionnaires to measure attitudes, opinions, or behaviors. It presents a statement and asks respondents to indicate their level of agreement or frequency on a symmetric, typically 5- or 7-point scale. The options range from one extreme (e.g., “Strongly Disagree”) to the opposite extreme (e.g., “Strongly Agree”), often including a neutral or middle point.\nExample:\n“How satisfied are you with our service?”\n\nStrongly Disagree\nDisagree\nNeutral\nAgree\nStrongly Agree\n\n\n\n\n\n\nQuantitative variables\nA quantitative variable records numerical values for which arithmetic operations—such as addition, subtraction, averaging, and differences—are meaningful. These variables represent how much, how many, or how long of something, and they arise from counting or measuring. Because the numbers carry inherent magnitude, quantitative variables support a wide range of numerical summaries and graphical displays.\nOne important subtype of quantitative variables is:\n\nDiscrete variables, which arise from counting. Discrete variables take on distinct, separated values—typically whole numbers—with gaps between possible values. Examples include the number of emergency room visits in a year, the number of defects on a manufactured item, the number of children in a household, or the number of emails received in a day. For these variables, values like 2.5 or 3.7 are not meaningful because you cannot have a fractional count of an event.\n\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nIn math, the possible values of discrete data form what is known as countable set. This means the values form a collection (set) of values that can be put into a one-to-one correspondence with the natural numbers. In other words, a set is countable if you can list its elements in a sequence, even if the list is infinitely long.\nThere are two type of countable sets:\n\nFinite Countable Set\n\nThe set has a limited number of elements.\nExample: \\(\\{2, 4, 6, 8\\}\\) - there are exactly 4 elements.\n\nCountably Infinite Set\n\nThe set has infinitely many elements, but you can still list them in an ordered sequence.\nExample: The set of natural numbers \\(\\{1, 2, 3, 4, ...\\}\\)\nEven the set of all integers \\(\\{..., -2, -1, 0, 1, 2, ...\\}\\) is countable - you can reorder them as \\(0, 1, -1, 2, -2, 3, -3, ..\\) and still list them one by one.\n\n\n\n\n\n\nContinuous variables arise from measurement rather than counting. In principle, they can take on any value within an interval on the number line, limited only by the precision of the measuring instrument. Examples include blood pressure, time to failure, height, weight, temperature, and distance. For these variables, values such as 172.4, 172.43, or 172.431 are all meaningful, even if we choose to record them with fewer decimal places.\n\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nThe possible values of continuous data form an uncountable set. A set is uncountable if there’s no way to list all its elements in a sequence, even infinitely.\n\nExample: Real numbers between 0 and 1 - there are infinitely more of these than there are natural numbers.\nIn set notation, an example of an uncountable set is \\(\\{x: x\\ge 3\\}\\).\n\n\n\n\nTo decide if a variable is discrete or continuous, first think of an interval of possible values of the variable2. Can you count how many values are in that interval? If so, then it is discrete, if not, then it is continuous. In practice, we can simplify this further: counts are discrete, measurements are usually continuous.\n\n\n\nMeasurement scales you’ll hear about\nIn addition to the broad distinction between categorical and quantitative variables, you may encounter another classification system based on measurement scales. These scales—nominal, ordinal, interval, and ratio—describe what kinds of comparisons and arithmetic operations are meaningful for a variable. While this taxonomy appears frequently in introductory texts, its real value lies in guiding interpretation, not in memorization. We have already defined nominal and ordinal scales above.\n\nInterval scales are numeric scales where differences are meaningful, but the zero point is arbitrary. Temperature measured in degrees Fahrenheit or Celsius is the classic example. A difference of 10°F represents the same change anywhere on the scale, but ratios do not carry meaning: 40°F is not “twice as hot” as 20°F because zero does not represent the absence of temperature.\nRatio scales are numeric scales with a true zero, representing the absence of the quantity being measured. On these scales, both differences and ratios are meaningful. Examples include temperature measured in Kelvin, as well as length, mass, time, age, and income. Statements like “10 minutes is twice as long as 5 minutes” or “this object weighs three times as much as that one” are valid precisely because the zero point has real meaning.\n\nYou do not need to memorize the full taxonomy to do good statistics. The practical takeaway is simpler: always think about what comparisons make sense for your variable. In particular, be cautious about interpreting ratios when working with interval scales, and ask yourself whether subtraction, averaging, or scaling by a factor has a clear real-world meaning. Keeping this perspective front and center helps prevent overinterpretation and ensures that numerical summaries align with the nature of the data.\n\n\n“Identifier” and date/time variables\nNot every column in a dataset is meant to be analyzed numerically. Some variables exist primarily to organize, link, or track observations, and treating them incorrectly can lead to serious analytical mistakes.\n\nIdentifier (ID) variables are used to uniquely label cases. Examples include student ID numbers, order or invoice numbers, patient IDs, license plate numbers, and ZIP codes. Although these values are often composed of digits, they are labels, not quantities. Arithmetic operations on ID variables are meaningless: averaging student ID numbers or computing differences between order numbers tells you nothing substantive about the data. Their purpose is to ensure that each row can be uniquely referenced, merged with other datasets, or traced back to its source—not to summarize or model.\n\nA common error is to accidentally treat ID variables as quantitative simply because they are stored as numbers. Good practice is to explicitly mark them as identifiers (or convert them to text) so they are not mistakenly included in summaries, plots, or statistical models.\n\nDate and time variables record when an observation occurred. These variables are more subtle because they can legitimately play different roles depending on the question being asked. On one hand, dates and times can be treated as categorical—for example, grouping observations by day of the week, month, or season. On the other hand, they can be treated as quantitative by measuring elapsed time, such as minutes since the start of an experiment, days since enrollment, or time to failure.\n\nBecause date/time variables are flexible, it is essential to be explicit about how they are being used. Are you comparing weekdays to weekends, or are you modeling a trend over time? The same column can support both approaches, but the analysis, visualization, and interpretation differ substantially.\nBeing deliberate about identifiers and date/time variables helps ensure that each column is used appropriately. It prevents nonsensical calculations, clarifies analytical intent, and keeps the focus on variables that genuinely carry information about the phenomenon under study.\n\n\nWorking in JMP\nJMP keeps two concepts straight for each column:\n\nData Type (what the values are): Numeric, Character, or Date/Time.\nModeling Type (how you plan to analyze them): Continuous, Nominal, or Ordinal.\n\nDouble-click a column header (or right-click → Column Info) to set these. Typical mappings:\n\nCharacter + Nominal → categorical labels (e.g., “TX”, “CA”).\nNumeric + Continuous → quantitative measures (e.g., weight, time).\nNumeric + Nominal → numeric codes that are actually categories (e.g., 0/1 flags, ZIP codes, jersey numbers).\nNumeric + Ordinal → ordered categories encoded as 1–5 (Likert items).\n\nIf a graph or analysis looks odd in JMP, check these settings first-they control which menu options and displays you’ll see in Graph Builder and Analyze.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nCategorical variable\nPlaces a case into a group/label; arithmetic on labels is not meaningful.\n\n\nNominal\nCategorical with no natural order.\n\n\nOrdinal\nCategorical with a natural order but uneven/unknown spacing.\n\n\n\nBinary | A categorical variable with exactly two categories. |\nQuantitative variable | Numeric values where arithmetic is meaningful. |\nDiscrete | Quantitative counts that change in whole steps. |\nContinuous | Quantitative measurements on a (nearly) continuous scale. |\nIdentifier (ID) | A label used to distinguish cases; not for numerical analysis. |\nDate/time | A timestamped variable that can be treated as categorical or quantitative depending on the question. |\nMeasurement scale | Describes how numbers relate to the thing measured (nominal, ordinal, interval, ratio). |\nInterval | Numeric scale with arbitrary zero; differences meaningful, ratios not. |\nRatio | Numeric scale with a true zero; differences and ratios meaningful. | |\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nClassify each variable as categorical nominal, categorical ordinal, binary, quantitative discrete, or quantitative continuous:\n\nNumber of missed classes this semester\nPain rating on a 0–10 scale\nWhether a chip passes final inspection\nZIP code\nBody temperature (°F)\n\nA researcher converts systolic blood pressure into “Low” (&lt;110), “Normal” (110–139), and “High” (≥140). Name one advantage and one drawback of this recoding.\nGive an example where a date/time variable should be treated as (a) categorical and (b) quantitative.\nFor each of the following, say whether ratios are meaningful and explain briefly:\n\nTemperatures in °C\nHours of weekly exercise\nIQ scores\n\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n(a)Quantitative discrete (counts). (b) Categorical ordinal (ordered scale with uneven spacing)-often treated as numeric for convenience, but it’s ordinal by design. (c) Binary (pass/fail). (d) Identifier (categorical nominal label; not a quantitative variable). (e) Quantitative continuous.\nAdvantage: simplifies communication and enables comparisons across broad groups. Drawback: throws away information; analyses lose power and can depend on arbitrary cutpoints.\n(a)Categorical: day of week when a call was received (Mon–Sun). (b) Quantitative: minutes since admission, time to recovery, or days from treatment to follow-up.\n(a)°C is an interval scale: ratios aren’t meaningful (20°C isn’t “twice as hot” as 10°C). (b) Hours of exercise is a ratio scale with a true zero: ratios are meaningful (4 hours is twice 2 hours). (c) IQ is typically treated as an interval scale: differences are interpretable; ratios are not.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "01.html#the-role-of-statistics-in-research",
    "href": "01.html#the-role-of-statistics-in-research",
    "title": "1  Introduction to Statistics",
    "section": "1.4 The Role of Statistics in Research",
    "text": "1.4 The Role of Statistics in Research\n\n“Scientific research is a process of guided learning. The object of statistical methods is to make that process as efficient as possible.” - George Box, William Hunter, and J. Stuart Hunter\n\nResearch is not a matter of “run a test and see what comes out.” It is a disciplined, iterative process that begins with a well-defined question, moves through thoughtful study design and careful data collection, and ends with conclusions that are supported by evidence and honest about uncertainty. At every stage, judgment matters.\nStatistics is the connective tissue in this loop. It helps you plan how to learn from data, provides tools to summarize and visualize what you observed, and supplies a principled way to assess what you can responsibly claim about the population. Without statistical thinking, it is easy to overinterpret results, confuse noise for signal, or answer a question you never intended to ask.\n\nFrom question to estimand to design\nEvery sound statistical project should clearly articulate three elements before any data are analyzed:\n\nThe research question is a plain-language statement of what you want to learn. It should be specific and tied to a population of interest. For example: “Does the new tutoring program improve exam scores for Calculus I students?”\nThe target parameter (often called the estimand) translates that question into a precise quantity in the population. In this example, the estimand might be the difference in mean exam scores between students who receive tutoring and those who do not. Naming the parameter forces clarity about what “improve” actually means.\nThe study design describes how data will be collected to estimate that parameter. This includes the sampling plan, how variables will be measured, and whether treatments or interventions will be assigned.\n\nThese three pieces must align. A mismatch—such as asking a causal question with a purely descriptive design—leads to weak or misleading conclusions.\nTwo broad families of study designs appear throughout statistics:\n\nAn observational study records what naturally occurs, without assigning treatments or interventions. These studies are common and often easier to carry out. They are well suited for describing patterns and associations, but they are vulnerable to confounding, where other variables are related to both the explanatory variable and the outcome.\nAn experiment actively assigns a treatment to units and compares outcomes across groups. When well designed—with randomization, control, and careful implementation—experiments allow for stronger causal conclusions because they help isolate the effect of the treatment from other factors.\n\nThe choice between these designs depends on the research question, ethical and practical constraints, and what kind of claim you ultimately want to make.\n\n\nThe statistical process: a workable blueprint\nWhile real research is rarely linear, the following workflow provides a practical and reusable blueprint:\n\nSpecify the problem clearly. Identify the population, the observational units, and the variables involved. Ambiguity here leads to confusion later.\nDesign the study. Choose an observational or experimental approach, plan how the sample will be obtained, and think carefully about potential sources of bias and confounding.\nCollect data with quality in mind. Aim for reliable measurement, consistent procedures, and thorough documentation. Poor data quality cannot be fixed by clever analysis.\nExplore first. Use exploratory data analysis (EDA)—graphs, tables, and summary statistics—to understand the data’s structure, detect errors or anomalies, and build intuition before formal modeling.\nModel and infer. Select statistical methods that match the question and the data type, whether that involves comparing groups, studying relationships, or making predictions.\nQuantify uncertainty. Report estimates along with measures of uncertainty, such as confidence intervals and (when appropriate) p-values. Always distinguish statistical significance from practical significance. These tools will be developed in Chapters 9 and 10.\nDecide and communicate. Interpret results in context. Explain what the findings mean, what they do not imply, and what actions—if any—are justified based on the evidence.\nMake it reproducible. Ensure that another person could follow your steps and reproduce your results from the same data. Reproducibility is a cornerstone of trustworthy science.\n\nIn practice, this process is rarely a straight line. Insights from exploratory analysis often send you back to refine the research question or adjust the design. That looping is not a failure—it is a sign of careful, thoughtful science guided by statistical reasoning.\n\n\nWorking in JMP\nJMP is built for this process:\n\nDocument the plan and steps. Use File → New → Journal to create a running research notebook. Paste screenshots, notes, and output as you go.\nSave scripts to reproduce output. In most reports, click the red triangle ► Save Script → To Data Table (or To Journal). This stores a runnable recipe with the data.\nExplore first. Start with Graph → Graph Builder and Analyze → Distribution to profile variables and check data quality.\nFit models with assumptions in view. Use Analyze → Fit Y by X for two-variable comparisons and Analyze → Fit Model for multiple predictors. Residual and diagnostic tools are right there in the platform menus.\nShare and rerun. Bundle data, scripts, and notes with Projects or send a Journal so collaborators can reproduce your analysis.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nResearch question\nThe plain-language question your study seeks to answer.\n\n\nTarget parameter\nThe specific population quantity you aim to learn (e.g., mean difference, proportion).\n\n\nStudy design\nThe plan for collecting data (sampling, measurement, assignment) to answer the question.\n\n\nObservational study\nA design that observes existing groups without assigning treatments.\n\n\nExperiment\nA design that assigns treatments (often at random) and compares outcomes.\n\n\nTreatment\nThe condition or intervention applied in an experiment.\n\n\nConfounding\nA third factor related to both treatment and outcome that distorts comparisons.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nA hospital asks: “Does a new scheduling system reduce ER wait times compared to the current system?”\n\nState a suitable target parameter.\nName a design choice (observational vs. experimental) and one reason for your choice.\n\nA university compares GPA between students who use tutoring and those who don’t, with no assignment or randomization. Name a potential confounder and how an experiment would address it.\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n(a)Target parameter: the mean difference in ER wait time (new minus current) for all eligible ER visits. (b) Design: a randomized experiment (e.g., randomize days or shifts to “new” vs. “current”), so groups differ only by scheduling system on average, improving causal interpretation.\nPotential confounder: prior academic preparation (e.g., incoming math placement). Students who seek tutoring might differ systematically. An experiment could randomly assign eligible students to tutoring or control (or randomize access), balancing confounders by design.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "01.html#footnotes",
    "href": "01.html#footnotes",
    "title": "1  Introduction to Statistics",
    "section": "",
    "text": "We will not discuss how to find these “chances” here. The chances of these two scenarios can be found using the binomial distribution discussed in ?sec-binomial.↩︎\nIn real world applications, you are usually limited by how you measure. For instance, you may be measuring the length of insects and you measure to the nearest millimeter. This limitation should not play a role in determining continuous RVs. So in theory, insects could measure between 10 and 20 millimeters. You only measure in millimeters but an insect could be 10.1, 10.114, 10.675, 10.000004, etc. Since there are infinite number of values in 10 to 20 that insects could measure in theory, we say the length is continuous.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "02.html",
    "href": "02.html",
    "title": "2  Collecting Data",
    "section": "",
    "text": "2.1 Sampling Methods\nA sample is a subset of a population, but not every subset is equally informative. A representative sample mirrors the key characteristics of the population closely enough that analyzing the sample leads to the same conclusions you would reach by studying the entire population. Put differently, it is a smaller group whose results accurately reflect those of the larger group. Designing studies that produce representative samples lies at the core of statistics-it allows us to save time and resources without compromising the credibility of our conclusions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "02.html#sec-sampling_methods",
    "href": "02.html#sec-sampling_methods",
    "title": "2  Collecting Data",
    "section": "",
    "text": "“And I knew exactly what to do. But in a much more real sense, I had no idea what to do.” - Michael Scott\n\n\n\nProbability vs. non-probability sampling\nThere are two big approaches to sampling:\n\nIn probability sampling, you use random mechanisms so that every member of the population has a known chance of being selected. This ensures that any differences between the sample and the population are due to random sampling error, not researcher choice, and makes it possible to quantify uncertainty in estimates.\nIn non-probability sampling, you choose participants based on convenience, volunteer responses, judgement or quotas. These methods are cheaper and often necessary for exploratory or qualitative work, but they make it hard to know whether the sample really reflects the population. Lack of a representative sample reduces the validity of conclusions and can introduce sampling bias.\n\nWhenever you want to generalize, probability sampling is the gold standard.\n\n\nSimple random sampling (SRS)\nIn a simple random sample (SRS), every individual in the population has an equal probability of being selected, and every possible sample of a given size is equally likely. To carry out an SRS, you must first have a sampling frame, which is a complete list of all units in the population. You then use a random mechanism-such as a random number generator or random number table-to choose the sample.\nIn practice, simple random sampling can be done with replacement or without replacement. Sampling without replacement is far more common in real studies, since it prevents the same individual from being selected more than once. Sampling with replacement is sometimes useful in theoretical settings, but it is rarely appropriate when collecting data from people or physical units.\nThe main advantage of an SRS is its fairness and transparency: because selection is purely random, it avoids systematic bias in who is included. However, SRS also has limitations. It requires a complete and accurate sampling frame, which may not exist or may be difficult to obtain. In addition, when a population is geographically dispersed or hard to reach, implementing an SRS can be costly or impractical.\n\n\n\n\n\n\nExample 2.1\n\n\n\nSuppose you want to survey 100 employees of a social media marketing company out of 1,000. You assign each employee a number from 1 to 1,000 and use a random number generator to select 100 distinct numbers. The employees corresponding to those numbers form your sample. Because each employee had an equal chance of being selected, the resulting sample is likely to be representative, especially if the sample size is sufficiently large.\nTo do this in JMP, suppose you have the list of 1,000 names like you see below.\n\nIf we wanted to get a random sample of 20, we would just click on Tables→Subset.\n\nThis will open up the following window.\n\nSelecting ‘Random - sample size’, entering the desired sample size in the box next to it, and then clicking ‘OK’ will make a new data table with 20 of the names selected at random.\nNote the option above labeled ‘Random - sampling rate’ will allow you to choose a proportion of the population to randomly sample instead of choosing a sample size.\n\n\n\n\n\n\n\n\nExample 2.2\n\n\n\nA university wants to estimate the average number of hours per week that undergraduates spend studying. The registrar has a list of all 12,000 enrolled undergraduates. Each student is assigned an ID number, and a computer randomly selects 400 unique IDs. Those students are contacted and asked to report their weekly study hours. This procedure produces a simple random sample because every student had the same chance of being included.\n\n\n\n\nSystematic sampling\nSystematic sampling is a practical alternative to simple random sampling that reduces logistical effort while preserving much of the randomness. Instead of selecting units entirely at random, you begin with an ordered sampling frame and choose every \\(k\\)-th unit, where \\(k\\) is the sampling interval. The value of \\(k\\) is typically determined by dividing the population size (\\(N\\)) by the desired sample size (\\(n\\)) (so \\(k \\approx N/n\\)). To introduce randomness, you first select a random starting point between 1 and \\(k\\), then continue by selecting units at fixed intervals.\nWhen the ordering of the list is unrelated to the variable being studied, systematic sampling closely approximates a simple random sample. It is often faster, easier to explain, and simpler to implement-especially in field settings or large administrative lists-because it avoids repeated random number generation.\nHowever, systematic sampling carries an important risk: periodicity. If the list has a repeating structure or is sorted in a way that is correlated with the outcome of interest, the method can unintentionally overrepresent some types of units and underrepresent others. For example, if employees are grouped by department, shift, or seniority, selecting every \\(k\\)-th name may systematically favor or exclude certain groups. In such cases, the resulting sample may be biased, even though the selection rule appears objective.\n\n\n\n\n\n\nExample 2.3\n\n\n\nAll employees of a company are listed in alphabetical order. You want to sample 100 employees from a list of 1,000, so you set \\(k = 10\\). You randomly select a starting position among the first ten names-say, the 6th person-and then select every 10th person on the list (6, 16, 26, 36, …) until 100 employees are chosen. This approach is easy to carry out and avoids duplicates, but it may be problematic if the alphabetical ordering aligns with job roles, seniority, or family relationships, which could distort the representativeness of the sample.\n\n\n\n\nStratified sampling\nIn stratified sampling, the population is first divided into distinct, non-overlapping subgroups called strata, based on a characteristic that is relevant to the study (such as gender, age group, geographic region, or job level). Each individual in the population belongs to exactly one stratum. After forming the strata, a random sample is drawn within each stratum, rather than from the population as a whole.\nThe most common approach is proportional stratified sampling, where the number of units sampled from each stratum is proportional to that stratum’s share of the population. This guarantees that the composition of the sample mirrors the composition of the population with respect to the stratifying variable. In some studies, researchers may intentionally oversample smaller or especially important strata (called disproportionate stratified sampling) to ensure enough data for detailed subgroup analysis, adjusting for this later in the analysis.\nStratified sampling has several advantages. By forcing representation from each stratum, it prevents small but important subgroups from being underrepresented due to random chance. In addition, when individuals within a stratum are relatively similar to one another but different across strata, stratification can substantially reduce sampling variability, leading to more precise estimates than a simple random sample of the same size.\nThe main drawback is increased complexity. Stratified sampling requires reliable information to classify individuals into strata before sampling begins, as well as separate sampling procedures within each stratum. This can increase administrative effort, cost, and the potential for implementation errors if strata are poorly defined or misclassified.\n\n\n\n\n\n\nExample 2.4\n\n\n\nA company has 800 junior employees and 200 senior employees. Because job seniority is expected to influence workplace satisfaction, the company divides employees into two strata: junior and senior. To draw a sample of 100 employees that reflects the population structure, the company randomly selects 80 junior employees and 20 senior employees. The resulting sample preserves the original 80/20 split, ensuring that both groups are appropriately represented and that comparisons between junior and senior employees are meaningful.\n\n\n\n\nCluster sampling\nCluster sampling also divides the population into groups, but the logic is fundamentally different from stratified sampling. In cluster sampling, each cluster is intended to be a small-scale version of the entire population-a mini-population that contains a diverse mix of individuals. Rather than ensuring representation from every group, the goal is to reduce cost by limiting how many groups are studied.\nTo implement cluster sampling, the population is first partitioned into clusters, often based on geography, organizational units, or naturally occurring groupings (such as schools, offices, or neighborhoods). A random sample of clusters is then selected, and data are collected from all individuals within the selected clusters (one-stage cluster sampling) or from a random subset of individuals within those clusters (two-stage cluster sampling).\nThe primary advantage of cluster sampling is efficiency. When populations are large and geographically dispersed, it is often impractical or prohibitively expensive to reach individuals scattered across many locations. By concentrating data collection within a small number of clusters, researchers can dramatically reduce travel, coordination, and administrative costs. For this reason, cluster sampling is commonly used in large-scale surveys, public health studies, and educational assessments.\nThe main drawback is increased variability. If individuals within a cluster tend to be similar to one another-and clusters differ meaningfully from one another-then sampling only a few clusters may fail to capture the full diversity of the population. This intra-cluster similarity reduces the effective amount of information in the sample and can lead to larger sampling variance than a simple random sample of the same size. In practice, this means cluster sampling often requires a larger total sample size to achieve comparable precision.\n\n\n\n\n\n\nExample 2.5\n\n\n\nA company operates offices in 10 cities across the country, each employing roughly the same number of workers and performing similar roles. To conduct an employee satisfaction survey, the company randomly selects 3 offices and surveys every employee in those locations. This approach greatly reduces travel and administrative effort. However, it relies on the assumption that offices are reasonably similar; if workplace culture or management practices vary substantially by city, the survey results may be less precise or even misleading.\n\n\n\n\nNon-probability methods\nIn some research settings, random selection is impossible, impractical, or unnecessary. Non-probability sampling methods rely on researcher judgment or participant availability rather than chance. Because individuals do not have known or equal probabilities of selection, these methods cannot guarantee that the sample is representative of the population. As a result, conclusions drawn from non-probability samples should be interpreted cautiously and generally should not be generalized to a broader population.\nDespite this limitation, non-probability methods are widely used in practice. They are common in exploratory studies, pilot research, qualitative work, and studies involving hard-to-reach populations, where constructing a complete sampling frame or implementing random selection is unrealistic. In such cases, the goal is often to generate insights, identify patterns, or refine research questions rather than to make precise population-level estimates.\n\nConvenience sampling\nA convenience sample consists of individuals who are easiest for the researcher to access. Participants are selected simply because they are readily available, willing, or nearby. This approach is inexpensive, fast, and easy to implement, which makes it appealing for classroom projects, preliminary studies, and early stages of research.\nHowever, convenience sampling comes with serious limitations. Because selection is driven by accessibility rather than randomness, certain groups may be systematically overrepresented while others are excluded entirely. This can introduce substantial bias, meaning the results may reflect the characteristics of the convenience group rather than those of the population of interest. For this reason, findings from convenience samples should not be treated as broadly generalizable.\n\n\n\n\n\n\nExample 2.6\n\n\n\nYou want to learn about student perceptions of campus support services. After each of your classes, you ask students in the room to complete a short survey. While this approach is quick and easy, it only captures the views of students enrolled in your classes. These students may differ from the broader student body in major, year, motivation, or academic engagement, so the resulting sample is not representative of all students at the university.\n\n\n\n\nVoluntary response sampling\nA voluntary response sample is formed when individuals choose for themselves whether to participate in a study. Invitations are typically broad-such as open surveys, online polls, or public feedback forms-but participation is entirely self-selected. Because respondents are not randomly chosen, the resulting sample often reflects the views of those who feel most strongly about the topic.\nThe key issue with voluntary response sampling is self-selection bias. People with intense opinions, strong grievances, or high levels of engagement are much more likely to respond than those who are indifferent or moderately affected. As a result, voluntary response samples tend to exaggerate extremes and provide a distorted picture of average attitudes or behaviors.\nAlthough voluntary response samples are easy and inexpensive to collect, they are inappropriate for drawing conclusions about a population as a whole. They are best used for gathering feedback, identifying potential concerns, or generating hypotheses rather than for making quantitative estimates.\n\n\n\n\n\n\nExample 2.7\n\n\n\nYou email a survey to the entire student body asking for opinions about a new campus policy. Only a small fraction of students respond, and those responses come primarily from students who are either strongly supportive or strongly opposed. Students with neutral or mildly held views are far less likely to participate, so the results cannot be trusted to represent the typical student’s perspective.\n\n\n\n\nPurposive sampling\nIn purposive sampling, participants are deliberately selected by the researcher because they possess characteristics, experiences, or knowledge that are especially relevant to the research question. Rather than aiming for representativeness, the goal is to obtain information-rich cases that can provide deep insight into a particular phenomenon.\nThis method is common in qualitative research, case studies, and applied settings where understanding how or why something occurs is more important than estimating its prevalence. Researchers often aim for diversity within the purposive sample-such as selecting participants with different backgrounds or experiences-to capture a range of perspectives.\nThe main limitation of purposive sampling is that selection is subjective and non-random. While the resulting data can be rich and informative, it cannot support population-level inference or precise numerical claims.\n\n\n\n\n\n\nExample 2.8\n\n\n\nYou want to understand how well university services support students with disabilities. You intentionally recruit students with different types of disabilities and varying levels of accommodation needs. This allows you to explore common challenges and contrasts across experiences, but it does not allow you to estimate what proportion of all disabled students share those experiences.\n\n\n\n\nSnowball sampling\nSnowball sampling is a recruitment method in which existing participants help identify and recruit additional participants. The process typically begins with a small number of initial subjects, who then refer others they know who meet the study criteria. Over time, the sample grows through these social connections, much like a snowball rolling downhill.\nSnowball sampling is especially useful for studying hidden, stigmatized, or hard-to-reach populations, such as individuals experiencing homelessness, undocumented workers, or members of informal networks. In these cases, traditional sampling frames may not exist, and trust is often essential for participation.\nHowever, snowball sampling provides little control over who is included. Because referrals occur within social networks, the sample may overrepresent individuals who are more connected or similar to the initial participants. This can introduce substantial bias and limit the generalizability of findings.\n\n\n\n\n\n\nExample 2.9\n\n\n\nTo study experiences of homelessness, you begin by interviewing one participant who agrees to take part in the study. She then introduces you to others she knows who are also homeless, and those participants provide additional referrals. While this approach allows you to access a difficult-to-reach population, the resulting sample may reflect a narrow subset of experiences shaped by shared social connections.\n\n\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nRepresentative sample\nA sample that accurately reflects key characteristics of the population.\n\n\nProbability sampling\nSampling technique using random selection so each unit has a known chance of inclusion.\n\n\nNon-probability sampling\nSampling techniques based on convenience or judgement without randomisation.\n\n\nSimple random sampling\nEvery unit has an equal chance of selection; implemented via random number generators.\n\n\nSystematic sampling\nSelecting every \\(k\\)-th unit from an ordered list after a random start.\n\n\nStratified sampling\nDividing the population into subgroups and randomly sampling within each subgroup.\n\n\nCluster sampling\nRandomly selecting entire groups (clusters) and studying all units within them.\n\n\nConvenience sampling\nIncluding the most accessible units; prone to sampling and selection bias.\n\n\nVoluntary response\nSampling based on participants who choose to respond, often those with strong opinions.\n\n\nPurposive sampling\nSelecting cases based on researcher judgement of what is most informative.\n\n\nSnowball sampling\nRecruiting participants via referrals from initial subjects, often for hidden populations.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nYou want to estimate the average GPA of all first-year students at your university.\n\nName two probability sampling methods you could use.\nBriefly explain why a convenience sample of your friends might mislead you.\n\nA researcher selects every 5th name from a sorted list of patients to survey. What sampling method is this? Under what circumstance might this method introduce bias?\nCompare stratified sampling and cluster sampling. Give an example of a scenario where each would be appropriate.\nExplain why voluntary response samples often yield extreme views and cannot be trusted for generalizing to a population.\nIn JMP, how could you create a simple random sample of 150 observations from a data table with 2,000 rows?\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n(a)Simple random sampling (assign each first-year student a number and randomly select using a random number generator); stratified sampling (divide students by major or residence hall and sample proportionally within each group). (b) Your friends are likely from similar classes or social circles, so they may have similar study habits; they might not reflect the broader student body.\nThis is systematic sampling. It works well if the list has no pattern related to the outcome. If patients are sorted by appointment time, every 5th patient might always be a morning appointment, which could bias results if morning and afternoon patients differ.\nStratified sampling divides the population into meaningful groups and samples within each (e.g., sampling men and women separately when studying height). It ensures each subgroup is represented. Cluster sampling selects whole groups (e.g., choosing three hospitals at random and surveying all nurses within them) to save cost when the population is geographically spread out.\nPeople with strong positive or negative feelings are more likely to volunteer, while those who are neutral remain silent. This self-selection skews the sample, so the responses do not reflect the average opinion in the population.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "02.html#experimental-design",
    "href": "02.html#experimental-design",
    "title": "2  Collecting Data",
    "section": "2.2 Experimental Design",
    "text": "2.2 Experimental Design\n\n“All life is an experiment. The more experiments you make the better.” -Ralph Waldo Emerson\n\nStatistics provides two complementary approaches for gathering evidence: surveys and experiments. In a survey, we select individuals from a population, collect information by asking questions, and summarize the resulting data. In an experiment, we actively assign treatments to units and observe their responses. In both cases, sound inference depends on thoughtful sampling and careful study design. While other methods of data collection exist, surveys and experiments are the approaches most people have in mind when they think about gathering data.\n\nPrinciples of good experimental design\nThe goal of an experiment is to isolate the causal effect of a treatment by systematically controlling other sources of variation. Experiments give the researcher direct control over how treatments are assigned. To ensure that any observed differences in outcomes can be attributed to the treatment itself-and not to hidden biases or confounding factors-well-designed experiments rely on four core principles.\n\nRandomization\nRandomization is the foundation of experimental design. Assigning experimental units to treatment conditions by chance ensures that, on average, the groups being compared are similar with respect to both observed characteristics (such as age or prior experience) and unobserved characteristics (such as motivation or genetic differences). This balance allows differences in outcomes to be interpreted as causal effects of the treatment rather than artifacts of preexisting differences.\nIn a completely randomized design, each unit is assigned to a treatment independently of the others. For example, in a sleep study, students might be randomly assigned to different cell phone usage limits before bedtime. This approach is simple and effective when experimental units are fairly homogeneous.\nIn a randomized block design, units are first grouped into blocks based on a characteristic known to influence the response, and treatments are then randomly assigned within each block. For instance, agricultural plots might be blocked by rainfall zone before assigning fertilizer types. Blocking removes predictable variation due to the blocking variable, allowing randomization to work more efficiently within each group.\nRandomization also applies beyond treatment assignment. In industrial or laboratory settings, the order of experimental runs should be randomized to prevent time-related factors-such as equipment warming, operator fatigue, or ambient temperature-from becoming confounded with treatment effects.\n\n\n\n\n\n\nExample 2.10\n\n\n\nIn a drug study with 20 mice and two test kits (A and B), you might randomly assign 10 mice to kit A and the remaining 10 to kit B. Differences in age, weight, or health are then spread randomly across the two groups, preventing these factors from systematically favoring one treatment over the other.\n\n\n\n\nControl and placebo\nA well-designed experiment includes a meaningful comparison, typically between a treatment group and a control group. The control group provides a baseline against which the treatment effect is measured. Depending on the context, the control may receive no treatment, standard care, or a placebo-a treatment designed to mimic the experience of receiving the real intervention without containing the active ingredient.\nPlacebo controls are especially important in medical and psychological studies because participants’ expectations alone can influence outcomes. Improvements due to belief, attention, or the act of being treated are known as placebo effects. By giving both groups identical experiences except for the active component, researchers can attribute differences in outcomes specifically to the treatment.\n\n\nReplication\nReplication refers to applying each treatment to multiple experimental units. Replication is essential because outcomes naturally vary from unit to unit, even under identical conditions. By observing this variability, researchers can estimate the amount of random noise in the data and determine whether observed differences between treatments are larger than would be expected by chance alone.\nReplication increases the precision of estimated treatment effects and provides the information needed for statistical inference. A single observation per treatment cannot distinguish a genuine effect from an unusual outcome.\n\n\n\n\n\n\nExample 2.11\n\n\n\nMeasuring battery life under a specific charging condition using several batteries gives a far more reliable estimate than testing just one battery, which might be unusually good or unusually poor.\n\n\n\n\nBlocking\nBlocking is used when a nuisance variable-one that is not of primary interest but is known to affect the response-can be identified in advance. In a block design, experimental units are grouped into relatively homogeneous blocks based on this variable, and treatments are randomized within each block.\nBlocking reduces unexplained variability by accounting for known sources of variation. This leads to more precise estimates of treatment effects without increasing sample size.\nNatural blocking structures often arise in practice. In biological experiments, animals may be grouped by litter; in chemical experiments, samples may come from different batches; in education studies, students may be grouped by classroom. Treating these groupings as blocks and randomizing within them prevents block-level differences from obscuring the treatment effect.\n\n\n\nPutting the principles together\nStrong experimental designs typically combine these principles rather than relying on just one. For example, an agricultural study might block plots by rainfall zone, randomly assign fertilizer treatments within each block, and replicate measurements across multiple growing seasons.\nExperiments also differ in structure. In between-subjects designs, each unit receives exactly one treatment condition. In within-subjects (or repeated-measures) designs, each unit experiences all treatment conditions, usually in a randomized or counterbalanced order. Within-subjects designs reduce variability by allowing each unit to serve as its own control, but they require careful attention to order effects, such as learning, fatigue, or carryover. Randomizing or counterbalancing the order of treatments is essential to preserve validity.\nTogether, randomization, control, replication, and blocking form the backbone of experimental design, enabling researchers to make credible causal claims from data.\n\n\nDesigning unbiased survey questions\nHigh-quality surveys require care in two distinct areas: how respondents are selected and how questions are written. Sampling determines who provides data (see Section 2.1); question design determines what information those respondents actually provide. Even a perfectly representative sample can produce misleading results if the questions themselves are biased, confusing, or poorly structured.\nWell-designed questions are clear, neutral, and interpretable in the same way by all respondents. Poorly designed questions can systematically distort responses, introduce bias, and lead decision-makers to draw incorrect conclusions. Below are several common pitfalls in survey question design, along with examples and revisions.\n\nLeading questions\nLeading questions subtly (or not so subtly) push respondents toward a particular answer by framing one response as more reasonable, popular, or desirable than others. This can inflate support for a policy, product, or opinion simply through wording rather than genuine sentiment.\nBiased:\n\n“Don’t you agree that our new app is much easier to use?”\n\nThis wording assumes agreement and pressures respondents to conform.\nUnbiased:\n\n“How would you rate the ease of use of our new app?”\n\nAnother example: Biased: “Most students think this course is well organized. Do you agree?” Unbiased: “How would you rate the organization of this course?”\n\n\nLoaded questions\nLoaded questions embed an assumption-often a controversial or emotionally charged one-into the question itself. Respondents are forced to accept the premise in order to answer, even if they disagree with it.\nBiased:\n\n“When did you stop wasting time on your phone?”\n\nThis question assumes the respondent wastes time on their phone and that they have already stopped.\nUnbiased:\n\n“How much time do you spend on your phone each day for non-work activities?”\n\nAnother example: Biased: “Why do you support unfair tuition increases?” Unbiased: “What is your opinion on recent tuition increases?”\n\n\nDouble-barreled questions\nDouble-barreled questions ask about two (or more) distinct issues but allow only a single response. Because respondents may have different opinions about each component, the resulting data are ambiguous and difficult-or impossible-to interpret.\nBiased:\n\n“Do you intend to leave work and return to full-time study this year?”\n\nA respondent might plan to leave work but not return to school, or vice versa.\nUnbiased:\n\n“Do you intend to leave your current job this year?” “Do you intend to return to full-time study this year?”\n\nAnother example: Biased: “How would you rate our products and level of service?” Unbiased:\n\n“How would you rate the quality of our products?” “How would you rate the quality of our customer service?”\n\n\n\nAmbiguous wording\nAmbiguous wording occurs when a question uses vague terms or phrases that different respondents may interpret differently. When this happens, people may answer different questions even though they are responding to the same survey item.\nBiased:\n\n“How do we compare to our competitors?”\n\nRespondents may interpret this as referring to price, quality, customer service, innovation, or brand reputation.\nUnbiased:\n\n“Compared to our competitors, how would you rate our prices?” “Compared to our competitors, how would you rate our customer service?”\n\nAnother example: Biased: “How often do you exercise regularly?” Unbiased: “On how many days per week do you engage in at least 30 minutes of physical activity?”\n\n\n\nWhy this matters\nSurvey questions shape the data you collect. Poorly worded questions can introduce bias just as surely as a flawed sampling method. Clear, neutral, and focused questions help ensure that responses reflect what respondents truly believe or experience, rather than how the question guided them. In practice, careful question design often requires multiple drafts, pilot testing, and revision-but the payoff is data that support credible, defensible conclusions.\nTo craft unbiased and effective survey questions, researchers should follow several key principles. These guidelines help ensure that respondents interpret questions consistently and feel free to answer honestly, leading to data that support meaningful conclusions.\n\nUse neutral language. Questions should be phrased without emotionally charged words, value judgments, or implied “correct” answers. Neutral wording allows respondents to express their true opinions rather than reacting to the tone of the question. Even subtle cues-such as describing a policy as “beneficial” or “harmful”-can influence responses. Replacing evaluative language with descriptive phrasing helps minimize this source of bias.\nBe specific and clear. Vague terms can mean different things to different respondents. Whenever possible, define key concepts, specify time frames, and avoid shorthand that assumes shared understanding. For example, instead of asking “How often do you use the library?” a clearer question would be “How many times have you visited the library in the past month?” Specific wording improves consistency and makes responses easier to interpret and analyze.\nAsk one thing at a time. Each survey item should measure a single concept. When a question combines multiple ideas, respondents may agree with one part but not the other, producing answers that are difficult to interpret. Splitting complex questions into separate items allows each concept to be measured cleanly and reduces confusion for respondents.\nBalance response options. Response scales should be symmetrical and evenly spaced, offering a full range of plausible choices. For example, a 5-point Likert scale ranging from “Strongly disagree” to “Strongly agree” treats positive and negative responses equally and includes a neutral midpoint if appropriate. Unbalanced or uneven scales can push respondents toward certain answers and distort the results.\nPilot test your survey. Even well-intentioned questions can be misunderstood. Pilot testing the survey with a small group of respondents helps identify ambiguous wording, confusing response options, or unintended interpretations. Feedback from pilot tests often reveals issues that are not obvious to the survey designer but can significantly affect data quality if left unaddressed.\n\nTogether, these principles promote clarity, neutrality, and fairness in survey design. By reducing confusion and bias, they help respondents provide thoughtful, accurate answers-and help researchers draw conclusions that are trustworthy and defensible.\n\n\nRecap\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nRandom assignment\nAssigning sampled units to treatment conditions by chance to create comparable groups.\n\n\nTreatment group / control group\nGroups receiving the experimental intervention and baseline comparison, respectively.\n\n\nPlacebo\nAn inert treatment used to mimic the experience of the intervention to control for expectations.\n\n\nReplication\nRepeating the same treatment on multiple experimental units to estimate variability.\n\n\nBlocking\nGrouping similar units and randomizing within each group to control a nuisance factor.\n\n\nBetween-subjects design\nEach unit experiences only one condition; comparisons are across subjects.\n\n\nWithin-subjects design\nEach unit experiences all conditions in random order.\n\n\nLeading question\nA survey question that suggests a particular answer.\n\n\nLoaded question\nA survey question containing an assumption or implication.\n\n\nDouble-barreled question\nA single question that asks about two things.\n\n\nAmbiguous wording\nVague terms that can be interpreted differently by different respondents.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nExplain the difference between random sampling and random assignment. Why are both important, and in what contexts do they apply?\nName the four principles of good experimental design and give a brief example of each.\nConsider this survey question: “How satisfied are you with the cost and quality of your textbooks?” Identify the problem and rewrite the question.\nIn a study of exam performance, 60 students volunteer for tutoring and 60 do not. The volunteer group has a higher average GPA than the non-volunteer group. Explain why this study may not show that tutoring causes better performance. How could you redesign it?\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nRandom sampling determines who gets into the study. Every member of the population has a known chance of selection, improving generalizability. Random assignment determines which condition participants experience, creating comparable groups and allowing causal conclusions. Surveys rely on random sampling; experiments rely on random assignment.\nRandomization: assign units by chance (e.g., randomize phone use levels to study sleep). Control/placebo: include a baseline or placebo condition to isolate the treatment effect. Replication: repeat treatments on multiple units, like testing several batteries under the same condition. Blocking: group units by a nuisance factor (e.g., soil type) and randomize within blocks.\nThe question is double-barreled-it asks about cost and quality. Rewrite as two separate questions (e.g., “How satisfied are you with the cost of your textbooks?” and “How satisfied are you with the quality of your textbooks?”).\nVolunteers may differ systematically from non-volunteers (e.g., motivation or prior GPA). Random assignment is missing. To infer causality, randomly assign students to tutoring or control groups and compare outcomes, possibly blocking on prior GPA.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "02.html#observational-studies-vs.-experiments",
    "href": "02.html#observational-studies-vs.-experiments",
    "title": "2  Collecting Data",
    "section": "2.3 Observational Studies vs. Experiments",
    "text": "2.3 Observational Studies vs. Experiments\n\n“You can observe a lot by just watching.” - Yogi Berra\n\nAt first glance, collecting data looks the same whether you’re watching what happens or deliberately changing something. But how you gather the data matters tremendously for what you can conclude.\n\nObservational studies: watching without intervening\nIn an observational study, researchers record what happens without actively assigning exposures or treatments. Common types include:\n\nCohort Studies\nCohort studies, where a group of people linked by a characteristic (e.g., birth year) is followed over time. Researchers compare outcomes between those exposed to some factor and those not exposed.\nA classic example of a cohort study is the Framingham Heart Study (FHS), which has been described in detail in the International Journal of Epidemiology. In 1948 the National Heart Institute recruited a community-based cohort of 5,209 adults aged 30–59 years from Framingham, Massachusetts, to investigate causes of cardiovascular disease (CVD). Two of every three families in the town were randomly sampled and invited; 4,494 (about 69%) agreed to participate, and an additional 715 volunteers joined. This initial prospective cohort has been followed every two to four years with detailed medical histories, physical examinations, electrocardiograms and laboratory tests. By following participants longitudinally for decades, the FHS identified major risk factors for CVD-such as high blood pressure, cholesterol, and smoking-helping to shape modern cardiovascular prevention guidelines.\n\n\nCase-Control Studies\nCase–control studies, where people with a condition (“cases”) are compared to similar people without it (“controls”) to look for differences in past exposures.\nFor example, a German study1 compared 118 patients with a rare form of eye cancer called uveal melanoma to 475 healthy patients who did not have this eye cancer. The patients’ cell phone use was measured using a questionnaire. On average, the eye cancer patients used cell phones more often. The cases were those who had developed uveal melanoma and the controls were those who did not uveal melanoma. The cell phone use was compared between the two groups.\nBecause participants choose their own behaviors, observational data reflect the real world and are often the only ethical way to study harmful exposures. For example, you can’t ethically assign people to smoke or not smoke, so the long-term effects of smoking are studied by tracking smokers and non-smokers over time.\nObservational studies are usually quicker and cheaper than experiments and have high ecological validity (they mirror everyday life). But they have a critical limitation: you can’t be sure whether differences in outcomes are caused by the exposure or by other factors that differ between groups. For example, a highly publicized 1985 study from Johns Hopkins University linked coffee consumption to an increased risk of heart disease, especially for heavy drinkers. The study’s findings, published in the American Journal of Epidemiology, were later challenged by other research that pointed out the failure to adequately control for the effect of cigarette smoking. Once smoking was controlled for, the link between coffee consumption and increased risk of heart disease was no longer significant.\n\n\n\nExperiments: deliberately changing something\nAs discussed previously, in an experiment, researchers assign treatments or interventions to units and observe the effects. Randomization-assigning units by chance-ensures that, on average, the groups are comparable on both observed and unobserved characteristics. The classic experimental design is the completely randomized design: participants are randomly allocated to receive a new drug, a placebo, or no treatment, and outcomes are compared. Experiments are considered the gold standard for establishing causality because randomization eliminates systematic differences between groups.\nExperiments also offer a controlled environment, making it easier to isolate the effect of a single factor. However, they can be expensive, time-consuming, or unethical to conduct.\nFor example, suppose we were interested in the association between eye cancer and smart phone use. Suppose we conduct an experiment, such as the following:\n\nPick half the students from your school at random and tell them to use a smart phone each day for the next 50 years.\nTell the other half of the student body not to ever use smart phones.\nFifty years from now, analyze whether cancer was more common for those who used smart phones.\n\nThere are obvious difficulties with such an experiment:\n\nIt’s not ethical for a study to expose over a long period of time some of the subjects to something (such as smart phone radiation) that we suspect may be harmful.\nIt’s difficult in practice to make sure that the subjects behave as told. How can you monitor them to ensure that they adhere to their treatment assignment over the 50-year experimental period?\nWho wants to wait 50 years to get an answer?\n\nThus, an observational study would be preferred over an experiment.\n\n\nWhy observational studies can mislead\nObservational data are susceptible to confounding-a situation where a third factor influences both the exposure and the outcome, creating a spurious association. For example, an observational cohort might find that people who meditate have lower rates of heart disease. But meditators may also exercise more and eat healthier diets, making it unclear whether meditation or lifestyle explains the difference. Similarly, people who choose to take daily vitamins might generally have healthier habits, so observed vitamin benefits may reflect those habits rather than the vitamins themselves.\nRandomization is the only method that can eliminate potential confounders by balancing both measured and unmeasured factors across treatment groups. In observational research, statistical methods like stratification, regression adjustment and propensity score matching can reduce bias, but they depend on untestable assumptions: all confounders must be measured correctly and modeled properly. Many important confounders may be unknown or infeasible to measure. Even meticulously controlled observational studies cannot remove all confounding. As a result, observational evidence alone “cannot support conclusions of causation”.\nBelow is a simple simulation illustrating confounding. Shoe size and reading ability appear positively related, but both are driven by age. When age is not controlled, a misleading association emerges.\n\n\n\n\n\n\n\n\n\nThe scatterplot shows a strong correlation between shoe size and reading, even though neither directly affects the other. The common cause is age. Observational studies must always consider whether a hidden variable like age could be responsible for an observed association.\n\n\nRecap\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nObservational study\nA study in which researchers record exposures and outcomes without assigning treatments or interventions.\n\n\nCohort study\nObservational design where a group is followed over time to compare outcomes between exposed and unexposed members.\n\n\nCase–control study\nObservational design where people with a condition (“cases”) are compared to similar people without the condition (“controls”) to look for differences in past exposures.\n\n\nExperiment\nA study where researchers introduce an intervention and randomly assign subjects to treatment or control groups.\n\n\nConfounding\nA situation where a third factor influences both the exposure and the outcome, potentially creating a spurious association.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nA nutrition researcher recruits people who already take daily multivitamins and compares their health outcomes to people who do not.\n\nIs this an observational study or an experiment?\nName at least two potential confounding variables.\n\nIn a randomized trial, half the participants are assigned to eat a Mediterranean diet and half to continue their usual diet. After a year, the first group shows lower cholesterol. Explain why randomization strengthens the causal interpretation.\nA cohort study finds that people who bike to work have lower rates of depression than those who drive. Suggest two reasons why this association may not reflect a causal effect of biking.\nDescribe a research question that would be unethical or impractical to answer via experiment but could be studied observationally. Explain why.\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\na)This is an observational study because participants choose whether or not to take multivitamins. b) Potential confounders include diet quality, exercise habits, socioeconomic status, access to healthcare, smoking status and other health behaviors.\nRandomization assigns diets by chance, so, on average, both known and unknown factors (age, lifestyle, genetics) are balanced across the groups. Therefore, differences in cholesterol are likely due to the diet rather than pre-existing differences.\nPeople who bike may have higher baseline fitness and better mental health; they might live in neighborhoods with better infrastructure or community support; they may also have lifestyles that promote well-being (e.g., more time outdoors). Any of these confounders could explain the observed association.\nStudying the long-term effects of smoking is unethical to do experimentally, because you can’t randomly assign people to smoke. Instead, researchers observe smokers and non-smokers and compare outcomes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "02.html#sources-of-bias",
    "href": "02.html#sources-of-bias",
    "title": "2  Collecting Data",
    "section": "2.4 Sources of Bias",
    "text": "2.4 Sources of Bias\n\n“Normally if given a choice between doing something and nothing, I’d choose to do nothing. But I would do something if it helps someone do nothing. I’d work all night if it meant nothing got done.” - Ron Swanson\n\nWhen we talk about bias in statistics, we mean a systematic error built into the way we select or measure our data. Bias is different from random sampling error. Sampling error comes from the natural variability you get when you observe only part of the population and tends to shrink as sample sizes grow. Bias, by contrast, does not go away with bigger samples; a flawed design simply produces more confident wrong answers. That makes it important to understand the different ways bias sneaks into our studies.\n\nHow bias differs from sampling error\nWhenever we select a sample, the numbers we compute (like the mean or proportion) will vary from one sample to the next. This variability is called sampling error. If we repeated our survey many times with different random samples, the average of those sample statistics would be the true population value, and the spread among them would reflect sampling error. Increasing the sample size reduces sampling error, but it does not correct for systematic flaws in how the sample was chosen. When the method of collecting or measuring data systematically favors some outcomes over others, we call it bias. A biased sample can be huge and still be wrong because its error is baked into the design.\nTo illustrate the difference, imagine a population with a true average income of $8 (in arbitrary units), made up of 70% low earners (income of 5) and 30% high earners (income of 15). Below we simulate two ways of sampling from this population: a fair simple random sample and a biased sample that over-selects high earners (80% high, 20% low). As the sample size grows, the random sample mean settles near the true average, while the biased sample mean stays high. This shows that increasing the sample size reduces random error but does not fix bias.\n\n\n\n\n\n\n\n\n\n\n\nCommon sources of bias\nBias can enter at many points in the data-collection process. Here are some of the most common culprits:\n\nCoverage (undercoverage) bias\nA coverage bias occurs when some members of the population are not included in the sampling frame. The Literary Digest’s famous 1936 presidential poll relied on telephone directories and car registration lists, thereby missing less affluent voters who tended to support Franklin Roosevelt. Because those voters were excluded, the sample favored wealthier respondents and overpredicted Alfred Landon’s support.\n\n\n\n\n\nNonresponse bias\nA nonresponse bias arises when selected individuals choose not to participate and the responders differ systematically from nonresponders. In the same 1936 survey only 25% of those sampled returned the mail-in ballot. Landon supporters were more likely to return the survey, so the results overestimated his popularity.\n\n\nVoluntary response bias\nWhen people opt into a survey on their own-like call-in radio polls about controversial topics-the sample disproportionately includes individuals with strong opinions. This voluntary response bias can produce extreme results because moderate voices remain silent.\n\n\nConvenience sampling bias\nA convenience sample chooses whoever is easiest to reach. If you stand outside a gym to survey “all adults in the city,” your sample will overrepresent health-conscious people. Convenience sampling often leads to coverage problems.\n\n\nResponse (measurement) bias\nEven if we select the right people, the way we ask questions or record data can introduce response bias. Response bias occurs when the measurement process influences the answer: leading questions or unbalanced answer choices can nudge respondents toward particular responses. Social desirability bias occurs when people underreport socially undesirable behaviors or overreport virtuous ones.\n\n\nSurvivorship bias\nWhen we only observe “survivors” and ignore those that dropped out or failed, we can mistake success for the rule.\n\n\n\nThe most famous example of this is the WWII bomber problem. During WWII, analysts tallied bullet holes on returning Allied bombers and saw clusters on wings and fuselage, with relatively few in engines and cockpit. The intuitive fix was to add armor where the holes were densest. Statistician Abraham Wald pointed out the trap: these data come only from planes that survived. Holes on the survivors mark places a plane can be hit and still make it home. The missing planes-those that didn’t return-are precisely the ones likely hit in the “clean” areas (e.g., engines). So Wald recommended reinforcing the areas with the fewest holes on the survivors, not the most. That’s survivorship bias: drawing conclusions from only the observed “winners” and ignoring the unseen “failures.”\n\n\nRecall bias\nIn retrospective studies, participants may not remember past events accurately. People who have developed an illness might recall exposures differently than healthy controls, leading to systematic differences.\n\n\nInterviewer bias\nThe interviewer’s tone, appearance or expectations can subtly influence responses. Neutral wording and training can reduce this effect.\n\n\nHealthy-user bias and attrition bias\nPeople who choose to participate in certain programs or who remain in a study for its duration often differ from those who do not, leading to biased estimates.\nEach of these biases stems from the way participants are chosen or how data are measured; they cannot be “averaged out” by larger samples.\n\n\n\nMitigating bias\nTo minimize bias:\n\nUse probability sampling whenever you want to generalize to a population. Random sampling helps guard against undercoverage and voluntary response bias.\nEnsure your sampling frame matches your target population. Consider oversampling underrepresented groups and weighting responses to reflect their true proportion.\nFollow up with nonresponders and offer multiple modes of participation to reduce nonresponse bias.\nDesign neutral, balanced questions and offer anonymity to reduce measurement and social desirability bias.\nDocument who was invited and who actually participated so you can assess potential biases.\nIn observational studies, adjust for measured differences between participants and nonparticipants using weighting or modeling; but remember that unmeasured biases may remain.\n\n\n\nRecap\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nCoverage bias\nSystematic error that arises when part of the population is missing from the sampling frame.\n\n\nNonresponse bias\nBias introduced when individuals who do not respond differ meaningfully from those who do respond.\n\n\nVoluntary response bias\nBias caused by allowing people to opt into a survey; respondents with strong opinions dominate the sample.\n\n\nResponse bias\nBias that arises from flaws in the measurement process, such as leading questions or social desirability.\n\n\nSampling error\nNatural variability in statistics from sample to sample; decreases with larger samples.\n\n\nBias\nSystematic error due to design or measurement; does not diminish with larger samples.\n\n\nSurvivorship bias\nFocusing only on observed “survivors” and ignoring those that failed, leading to overly optimistic conclusions.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nA tech company sends an email survey to customers using its premium service. Over half of the recipients do not respond. The company concludes that 85 % of its customers are satisfied.\n\nIdentify two potential sources of bias.\n\nSuggest one way to mitigate each bias.\n\nA political action group hosts an online poll on its website asking visitors whether they support a proposed tax increase. Seventy-five percent say “no.” What type of bias is most likely, and why does this poll not reflect general public opinion?\nSuppose you draw a simple random sample of 1,000 Baylor students from a roster and send them a questionnaire. Only 200 students respond. How could you use follow-ups or weighting to reduce bias? Explain your reasoning.\nExplain the difference between sampling error and bias in your own words. Why can a huge sample still give a wrong answer?\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n(a)Coverage bias and nonresponse bias. The company sampled only premium users (ignoring basic or free users) and most of the sampled customers did not respond, so respondents may differ from nonrespondents. (b) To reduce coverage bias, draw a sample from all customers or weight responses to reflect the full user base. To reduce nonresponse bias, send reminders, offer incentives or provide alternative modes (e.g., phone, mail).\nThis is voluntary response bias: only visitors who care enough to vote participate, and they may have strong opinions. A poll embedded on a partisan website cannot be generalized because participants are self-selected and not representative of the broader population.\nThe low response rate introduces nonresponse bias. You could send follow-up reminders, offer incentives, or contact nonrespondents by phone to increase participation. If demographic data are available for all sampled students, you can apply weights so that the 200 responders reflect the distribution of the 1,000 sampled students (and thus the target population).\nSampling error is the random fluctuation you see from one sample to the next; it decreases with larger samples. Bias is a systematic error built into the design or measurement; it doesn’t shrink with bigger samples. A huge convenience or volunteer sample can still give a wrong answer if it systematically excludes part of the population or asks questions in a biased way.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "02.html#footnotes",
    "href": "02.html#footnotes",
    "title": "2  Collecting Data",
    "section": "",
    "text": "Stang, A., Anastassiou, G., Ahrens, W., Bromen, K., Bornfeld, N., & Jöckel, K. H. (2001). The possible role of radio frequency radiation in the development of uveal melanoma. Epidemiology, 7-12.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "03.html",
    "href": "03.html",
    "title": "3  Describing Data with Tables and Graphs",
    "section": "",
    "text": "3.1 Organizing Categorical Data\nWhen you collect data that fall into groups—like preferred streaming service, political affiliation, or type of pet—the first step is to count how many observations fall into each category. Those counts form the backbone of both tables and graphs for categorical data. In this section we’ll learn how to build simple frequency tables, translate them into proportions or percentages, and organize two categorical variables together in a two‑way table. Along the way we’ll see when different visual summaries make sense and preview bar and pie charts (covered in detail in Section 3.2).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Describing Data with Tables and Graphs</span>"
    ]
  },
  {
    "objectID": "03.html#sec-03_01",
    "href": "03.html#sec-03_01",
    "title": "3  Describing Data with Tables and Graphs",
    "section": "",
    "text": "“The greatest value of a picture is when it forces us to notice what we never expected to see.” – John Tukey\n\n\n\nFrequency and relative frequency tables\nA frequency table is a basic but powerful tool for summarizing a categorical variable. It lists each distinct category and records how many observations fall into that category. By organizing raw data into counts, a frequency table provides an immediate snapshot of how the data are distributed across categories, making patterns and imbalances easy to see.\nA typical frequency table has two columns: one for the category labels and one for their frequencies (counts). Constructing a frequency table involves identifying all possible categories and tallying the number of observations in each. This process is often the first step in data analysis because it reduces a potentially large and messy dataset into a clear and interpretable summary.\nWhile frequency tables report absolute counts, these counts can be misleading when sample sizes differ. For example, a category with 50 observations may seem large, but its importance depends on whether the total sample size is 100 or 1,000. To address this, we often compute relative frequencies.\nA relative frequency expresses the count for each category as a proportion of the total number of observations. When a column of relative frequencies—or percentages—is added to a frequency table, the result is called a relative frequency table. Relative frequencies are calculated by dividing each category’s count by the total sample size. Because they represent proportions of the whole, relative frequencies always sum to 1 (or 100% when expressed as percentages).\nRelative frequency tables are especially useful for comparing distributions across different samples or populations. By focusing on proportions rather than raw counts, they allow meaningful comparisons even when the underlying sample sizes are not the same. For this reason, relative frequencies are commonly used in reports, visualizations, and summaries intended for broad audiences.\nIn practice, frequency and relative frequency tables often serve as the foundation for graphical displays such as bar charts and pie charts, providing a clear numerical summary that complements visual representations of categorical data.\n\n\n\n\n\n\nExample 3.1: Common symptoms in a clinic\n\n\n\nImagine you survey 30 patients at a local clinic about the primary symptom that brought them in. You record four categories: “Headache,” “Back pain,” “Fatigue,” and “Nausea.” We can organize the responses in a simple table of counts and proportions. Below we simulate such a survey and display the results.\n\n\n\n\n\nresponse\nfrequency\nrelative_frequency\n\n\n\n\nBack pain\n9\n0.3000000\n\n\nFatigue\n9\n0.3000000\n\n\nHeadache\n8\n0.2666667\n\n\nNausea\n4\n0.1333333\n\n\n\n\n\n\n\n\n\n\n\n\nThe table lists the four categories in alphabetical order with their counts and relative frequencies. For instance, if 8 of the 30 patients reported “Headache,” the relative frequency of “Headache” is \\(8/30 \\approx 0.27\\). The accompanying bar chart gives a visual sense of the same information: each bar’s height corresponds to a category’s frequency, and the bars are separated to emphasize that the categories have no inherent order. In practice you might reorder the bars to make the graph easier to read—perhaps putting the largest category first.\n\n\n\n\n\n\n\n\nPareto charts\n\n\n\n\n\nSometimes you want to highlight the few categories that account for most of the observations. A Pareto chart is a bar chart arranged in descending order of frequency and often paired with a cumulative percentage line. It helps you identify the “vital few and trivial many” in quality control and business applications. Pareto charts are useful when there are many categories and you want to focus attention on the most common causes or responses.\n\n\n\n\n\n\n\n\n\nTip:\n\n\n\n\n\nIn JMP you can create a frequency table by selecting Analyze → Distribution, assigning your categorical variable to the X role, and examining the resulting counts. To add relative frequencies, use the red triangle menu (▸) to choose Display Options → Show Percent. JMP’s Graph Builder will automatically construct a bar chart when you drag a categorical variable to the X‑axis and the count statistic to the Y‑axis.\n\n\n\n\n\nTwo-way (contingency) tables\nWhen data include two categorical variables, a natural question is whether—and how—those variables are related. A two-way table, also called a contingency table, provides a clear way to summarize and explore this relationship. Rather than listing categories separately, a contingency table displays the counts for every combination of categories across the two variables.\nIn a contingency table, one categorical variable defines the rows and the other defines the columns. Each cell in the table shows how many observations fall into the corresponding pair of categories. The table also often includes row totals, column totals, and a grand total, which help place the individual cell counts in context.\nContingency tables are especially useful for examining associations between variables. By comparing counts or proportions across rows or columns, you can look for patterns such as whether certain categories tend to occur together more often than would be expected by chance. For example, you might ask whether voting preference differs by age group, or whether product satisfaction varies by subscription type.\nTo aid interpretation, contingency tables are often converted to conditional distributions by computing row or column percentages. Row percentages answer questions like “Given this row category, how are observations distributed across the columns?” Column percentages reverse the conditioning. Choosing which percentages to compute depends on the research question and which variable is considered explanatory versus response.\nContingency tables form the foundation for formal statistical analysis of categorical data. In later chapters, these tables will provide the structure for chi-square tests of independence, which assess whether the observed association between two categorical variables is stronger than would be expected from random variation alone. Even before formal testing, however, contingency tables offer a powerful descriptive tool for uncovering relationships in categorical data.\n\n\n\n\n\n\nExample 3.2: symptom by age group\n\n\n\nSuppose we collect data on the same symptom question but also record each patient’s age group: “Under 30,” “30–50,” or “Over 50.” We can summarize the joint distribution in a two‑way table.\n\n\n\n\n\nage_group\nBack pain\nFatigue\nHeadache\nNausea\n\n\n\n\n30–50\n3\n4\n5\n3\n\n\nOver 50\n2\n3\n1\n0\n\n\nUnder 30\n4\n2\n2\n1\n\n\n\n\n\nEach cell in the table shows the number of patients who fall into the corresponding combination of age group and symptom. We can also compute row or column relative frequencies to see percentages within each group; for example, dividing each row by its total gives the distribution of symptoms within each age group. Contingency tables allow us to see whether symptom patterns differ across age groups and serve as input for clustered or stacked bar charts (discussed in Section 3.2).\n\n\n\n\nWhy percentages matter\nBecause categorical variables can have different numbers of levels and sample sizes can vary, relative frequencies are essential for fair comparisons. Reporting only counts can be misleading: 20 supporters of a movie genre in a survey of 50 people represent a large fraction, while 20 supporters in a survey of 500 people represent a much smaller fraction. Percentages standardize the scale.\nWhen displaying percentages, make sure they add to 100%. In a pie chart (a circular graph we’ll describe in the next section), each slice represents a category’s percentage of the whole. Pie charts are useful for showing how the total is divided among categories, but they become cluttered with too many slices. Bar charts are more flexible: you can reorder the bars, show counts or percentages, and compare multiple groups using side‑by‑side or stacked bars.\n\n\nWorking in JMP\nIn JMP, tables and graphs for categorical variables are straightforward:\n\nTo create a frequency table, go to Analyze → Distribution, assign your categorical variable to X, and click OK. The report shows counts and percentages; use the red triangle (▸) menu to toggle percentages, counts, or both.\nFor two categorical variables, use Analyze → Fit Y by X and assign one variable to Y and the other to X. Choose Contingency Table from the platform to see the two‑way counts and associated statistics.\nTo visualize categorical distributions, open Graph Builder, drag the categorical variable to the X‑axis, and drop the N summary statistic onto the Y‑axis. You can change the chart type to “Bar” or “Pie.” Dragging a second categorical variable onto the Group drop zone will create clustered or stacked bars.\n\n\n\nRecap\n\n\n\n\n\n\n\nKeyword\nDefinition\n\n\n\n\nFrequency table\nA table that lists each category of a variable and the number of observations in that category.\n\n\nRelative frequency\nThe proportion or percentage of observations in a category, equal to the category’s count divided by the total count.\n\n\nRelative frequency table\nA frequency table with an additional column showing the relative frequency of each category.\n\n\nTwo‑way (contingency) table\nA table that displays the counts for each combination of levels of two categorical variable.\n\n\n\n\n\nCheck your understanding\n\n\n\n\n\n\nProblems\n\n\n\n\n\n\nIn a survey of 80 households, 32 own a dog, 20 own a cat, 12 own both, and the remainder own no pets. Construct a frequency table that shows the number and percentage of households in each pet ownership category (Dog only, Cat only, Both, None). Which visualization—a bar chart or a pie chart—would you choose, and why?\nExplain the difference between a frequency table and a relative frequency table. In what situations is it more informative to look at relative frequencies rather than absolute frequencies?\nWhat is a two‑way (contingency) table? Describe a scenario where a two‑way table could help you explore the relationship between two categorical variables.\nBar charts have spaces between bars and can be drawn in any order. Why are these design choices appropriate for categorical variables? What might go wrong if you drew the bars touching or forced them into a numerical order?\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nPet ownership table. The four categories and their counts are: Dog only (20), Cat only (8), Both (12), None (40). The total number of households is 80. The relative frequencies are 25% dog only, 10% cat only, 15% both, and 50% none. A bar chart would be preferable here because it allows you to order the bars from most to least common and makes it easy to compare magnitudes. A pie chart could work for four categories, but it becomes harder to read when slices are similar in size or when there are many categories.\nFrequency vs. relative frequency. A frequency table reports the counts of observations in each category. A relative frequency table adds a column showing the proportion or percentage of observations in each category. Relative frequencies are more informative when comparing groups of different sizes or when you want to focus on the distribution rather than the sample size—for example, comparing survey results from two classes of different sizes.\nContingency table example. A two‑way table displays counts for each combination of levels of two categorical variable. For instance, you could record whether each patient in a clinic has insurance (Yes/No) and whether they arrived on time (On time/Late). A contingency table would show how many patients fall into each combination (e.g., insured & on time, insured & late, uninsured & on time, uninsured & late), helping you explore whether punctuality differs by insurance status.\nDesign choices. Categories have no intrinsic numeric order, so bars in a bar chart can be arranged in any order without misrepresenting the data. Leaving space between bars reinforces that the categories are distinct and unordered. If you drew the bars touching, it might suggest a continuous scale (like a histogram), which could confuse readers. Forcing categories into a numerical order might imply ranking where none exists.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Describing Data with Tables and Graphs</span>"
    ]
  },
  {
    "objectID": "03.html#sec-03_02",
    "href": "03.html#sec-03_02",
    "title": "3  Describing Data with Tables and Graphs",
    "section": "3.2 Bar Charts and Pie Charts",
    "text": "3.2 Bar Charts and Pie Charts\n\n“Normally if given a choice between doing something and nothing, I’d choose to do nothing. But I would do something if it helps someone do nothing. I’d work all night if it meant nothing got done.” – Ron Swanson\n\nWhen you’ve tallied the counts of a categorical variable, your next job is to turn those numbers into a picture. Two of the simplest pictures—bar charts and pie charts—seem deceptively alike: each shows categories and their sizes. But as we’ll see, they serve different purposes and come with different design rules.\n\nWhat is a bar chart?\nAs we have already seen in Section 3.1, a bar chart is one of the most common and effective ways to visualize categorical data. It displays distinct categories along one axis and represents a numerical value for each category—such as a count, proportion, or percentage—using the length of a bar along the other axis. Bar charts can be drawn with bars oriented vertically (often called column charts) or horizontally, depending on which layout best supports readability.\nA defining feature of bar charts is that the bars are separated by gaps. These gaps signal that the categories are discrete and unordered, distinguishing bar charts from histograms, which display continuous data with adjacent bins. The axis along which the bars extend typically starts at zero, ensuring that bar lengths accurately reflect the magnitudes being compared.\nBar charts are especially effective because humans are very good at comparing lengths that share a common baseline. This makes it easy to see which categories are larger or smaller, to compare differences across groups, and to identify patterns or outliers at a glance. For this reason, bar charts are a natural choice for summarizing frequency tables, relative frequency tables, and summary statistics across categories.\nBar charts are also flexible. They can be used to display raw counts, percentages, averages, or other summary measures, as long as the underlying variable is categorical. When categories have long labels or there are many of them, horizontal bar charts often improve readability. In more advanced settings, grouped or stacked bar charts can be used to compare categories across additional variables.\nBecause of their clarity and versatility, bar charts are often the first visualization used to explore categorical data and to communicate results to a broad audience.\n\n\n\n\n\n\nExample 3.3: Distribution of blood types\n\n\n\nSuppose a hospital records the blood type (A, B, AB or O) of 200 randomly chosen donors. The counts are shown in the table below along with a bar chart. Notice that the bars are separated and can be reordered to make patterns easy to see.\n\n\n\n\n\ntype\ncount\nprop\n\n\n\n\nA\n66\n0.330\n\n\nAB\n9\n0.045\n\n\nB\n31\n0.155\n\n\nO\n94\n0.470\n\n\n\n\n\n\n\n\n\n\n\n\nThe vertical bar chart emphasizes how common type O is relative to the others. You could flip the axes to make a horizontal bar chart if your category names are long or if you prefer to read labels on the y‑axis.\n\n\n\n\nDesign tips for bar charts\nBar charts are powerful because they encode values using length, which humans perceive accurately when bars share a common baseline. Poor design choices can undermine that strength and unintentionally (or intentionally) mislead the reader. The following guidelines help ensure that bar charts communicate information honestly and clearly.\n\nStart the axis at zero\nBecause bar charts represent magnitude through bar length, the axis should almost always start at zero. Truncating the axis exaggerates differences by making small changes appear visually dramatic. While truncated axes are sometimes acceptable for line charts, they are almost always misleading for bar charts.\nWhat goes wrong if you don’t: Two categories with similar values may appear drastically different, leading readers to overestimate the importance of the difference.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeep consistent spacing\nBars should be evenly spaced, with visible gaps between them. The gaps reinforce that categories are discrete, not continuous, and help the eye separate groups cleanly.\nWhat goes wrong if you don’t: Inconsistent or missing gaps can make bars blend together, confusing the chart with a histogram or suggesting unintended relationships.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSort deliberately\nThe order of bars should help the reader understand the data. Common choices include alphabetical order, chronological order, or ordering by value. A deliberate ordering allows patterns and comparisons to emerge naturally.\nWhat goes wrong if you don’t: Random or arbitrary ordering forces the reader to work harder and can hide meaningful trends.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAvoid clutter and gimmicks\nDecorative elements—such as 3-D bars, shadows, gradients, icons, or excessive labels—do not add information. Instead, they distort perception, make values harder to compare, and distract from the data itself.\nWhat goes wrong if you don’t: 3-D effects change apparent bar lengths depending on viewing angle, and visual clutter overwhelms the message.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy these rules matter\nEach of these principles protects the integrity of the visual message. Bar charts are most effective when they are simple, proportional, and transparent. Violating these guidelines can exaggerate differences, obscure patterns, or mislead readers—sometimes unintentionally, sometimes not.\nFollowing these design tips ensures that your bar charts support accurate interpretation and reinforce, rather than undermine, the credibility of your analysis.\n\n\nWhat is a pie chart?\nA pie chart is a graphical display used to show how a whole is divided into parts. The entire circle represents the total—typically 100% of the observations—and each slice corresponds to a category within a categorical variable. The size of each slice is proportional to the category’s share of the total, with larger proportions occupying larger angles and areas of the circle.\nPie charts are widely recognized and easy to interpret at a glance. Their circular shape immediately signals a “part-of-a-whole” relationship, making them intuitive for audiences without formal statistical training. For this reason, pie charts are often used in presentations, reports, and media when the goal is to convey how a total is allocated across a small number of categories.\nHowever, pie charts have important limitations. Human perception is better at comparing lengths along a common baseline than comparing angles or areas. As a result, it can be difficult to judge small differences between slices, especially when categories have similar proportions or when the chart contains many slices. Labels can also become crowded as the number of categories increases.\nBecause of these limitations, pie charts work best when:\n\nThere are only a few categories,\nThe proportions differ substantially,\nThe goal is to emphasize the overall composition rather than precise comparisons.\n\nIn situations where accurate comparison across categories is important, a bar chart often provides a clearer and more informative alternative. Nonetheless, when used sparingly and appropriately, pie charts can be an effective way to communicate simple part-whole relationships.\n\n\n\n\n\n\nExample 3.4: Reasons for missing an appointment\n\n\n\nA dental clinic tracks why patients miss scheduled cleanings. Out of 100 missed appointments, 50 were due to forgetfulness, 20 to fear, 15 to cost, and 15 to other reasons. A pie chart makes the share of each reason obvious.\n\n\n\n\n\n\n\n\n\nThe slices emphasize that half of the missed appointments were simply forgotten. However, imagine adding three more reasons of similar size. The slices would become crowded and hard to compare. Pie charts work only when the categories sum to a meaningful whole and there are no more than a few slices.\n\n\n\n\nWhen to use bar charts vs. pie charts\nAlthough the same categorical data can often be displayed using either a bar chart or a pie chart, the two serve different analytical purposes and are not interchangeable. Choosing the right chart depends on what you want the reader to notice and compare.\n\nUse a bar chart when you want to:\n\nCompare values across categories or between groups. Bar charts place values along a common baseline, making differences in magnitude easy to see. This is especially important when categories have similar values or when precise comparisons matter, such as comparing approval ratings across departments or test scores across teaching methods.\nDisplay statistics that do not form a meaningful whole. Many summary statistics—such as averages, medians, rates, or scores—do not add up to 100% and therefore do not represent parts of a single total. A pie chart would be inappropriate in these cases, while a bar chart can display these values clearly and honestly.\nShow many categories, even if some are small. Bar charts remain readable with a large number of categories, particularly when sorted or displayed horizontally. Small values can still be compared accurately, whereas tiny pie slices are difficult to see and label.\n\nBecause bar charts rely on length comparisons, they align well with how people naturally judge quantity, making them the default choice for most categorical comparisons.\n\n\nUse a pie chart only when:\n\nThe values represent parts of a whole that add up to 100%. Pie charts are specifically designed to show how a total is divided among categories. If the data do not represent proportions of a single whole, a pie chart is misleading.\nThe number of categories is small. Pie charts work best with a limited number of slices—ideally no more than four or five. Too many slices make the chart cluttered and difficult to interpret.\nThe goal is to communicate the overall composition, not precise differences. Pie charts are effective for conveying the big picture—such as showing that one category dominates the total—rather than for comparing closely sized categories.\n\n\n\n\nA practical rule of thumb\nIf you find yourself squinting at a pie chart to decide which slice is larger, that’s a sign the chart is doing too much. In those cases, switch to a bar chart. Because humans are far better at judging lengths than angles or areas, bar charts almost always provide clearer and more accurate comparisons.\nIn practice, bar charts should be your default choice, with pie charts reserved for simple, high-level part-of-a-whole messages.\n\n\nClustered and stacked bar charts\nSometimes you have two categorical variables and want to see how their categories interact. We introduced two‑way tables in Section 3.1; here’s how to graph them.\nA clustered (side‑by‑side) bar chart groups bars for each level of a second variable next to each other so you can compare across groups. For example, imagine you survey 120 patients about how satisfied they were with a new physical therapy program (satisfied, neutral, dissatisfied) and record whether they were in the treatment or control group. A clustered bar chart shows differences in satisfaction between the two groups.\n\n\n\n\n\n\n\n\n\nIn a stacked bar chart, bars for each category are stacked atop one another. This emphasizes the total size of each category but makes it harder to compare the segments across stacks. You might use a stacked chart to show how types of injuries (sprain, fracture, other) contribute to emergency visits across departments; if you convert each bar to 100% of its height, you get a 100% stacked bar chart that highlights composition within each group.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautions with stacked bars\nStacked bars are useful when you care about the total across categories, but they hide patterns in the middle segments. In the last plot, you can easily compare the overall emergency visits across departments and the share of fractures, but it’s harder to compare the “other” injuries across departments because their segments float at different heights. If your goal is to compare subgroups, a clustered bar chart is usually better.\n\n\n\nWorking in JMP\nIn JMP, bar and pie charts live in the Graph Builder. Drag your categorical variable to the X‑axis and drop the N or % statistic onto the Y‑axis to create a bar chart. To cluster by a second categorical variable, drop it in the Group or Overlay zone, and choose Bar (Horizontal) or Bar (Vertical) from the chart palette. To stack, use the Stack option in the legend. To make a pie chart, drag the categorical variable to a blank canvas and choose Pie; JMP will automatically convert counts to percentages and label the slices. Use the red triangle (▸) menu to display data labels, reorder slices, or combine small categories into an “Other” slice.\n\n\nRecap\n\n\n\n\n\n\n\nKeyword/Concept\nDefinition or note\n\n\n\n\nBar chart\nGraph that displays categories along one axis and uses the length of bars to represent numeric values; great for comparing counts or percentages across categories.\n\n\nPie chart\nCircular chart in which slices represent how a total is divided among categories; appropriate only when values sum to a meaningful whole and the number of categories is small.\n\n\nClustered (side‑by‑side) bar chart\nBar chart where categories are grouped side by side for levels of a second variable; useful for comparing groups across categories.\n\n\nStacked bar chart\nBar chart where bars for each subgroup are stacked; shows composition and totals but makes it harder to compare individual segments.\n\n\n\n\n\n\n\n\n\nCheck your understanding\n\nA clinical trial reports the average pain score (on a 0–10 scale) for three physical therapy programs. Should you use a bar chart or a pie chart? Explain your reasoning.\nA nutritionist surveys 500 patients about their preferred breakfast type: cereal, fruit, eggs, or none. The counts are 150, 120, 80, and 150. Sketch how you would display this information with a pie chart. When might a bar chart be preferable?\nIn a mental health study, participants are classified into stress levels (low, moderate, high) and whether they attended counseling (Yes/No). Which type of bar chart would you use to compare stress levels between counseling and non‑counseling participants? What pattern would indicate that counseling is associated with lower stress?\nA bar chart shows the average number of cavities per patient in three dental clinics: 1.2, 1.3 and 1.4 cavities. The y‑axis starts at 1.0. Explain why this design might mislead and how to fix it.\nGive two reasons why pie charts often make it harder to compare categories than bar charts.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nYou should use a bar chart because the numbers represent average pain scores and do not sum to a meaningful whole. Pie charts imply a part‑to‑whole relationship and would be misleading here.\nThe four breakfast types form parts of the whole sample (500 patients). In a pie chart, the slices would be 30% cereal, 24% fruit, 16% eggs, and 30% none. However, a bar chart may be preferable because it allows you to order the categories from most to least common and makes it easier to compare the cereal and none categories, which are equal in size.\nA clustered (side‑by‑side) bar chart would let you compare the counts (or proportions) of low, moderate, and high stress within the counseling and non‑counseling groups. If counseling is associated with lower stress, you would expect the “low” bar to be taller (or the “high” bar shorter) in the counseling group than in the non‑counseling group.\nStarting the y‑axis at 1.0 truncates the bars and exaggerates small difference. To avoid misleading readers, start the axis at zero. Alternatively, use a dot plot or annotate the differences directly if the differences are small but meaningful.\nFirst, people judge lengths more accurately than angles; in a pie chart it is hard to gauge the exact size of a slice. Second, when slices are similar in size or there are many categories, comparing slices becomes difficult and the chart becomes cluttered. Bar charts avoid these issues by using a common baseline and allowing many bars.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Describing Data with Tables and Graphs</span>"
    ]
  }
]